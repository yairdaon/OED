\documentclass{amsart}

\input{definitions}

\numberwithin{equation}{section}

\begin{document}

\title[Sensor Clusterization in D-optimal design in infinite
  dimensions]{Sensor Clusterization in D-optimal Design in Infinite
  Dimensional Linear Bayesian Inverse Problems}

\author{Yair Daon}
\address{Porter School of the Environment and Earth
  Sciences, Tel Aviv University\\ Tel Aviv, Israel}
%% \curraddr{Porter School of Environment Studies, Tel Aviv University\\ Tel Aviv, Israel}
%% \curraddr{}
\email{yair.daon@gmail.com}
%% \thanks{}

%    The 2010 edition of the Mathematics Subject Classification is
%    the current definitive version.
\subjclass{Primary: 
  62F15, % Statistics - bayesian inference
  35R30, % PDE - Inverse problems
  Secondary:
  28C20, % Measure and integration - set functions and measures and integrals in
  %infinite-dimensional spaces (Wiener measure, Gaussian measure, etc.
}
%% Primary: 62F15, 35R30; Secondary: 28C20
\date{\today}

\begin{abstract}
  We investigate the problem of sensor clusterization in optimal
  experimental design for infinite-dimensional Bayesian
  linear inverse problems. We suggest an analytically
  tractable model for such designs and reason how it may lead to
  sensor clusterization in the case of iid measurement noise. We also
  show that in the case of spatially correlated measurement error
  clusterization does not occur. As a part of the analysis we prove a
  matrix determinant lemma analog in infinite dimensions, as well as a
  lemma for calculating derivatives of $\log \det$ of operators.
\end{abstract}

\maketitle

\section{Introduction}\label{section:OED intro}
Experimental design is an important part of many scientific
investigations. When considering an inverse problem, one can often
specify sensor locations (e.g.\ in geophysics and oceanography
applications), certain wavelengths (e.g.\ in MRI) or wave reflections
from the ground (e.g.\ searching for oil or using a radar). Whatever
the allowed set of measurements is, one should select the optimal
measurements to take, in order to increase accuracy, reduce costs, or
both.

Designing experiments is usually done by optimizing some \emph{design
  criterion} and this is true both for frequentists
\cite{Silvey13,Ucinski05} as well as for Bayesians
\cite{ChalonerVerdinelli95}, see \cite{ChalonerVerdinelli95} for an
investigation of the analogy between the two approaches. Although
there is a plethora of design criteria, in the current work we will
focus on just one of these, commonly referred to as \emph{D-optimal
  design}. It has a simple and appealing motivation in the Bayesian
context as explained in \cite{ChalonerVerdinelli95}: for a linear
model, a D-optimal design seeks to maximize the expected information
gain (KL divergence \cite{KullbackLeibler51,CoverThomas12}) between
posterior and prior. In finite dimensions, maximizing the expected
information gain amounts to minimizing the determinant of the
posterior covariance matrix. In a frequentist setting, an optimal
design minimizes the volume of the uncertainty ellipsoid \cite[page
  16]{Ucinski05}, but this is done for the Fisher information matrix
and not the posterior covariance. However, \cite{ChalonerVerdinelli95}
shows that the latter is just a regularized version of the former.

The previous discussion is classical for experimental design when
inference is to take place over a finite (not too large) number of
parameters. The subject of optimal experimental design for function
inference in a Bayesian context was pioneered by
\cite{AlexanderianGloorGhattas14,AlexanderianPetraStadlerEtAl16,
  AlexanderianPetraStadlerEtAl14}. Similarly to the finite dimensional
case, it can be shown that a D-optimal design arises naturally for
linear models when one wishes to maximize the KL divergence between
posterior and prior and this amounts to minimizing the determinant of
the posterior covariance operator (understood as a product of its
eigenvalues). Some difficulties arise in the process, but remedies can
be found as shown in \cite{AlexanderianGloorGhattas14}.

It seems counter intuitive that when one computes an optimal design
using the D-optimal criterion (and others that will not be
investigated here), the optimization process results in measurements
that are very similar. For example, if a measurement is thought of as
measuring some function value at $\x \in \Omega \subseteq \R^d,
d=1,2,3$ (with added error) then the optimization procedure sometimes
places sensors in close proximity to each other (as can be seen
in figure \ref{fig:clusterization illustration}). Following
\cite{Ucinski05}, we refer to this phenomenon as \emph{sensor
  clusterization}.

\subsection{Related Work}
The phenomenon of sensor clusterization seems to be known in several
different contexts. In a frequentist and finite-dimensional context,
\cite{Fedorov96} and \cite[chapter 2.4.3]{Ucinski05} discuss this
phenomenon and suggest an approach called clusterization-free design,
where the user enforces measurement locations to be far from each
other. One way to do this is by introducing correlated errors which,
philosophically, accounts for both measurement error and model error.
Another method considered is imposing distance constraints between
measurements. A somewhat different approach is suggested in \cite[page
  49]{Fedorov12}, where close by design measurements are merged --- a
procedure which obviously does not avoid clusterization. The same problem
arises in time-series analysis for pharmacokinetic experiments. The
authors of \cite{Hooker09} suggest modeling auto-correlation time in
the noise model, which is equivalent to the correlated errors
mentioned above.

Any of the above mentioned approaches might serve as a remedy and push
sensors away from each other. Yet, none offers any insight as to why
clusterization occurs. Also, as better models are employed, model
error is decreased and the clusterization phenomenon will eventually
reappear. While these approaches are practical and help us avoid the
problem, they do not provide insight as to why sensors are clustering.

\subsection{Contribution}
We propose and thoroughly study an analytically tractable model for
understanding D-optimal designs. In this model, an optimal design
arises as a solution to a constrained optimization problem, formulated
using Lagrange multipliers (section \ref{section:D and grad}). In the
case of no model error we show this problem is in fact a set of
eigenvector-eigenvalue problems (equation \eqref{eq:eigenproblem}). We
then characterize an optimal design as one that reduces the highest
uncertainties, and show how that can give rise to repeated
measurements (section \ref{subsec:clusterization} and figure
\ref{fig:clusterization}).

In the process we generalize several lemmas from linear algebra to
infinite-dimensional settings --- the Matrix Determinant Lemma and a
lemma for calculating the derivative of determinants. We also provide
other tools for understanding D-optimal designs in
infinite-dimensional Bayesian inverse problems, including lemmas for
calculating the increase in the design criterion per observation ---
Lemmas \ref{lemma:design increase} and Corollary \ref{cor:zero mod
  err}. %%  We also show that in our model with independent measurement
%% noise, an interesting phenomenon arises, in which the eigenvectors
%% of the measurement operator (denoted below $\obs^*\obs$) are the
%% same as the eigenvectors of the prior covariance in observation
%% space, which helps arrive at the results outlined in the previous
%% paragraph.
The arguments presented are completely generic and apply equally to
any of the scenarios mentioned in the introduction.

%% \subsection{Limitations}\label{subsec:limitations}
%% There are two main drawbacks of the arguments presented here. First,
%% one can argue that we allow considerably less restrictive observations
%% than any real-life problem does. This is because our observation model
%% allows taking measurements that are arbitrarily weighted linear
%% combinations of eigenvectors. The second drawback is that we do not
%% show rigorously that clusterization necessarily occurs in our model,
%% we only show that it can occur.


\subsection{An Example of Clusterization}\label{subsec:example}
\begin{figure}
  \begin{tikzpicture}[thick, scale=1.3, every node/.style={scale=0.99}]
    \begin{axis}
      [
      xmin = 0,
      xmax = 3.14,
      xlabel = {$x$},
      ylabel = posterior std,
      ymin   = 0,
      %compat = 1.3,
      % ymax   = 130,
      % ytick = \empty,
      legend cell align=left,
      % legend style={at={(0.45,0.2)}}
      legend pos= outer north east 
      ]
      % \draw[black!30!white, thin] (50,0) -- (50,130);
      % 
      %% \addplot [thin, black, mark=none] table{stdv-heat-sens1-var1.txt};
      %% \addlegendentry{1 sensors};
      
      %% \addplot [thin, blue, mark=none] table{stdv-heat-sens2-var1.txt};
      %% \addlegendentry{2 sensors};
      
      %% \addplot [thin, red, mark=none] table{stdv-heat-sens3-var1.txt};
      %% \addlegendentry{3 sensors};
      
      \addplot [thin, green, mark=none] table{stdv-heat-sens4-var1.txt};
      \addlegendentry{4 sensors};
      
      \addplot [thin, purple, mark=none] table{stdv-heat-sens5-var1.txt};
      \addlegendentry{5 sensors};
      
      \addplot [thin, cyan, mark=none] table{stdv-heat-sens6-var1.txt};
      \addlegendentry{6 sensors};

  
      %% \addplot [black,  only marks, mark=x, mark size=1.5] 
      %% table{locs-heat-sens1-var1.txt}; 
      %% \addplot [blue,   only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens2-var1.txt}; 
      %% \addplot [red,    only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens3-var1.txt};
      \addplot [green,  only marks, mark=x, mark size=1.5] 
      table{locs-heat-sens4-var1.txt}; 
      \addplot [purple, only marks, mark=x, mark size=1.5] 
      table{locs-heat-sens5-var1.txt}; 
      \addplot [cyan,   only marks, mark=x, mark size=1.5] 
      table{locs-heat-sens6-var1.txt}; 
  
      
    \end{axis}
  \end{tikzpicture}
  \caption{Shown are pointwise posterior standard deviations for
    various numbers of sensors, and location of these sensors for the
    1D heat equation with homogeneous Dirichlet boundary described in
    section \ref{subsec:example}. Note the clusterization that occurs
    when 6 sensors are used --- only 4 measurement locations are
    included. Observed data does not influence the pointwise
    variances, hence it is omitted.}
\end{figure}\label{fig:clusterization illustration}
In section \ref{section:abstract OED} we present a more abstract and
general formulation of the problem, but for the purpose of
illustration, we present the problem via a toy model --- the 1D heat
equation in $[0,\pi]$ with homogeneous Dirichlet boundary
conditions.

The 1D heat equation is:
\begin{subequations}\label{eq:heat equation}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,\pi] \times [0,\infty),\\
      u &= 0 &&\qquad \text{on } \{0, \pi\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,\pi] \times \{0\}.
  \end{alignat}
\end{subequations}

We would like to infer the initial condition $u_0$. For that purpose,
we measure $u$ at some set of locations $\x_j \in [0,\pi], j=1, \dots,m$
and a final time $T > 0$. We assume centered Gaussian measurement
error, so we can observe $v(\x_j,T) = u(\x_j,T) + \eps(\x_j)$ with
$\eps(\x_j) \sim \normal(0, \sigma^2), \sigma > 0$ iid. We model the
initial condition as $u_0 \sim \normal(0,\prcov)$, for $\prcov =
(-\Delta)^{-1}$ with homogeneous Dirichlet boundary condition. It is
well known \cite{Tarantola05} that for linear problems, with Gaussian
prior and error, the posterior is also Gaussian with a covariance that
does not depend on the observed data. The posterior covariance
$\postcov$ is known to have a closed form formula, even in infinite
dimensions\cite{Stuart10}. If we denote by $\fwd$ the dynamics
operator, so that $u( \cdot,T) = \fwd u_0$, and the observation
operator $\obs$ so that $u(\x_j,T) = (\obs u)_j, j=1,\dots,m$, then the
posterior covariance is known and depends only on $\prcov, \fwd, \obs$ and
$\sigma^2$ (see section \ref{section:abstract OED} and
\eqref{eq:postcov} specifically).

We will consider generalization of the information-theoretic design
criterion presented in section \ref{section:OED intro} to infinite
dimensions (section \ref{subsec:D optimal design}). We choose
$\x_j,j=1,\dots,m$ so as to minimize an expression analogous to the
log-determinant of the posterior covariance operator (corresponding to
a D-optimal design). We can see the clusterization effect in figure
\ref{fig:clusterization illustration} --- the design criterion
suggests placing sensors in locations that are extremely close.


\subsection{Remarks to the Reader}\label{subsec:plan}
The Lagrange multipliers problem for iid measurement noise is
discussed in section \ref{section:vanishing}. In that section, it is
shown how sensor clusterization can occur. The reader may skip if they
are constrained by time. It is advised that a reader less comfortable
with Hilbert and Banach spaces mostly ignore these terms and consider
$\hil$ and $\ban$ finite dimensional vector spaces.

%% Seeking an optimal design amounts to maximizing $\tar(\obs)$ in
%% $\obs$, subject to constraints posed on $\obs$. These enter as
%% constraints on the allowed set of measurement vectors we can choose
%% from --- the rows of $\obs$. These constraints are determined by the
%% problem structure --- for example, in the 1D heat equation example
%% considered in section \ref{subsec:example}, we were constrained to
%% choose $\obs = (\meas_1,\dots,\meas_m)^t$ with $\meas_j = \delta_{\x_j}$
%% for some $\x_j \in [0,\pi]$. We present an approach to understanding
%% optimal experiment design based on the first-order necessary condition
%% for optimal design, where the constraints are incorporated using
%% Lagrange multipliers. This condition is used to understand what causes
%% clusterization. One problem with such an approach is that the set we
%% can choose $\obs$ from is complicated and difficult to work with. For
%% example, if $\ban = C(\Omega)$ for some compact $\Omega$, then the
%% allowed set of measurement vectors may only be point evaluations --- a
%% set that is not easy to describe as a level set of some simple
%% function, a description crucial for the use of Lagrange multipliers.
%% In order to facilitate analysis, we consider an easier constraint in
%% section \ref{subsec:unit length}. Reverting to this simpler constraint
%% is a limitation of our analysis but it provides some insight into the
%% cause of clusterization.

\section{Preliminaries and Notation}\label{section:prelim}

\subsection{Overview}
The theoretical foundations for inverse problems over function spaces
can be found in \cite{Stuart10}. In this section we define notations that will be
  used throughout the paper.
%% A reader mostly familiar with the
%% literature may initially want to skip this section and only consult
%% the list below, which summarizes the notation used. We denote:
%% \begin{itemize}
%%   \item Parameter space $\hil$.
%%   \item Data space $\ban$.
%%   \item Forward dynamics operator $\fwd: \hil \to \ban$.
%%   \item Observation operator $\obs: \ban \to \mathbb{R}^m$.
%%   \item Prior measure $\pr = \mathcal{N}(0, \prcov)$.
%%   \item Data $\data \in \mathbb{R}^m$.
%%   \item Observation and model errors, captured in $\Sigma(\obs)$.
%% \end{itemize}
%% The design objective was derived in \cite{AlexanderianGloorGhattas14}
%% and recorded in \eqref{eq:objective} below.



\subsection{Bayesian Linear Inverse Problems}\label{section:abstract OED}
Let $\hil$ be a Hilbert space and $\ban$ a Banach space, both
infinite-dimensional. Consider $\fwd: \hil \to \ban$, a linear
operator (the ``forward operator''). We are interested in forward
operators that are strongly smoothing (have fast decaying singular
values --- the heat operator from section \ref{subsec:example} is a
prime example). We want to infer $\param \in \hil$, some parameter of
the dynamics given noisy observations of $\fwd \param$. We take a zero
mean Gaussian prior $\pr$ for $\param$ (otherwise the mean can just be
subtracted everywhere). Hence, $\param \sim \pr = \normal(0,\prcov)$
with some appropriate covariance operator $\prcov$
\cite{Stuart10}. Observations (measurements) are taken via the linear
observation operator $\obs \in ( \ban^* )^m$, where $m$ is the number
of observations allowed (e.g. sensors at our disposal). In an analogy
with linear algebra (where row vectors are thought of as linear
functionals), we think of $\obs$ as having ``rows'' $\meas_j,
j=1,\dots,m$:
\begin{equation}\label{eq:O}
  \obs = (\meas_1,\dots, \meas_m)^t, \meas_j \in \ban^*, j = 1,\dots,m.
\end{equation}
This way, for $u \in \ban$ we have $\obs u = (\meas_1(u), \dots,
\meas_m(u) )^t \in \R^m$.
%% but we may drop the parentheses and write $\meas_j u = \meas_j(u)$.
A few observations regarding $\obs$ are in order. First, it is good to
keep in mind that $( \ban^* )^m$ is a Banach space with norm $\| \obs
\| = \sum_{j=1}^m \|\meas_j\|$. Next, for $u \in \ban$ and $v\in
\R^m$:
\begin{align*}
  \big (\obs^*v \big ) (u) &= \langle v, \obs u \rangle_{\R^m} = \sum_{j=1}^m  v_j \meas_j(u)
  = v^t \left ( \obs u \right ) = (v^t \obs) (u),
\end{align*}
and thus:
\begin{align}\label{eq:obs*}
  \obs^*v &= \sum_{j=1}^m v_j \meas_j = v^t \obs.
\end{align}

\begin{observation}
  It is best to think of $\meas_j,j=1,\dots,m$ as row vectors and of
  $\obs$ as a matrix with rows $\meas_j$. 
\end{observation}

Each measurement consists of a linear functional, chosen from an
allowed set of linear functionals in $\ban^{*}$. Data is acquired via
noisy measurements
\begin{align*}
  \data := \obs (\fwd \param + \eps') + \eps = \obs \fwd \param + \obs \eps' + \eps
\end{align*}
with $\data \in \R^m$ and $\eps, \eps'$ defined next. Measurement
error consists of two parts. First, there is spatially correlated
error $\eps' \sim \normal(0,\modcov)$, modeled as a centered Gaussian
measure on $\ban$ with covariance operator $\modcov: \ban^* \to \ban$
(see \cite[section 6]{Stuart10} for some details on Gaussian measures
on Banach spaces). Then there is measurement / independent error
$\eps \sim \normal(0, \sigma^2 I_m)$, with $I_m$ the $m \times m$
identity. Both error terms and the prior are assumed independent of
each other. Let us now understand the terms present in the above
expression.

\subsubsection{Dynamics and Observation Operators}\label{subsec:dynamics}
One can think of $(\obs \fwd \param)_j$ as pointwise evaluations of a
continuous function if $\ban$ is some appropriate function space,
e.g.\ $C(\Omega)$, the Banach space of real-valued continuous
functions over a compact $\Omega$ with the $\| \cdot \|_{\infty}$
norm. These measurements can be represented using the linear
functionals $\delta_{\x_j} \in C(\Omega)^*$. In a different setting,
we may be able to measure other quantities, e.g.\ some Fourier
coefficients as is the case in MRI applications. Either way, the rows
of $\obs$ are linear functionals and we refer to these as {\it
  measurement vector}s. Note that we cannot choose every measurement
vector we want, though. For example, we may be restricted only to
pointwise evaluations of $\fwd \param$ by the sensors at our disposal,
in which case measuring the mean $\ell(u) = \int_{\Omega}u$ (or any
non-atomic measure) of a function is not possible.

\subsubsection{Error Terms}
Considering the sum of the error terms it is easy to see that $\obs
\eps' + \eps \in \R^m$. It is a centered Gaussian random vector and
its covariance matrix is
\begin{align}\label{eq:Sigma}
  \begin{split}
    \Sigma(\obs) :&= \mathbb{E}[ (\obs \eps' + \eps)  (\obs \eps' + \eps)^t ] 
    % 
    % 
    = \obs \modcov \obs^* + \sigma^2I , 
  \end{split}
\end{align}
where
\begin{equation}\label{eq:modcov explained}
  [\obs \modcov \obs^*]_{ij} = e_i^t \obs \modcov \obs^* e_j = \meas_i (\modcov \meas_j),
\end{equation}
where the first equality in \eqref{eq:modcov explained} holds by
definition and the second by \eqref{eq:obs*}. The explicit dependence
on $\obs$ will be mostly dropped for notational convenience, so
$\Sigma(\obs) = \Sigma$. Thus, for a fixed $\obs$ (i.e.\ a fixed set
of measurements) we can equivalently write $\data = \obs \fwd \param +
\bar{\eps}$ with $\bar{\eps} \sim \normal(0,\Sigma)$. At this point,
it is useful to record a known result regarding the posterior
covariance operator of our inverse problem, namely
\begin{align}\label{eq:postcov}
  \postcov = (\prcov^{-1} + \fwd^* \obs^* \Sigma^{-1} \obs \fwd
  )^{-1}.
\end{align}
Usually, this is written using $\tmp := \obs \fwd$ and $\tmp^*$ but
since we need to separate the observation operator $\obs$ from the
forward operator $\fwd$ we write the explicit expression
\eqref{eq:postcov}.

Taking $\modcov = 0$ is common practice
\cite{Tarantola05,KaipioSomersalo05,Vogel02} and then we are reduced
to the case where we take iid observations and $\Sigma = \sigma^2I$ is
simply a scalar matrix that does not depend on $\obs$. Note that
taking an error model with a non-scalar covariance as we do here
allows us to consider model error (modeled by $\modcov$) as well as
measurement error (modeled by $\sigma^2$). For example, say we believe
our forward model does not capture some small scale phenomenon.  Then
we may express this belief by saying $\tru = \fwd + \err$, with $\fwd$
depending on $\param$ and $\err$ depending on $\sspar$, and $\param
\perp \sspar$. We do not know much about this effect but it is
reasonable to assume it changes continuously in our domain. We (may
choose to) model it as $\normal (0, \modcov)$ and take $\modcov$ to
reflect the spatial (or other) variability we imagine $\err \sspar$
has. Such small scale phenomenon can arise as a modeling issue, where
we might not model the system in its entirety. It can also arise from
a numerical source, where our discretization of the system is not fine
enough to capture all small scale phenomena. We will see that assuming
some correlation in the error may reduce the clusterization
phenomenon, as is also reported in the literature \cite{Ucinski05}.

\subsection{D-Optimal Design in Infinite Dimensions}\label{subsec:D optimal design} 
The meaning of D-optimal design in infinite-dimensional Hilbert spaces
was investigated in \cite{AlexanderianGloorGhattas14}. Note that the
authors make assumptions that amount to $\Sigma=I$ (implied by
$\modcov = 0,\sigma^2=1$), but we choose not to take these
simplification here. This is because $\Sigma$ can determine ``how
much'' clusterization we see. As stated
\cite[pp. 681]{AlexanderianGloorGhattas14}, the results hold for more
general covariance matrices. The conclusion is that in
infinite-dimensions, a D-optimal design is well-defined as maximizing
the expected KL divergence between posterior and prior (denoted $\tar$
below and throughout the paper) and is summarized, using our notation,
in the following theorem:
\begin{theorem}[Slightly modified Theorem 1 from \cite{AlexanderianGloorGhattas14}]
  Let $\pr = \normal(0,\prcov)$ be a centered Gaussian prior
  on $\hil$   and let $\post = \normal(\postmean,\postcov)$ 
  the posterior measure on $\ban$ for the Bayesian linear
  inverse problem $\data =  \obs \fwd\param + \obs \eps' + \eps$ discussed
  above. Then 
  \begin{align}\label{eq:objective}
    \begin{split}
      \tar( \obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
      % 
      % 
      % 
      &= \frac12 \log \det 
      ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}).
    \end{split}
  \end{align}
\end{theorem}


\section{The Constrained Optimization Problem of D-Optimal Design}\label{section:D and grad}

\subsection{Overview}
From \eqref{eq:objective} we wish to characterize solution(s) of the
following optimization problem for $\tar$. %%: (\ban^*)^m \to \R$:
\begin{align}\label{eq:optimization}
  \obs^{\star} := \argmax_{\obs} \tar( \obs ) 
  = \argmax_{\obs} \frac12 \log \det 
  (I + \prcov^{1/2} \fwd^*\obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}),
\end{align}
where $\obs$ is constrained to some allowed set of measurements
(e.g.\ pointwise evaluations on $[0,\pi]$).
%%, for the example in section \ref{subsec:example}).
We seek a formulation of the optimal design using Lagrange
multipliers. In section \ref{section:objective} we find the gradient
of $\tar$ and record it in \eqref{eq:tar grad}. In section
\ref{subsec:unit length} we relax the constraints on $\obs$ so that
the optimization problem in \eqref{eq:optimization} is analytically
tractable.  We find gradients for the new constraints and record them
in \eqref{eq:grad constraints}. Finally, in section
\ref{subsec:necessary} we use these to formulate the optimal design
problem as a Lagrange multipliers problem in equation
\eqref{eq:conditions}.


\subsection{The Objective and its Gradient}\label{section:objective}
In order to use Lagrange multipliers, we need to find the gradient of
$\tar$. This section is mostly technical and devoted to calculating
said gradient. The result is recorded in \eqref{eq:tar grad}.

We start by calculating the first variation. Some calculations are
delegated to Lemma \ref{lemma:aux calc} in the appendix. Also, we use
the following lemma whose proof is also delegated to the appendix
\ref{lemma:lax}:
\begin{restatable*}[Generalized from \cite{Lax97}]{lemma}{lax}\label{lemma:lax}
  Let $Y(t)$ be a differentiable operator-valued function. Assume 
  $I+Y(t)$ is invertible, $Y(t)$ self-adjoint and trace-class. Then
  \begin{equation*}
    \frac{\der \log \det (I+Y(t))}{\der t} = \tr{(I+Y(t))^{-1} \dot{Y}(t)}.
  \end{equation*}
\end{restatable*}

First variation of $\tar (\obs)$ in the direction $V$ is:
\begin{align*}%%\label{eq:tar var}
  \begin{split}
    \delta \tar(\obs) V 
    :&= \frac{\der}{\der\tau} \Big |_{\tau=0} \tar(\obs + \tau V)\  \text{ (by definition of variation)}\\
    % 
    % 
    % 
    &= \frac12 \frac{\der}{\der \tau} \Big |_{\tau=0} \log \det 
    (I + \prcov^{1/2} \fwd^* T(\obs+\tau V)\fwd \prcov^{1/2} ) \text{ (by definition \eqref{eq:objective})}\\
    % 
    % 
    % 
    &= \frac12 \tr{( I + \prcov^{1/2} \fwd^* \obs^* \Sigma^{-1}
    \obs\fwd \prcov^{1/2} )^{-1}
    \frac{\der}{\der \tau} \Big |_{\tau=0}
    \prcov^{1/2} \fwd^* T(\obs+\tau V) \fwd \prcov^{1/2}}\ \text{ (by \ref{lemma:lax})}\\
    % 
    % 
    % 
    &= \frac12 \ttr{\postcov \fwd^* (V^* \Sigma^{-1} \obs 
      - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
      &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs 
      + \obs^* \Sigma^{-1} V ) \fwd}  \text{ (by \ref{lemma:aux calc})}\\
%%   \end{split}
%% \end{align} 
%% Taking adjoints and cyclic permutations we see that
%% \begin{align*}
%%   \tr \postcov \fwd^* V^* \Sigma^{-1} \obs \fwd 
%%   &= \tr \postcov \fwd^* \obs^* \Sigma^{-1} V \fwd,
%% \end{align*}
%% and
%% \begin{align*}
%%   \tr \postcov \fwd^* \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \fwd
%%   &= \tr \postcov \fwd^*  \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs \fwd.
%% \end{align*}
%% We use these in the first equality of the following derivation,
%% which continues the one in \eqref{eq:precursor}.
%% %
%% \begin{align} \label{eq:tar var}
%%   \begin{split}
      %% \delta \tar(\obs) V 
    &= \tr{\postcov \fwd^* ( \obs^* \Sigma^{-1} V 
    - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs ) \fwd} \\
    %
    %
    % 
    &= \tr{\postcov \fwd^* \obs^* \Sigma^{-1} V 
    ( I - \modcov \obs^* \Sigma^{-1}\obs ) \fwd} \\
    % 
    %
    %
    &= \tr{V ( I - \modcov \obs^* \Sigma^{-1}\obs )
    \fwd \postcov \fwd^* \obs^* \Sigma^{-1}}.
  \end{split}
\end{align*} 
Denote
\begin{align}\label{eq:tar grad}
  \nabla \tar(\obs) &:= (I - \modcov \obs^* \Sigma^{-1} \obs) \fwd
  \postcov \fwd^* \obs^*\Sigma^{-1},
\end{align}
the gradient of $\tar (\obs )$ and we now justify this definition.
Since the trace of $A := V \nabla \tar(\obs) \in \R^{m \times m}$ is
just $\tr{A} = \sum_{j=1}^m e_j^t A e_j$ (with $e_j$ the $j$th standard
basis vector), we see that
\begin{align*}
  \delta \tar(\obs)V = \tr{V \nabla \tar(\obs)} = \sum_{j=1}^m
  V_j(\nabla \tar(\obs)_j),
\end{align*}
with $V_j \in \ban^*$ and $\nabla \tar(\obs)_j \in \ban^{**}\subseteq
\ban, j=1,\dots,m$. Thus, $\nabla \tar( \obs ) \in (\ban^{**})^m
\subseteq \ban^m$ is indeed the correct gradient and the notation
\eqref{eq:tar grad} is justified. Note that by the comment following
\eqref{eq:obs*}, we view $V$ (defined on the same space as $\obs$) as
a column vector. Then the gradient $\nabla \tar(\obs)$ should be
viewed as a row vector, as the product $V \nabla \tar$ is in $\R^{m
  \times m}$. This will prove important in section
\ref{subsec:necessary}.

\subsection{Unit Length Constraints and their Gradients}\label{subsec:unit length}
In this section we suggest relaxed constraints in
\eqref{eq:constraints} and derive their gradient in \eqref{eq:grad
  constraints}, for the purpose of using them in a Lagrange
multipliers problem.

As mentioned before, we cannot choose any $\obs$ when maximizing
$\tar(\obs)$. Recall $\obs = (\meas_1,\dots,\meas_m)^t \in (\ban^*)^m$
and each $\meas_j,j=1,\dots,m$ must be chosen from some allowed set of
functionals in $\ban^*$. This set differs based on the kind of sensors
we have at our disposal and the properties of $\ban$, as discussed in
section \ref{subsec:dynamics}. The following proposition will give us
better understanding of the constraints. 
\begin{restatable*}{proposition}{biggerbetter}\label{prop:bigger better}
  Let $\obs = (\meas_1,\dots,\meas_m)^t$, $j \in \{1,\dots,m\}$, $\sigma^2
  > 0$ and $\lambda > 1$. Then $\tar(\obs)$ increases if we use
  $\lambda \meas_j$ in $\obs$ instead of $\meas_j$.
\end{restatable*}
Proof of Proposition \ref{prop:bigger better} is delegated to the
appendix but the idea behind it is simple: making rows of $\obs$ large
is equivalent to making $\sigma^2$ small. This can be understood
easily from the formulation of the inverse problem $\data = \obs \fwd
\param + \obs \eps' + \eps$, where $\eps'$ can be taken as model error
and $\eps$ is iid measurement error.

We now suggest a more convenient-to-work-with set of constraints. By
Proposition \ref{prop:bigger better}, we cannot just take all $\ban^*$
to be the allowed set of measurement vectors, since this would
effectively eliminate measurement error entirely. We note that for
point evaluations, the norm of the measurement is always one:
\begin{align*}
  \| \delta_{\x} \| = \sup_{0 \neq u \in C(\Omega)} \frac{
    |\int_{\Omega}u(\y) \delta_{\x}(\y) \der \y| 
  }{
    \sup|u|}
  = \sup_{0 \neq u \in C(\Omega)} \frac{|u(\x)|}{ \sup|u|} = 1,
  \forall \x \in \Omega.
\end{align*}
Following this, we also restrict the norm of our measurement vectors
to be one. But the norm on functionals is hard to work with. Instead,
we assume there exists a Hilbert space $\bar{\hil} \subseteq \ban^*$
and take measurements $\meas_j \in \bar{\hil}^* = \bar{\hil}$ such
that $\| \meas_j\| = 1, j=1,\dots,m$. Recall that we think of $\ban$ as
a somewhat restricted space ($C(\bar{\Omega})$ with the sup-norm over
a bounded domain $\Omega$, for example) and so $\ban^*$ can be thought
of as large. Then $\ban^*$ can accommodate a reasonable Hilbert space
$\bar{\hil}$. For example, if $\ban = C(\bar{\Omega})$, we take
$\bar{\hil} = L^2(\Omega) \subseteq \ban^*$.
%% (we can ignore the
%% boundary since is has zero measure).
Then
%
\begin{equation*}
  \ban \subseteq \bar{\hil} \subseteq \ban^*,
\end{equation*}
since members of $C(\bar{\Omega})$ are bounded, hence square integrable. The
unit norm constraint can be written using \eqref{eq:obs*} as a series of $m$ equality
constraints (one for each measurement) on $\obs$ as
\begin{align}\label{eq:constraints}
  \phi_j(\obs) :=\frac12 \| \obs^* e_j\|_{\bar{\hil}}^2 - \frac12 = 0,j=1,\dots,m
\end{align}
with $e_j \in \R^m$ the $j$th standard basis vector. The first variations are:
\begin{align*}
  \delta \phi_j(\obs)V  
  &= \frac12\lim_{\tau \to 0}\tau^{-1}
  ( \|(\obs + \tau V)^*e_j \|_{\bar{\hil}}^2 - \|\obs ^*e_j \|_{\bar{\hil}}^2  ) \\
  %
  %
  %
  &= \frac12\lim_{\tau \to 0}\tau^{-1}
  ( \langle (\obs + \tau V)^*e_j, (\obs + \tau V)^*e_j \rangle_{\bar{\hil}} - 
  \langle \obs^*e_j, \obs^*e_j \rangle_{\bar{\hil}} ) \\
  % 
  % 
  %
  &= \frac12\lim_{\tau \to 0}\tau^{-1}
  (2\tau \langle \obs^*e_j,V^*e_j \rangle_{\bar{\hil}} 
  +\tau^2 \langle V^*e_j, V^*e_j \rangle_{\bar{\hil}} ) \\
  %
  %
  % 
  &= \langle \obs^*e_j,V^*e_j \rangle_{\bar{\hil}} \\
  %
  %
  % 
  &= \langle V \obs^*e_j,e_j \rangle_{\R^m} \\
  %
  %
  %
  &= e_j^t V \obs^* e_j \\
  % 
  %
  %
  &= \tr{V \obs^* e_je_j^t}.
\end{align*}
Using the same arguments we used to justify the definition of
\eqref{eq:tar grad}, we conclude that
\begin{align}\label{eq:grad constraints}
\nabla \phi_j(\obs) = \obs^* e_j e_j^t = \meas_j e_j^t , j=1,\dots,m,
\end{align}
with $\nabla \phi_j(\obs) \in \bar{\hil}^m \subseteq \ban^m$ and where the
last equality follows since $\obs^*e_j = \meas_j$ by \eqref{eq:obs*}. As
we noted at the end of section \ref{section:D and grad}, the
gradient $\nabla \phi_j(\obs)$ is a row vector.

\subsection{Necessary Conditions for Optimal Design}\label{subsec:necessary}
Necessary conditions for optimality are found using Lagrange
multipliers:
\begin{align}
  %\begin{split}
  &\nabla \tar(\obs) = \sum_{j=1}^m \xi_j \nabla \phi_j (\obs)
  \label{eq:Lagrange mult1} \\
    &\phi_j(\obs) = 0, j = 1,\dots,m. \label{eq:Lagrange mult2}
  %\end{split}
\end{align}
%% This means that any optimal design satisfies \eqref{eq:Lagrange mult1}
%% and \eqref{eq:Lagrange mult2}.
Recall where the objects involved are defined: the observation operator,
$\obs \in \bar{\hil}^m \subseteq (\ban^*)^m$. The objective $\tar:
(\ban^*)^m \to \R$ and its gradient is $\nabla \tar(\obs) \in
\ban^m$. The constraints are defined similarly --- $\phi_j: (\ban^*)^m
\to \R$ and $\nabla \phi_j(\obs) \in \ban^m$.

Focusing on \eqref{eq:Lagrange mult1} and using the gradients of the
objective and constraints \eqref{eq:tar grad} and \eqref{eq:grad
  constraints} we have:
\begin{equation*}
  (I - \modcov \obs^* \Sigma^{-1} \obs) \fwd \postcov \fwd^* \obs^*\Sigma^{-1}
  = \sum_{j=1}^m \xi_j \obs^* e_je_j^t = (\xi_1 \meas_1,\dots,\xi_m \meas_m).
\end{equation*}
If we let $\Xi = \diag(\xi_j)$, then with some abuse of
notation, we can write this more compactly as:
\begin{equation}\label{eq:conditions}
  ( I - \modcov \obs^* \Sigma^{-1} \obs) \fwd \postcov \fwd^* \obs^*  \Sigma^{-1}
  = \obs^* \Xi.
\end{equation}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION Analysis of Optimal Designs --- Vanishing Model Error
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis of Optimal Designs --- Vanishing Model Error}\label{section:vanishing}
\subsection{Overview}
In this section we analyze optimal designs when no model error is
present. We show how sensor clusterization arises in section
\ref{subsec:clusterization}. We prepare the ground by first deriving
an eigenvalue problem in $\obs$ for an optimal design in section
\ref{subsec:eigenproblem}. This eigenvalue problem is recorded in
\eqref{eq:mod conditions}. Next, we characterize optimal designs as
ones that make all eigenvalues in equal in the above mentioned
eigenproblem. This is crucial for clusterization to occur. The
derivation is presented in section \ref{subsec:characterization}.


\subsection{The Eigenvalue Problem}\label{subsec:eigenproblem}
When $\modcov = 0$, then $\Sigma= \sigma^2I$. The necessary
first-order condition for optimality \eqref{eq:conditions} becomes
\begin{equation}\label{eq:eigenproblem}
  \sigma^{-2}\fwd \postcov \fwd^* \obs^* = \obs^* \Xi, 
\end{equation}
with $\Xi$ diagonal. This is an eigenvalue problem for the
self-adjoint operator $\sigma^{-2}\fwd \postcov \fwd^*$ where the rows
of $\obs$, namely $\meas_j,j=1,\dots, m$, are the eigenvectors. Below,
we use \eqref{eq:postcov} in the second equality. If $\fwd$ is
invertible, the third equality is trivial. Otherwise, we use some
calculations delegated to lemma \ref{lemma:twice woodbury} in the
appendix:
\begin{align}\label{eq:mod conditions}
  \begin{split}
  \obs^* \Xi &= \sigma^{-2}\fwd \postcov \fwd^* \obs^*  \\
  %
  %
  %
  &= \sigma^{-2} \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* \obs^* \obs \fwd )^{-1} \fwd^* \obs^* \\
  %
  %
  %
  &= \sigma^{-2} \left ( (\fwd\prcov\fwd^*)^{-1} + \sigma^{-2}  \obs^* \obs \right )^{-1} \obs^*
  \end{split}
\end{align}  
This presents the optimal design problem as a problem on $\ban$ and
not on $\hil$ --- the term $(\fwd \prcov \fwd^*)^{-1}$ is the prior
precision in $\ban$ and $(\fwd\prcov\fwd^*)^{-1} + \sigma^{-2} \obs^*
\obs$ is the posterior precision in $\ban$.

\subsection{Characterizing D-Optimal Designs}\label{subsec:characterization}
In light of \eqref{eq:mod conditions}, let us denote eigenvalues of
$\fwd \prcov \fwd^*$ by $\lambda_1 \geq \lambda_2 \geq \dots$ and its
eigenvectors $\{\ev_i\}_{i=1}^{\infty}$. The following lemma (proved
in the appendix \ref{lemma:sim diag}) will be useful:

\begin{restatable*}{lemma}{simdiag}\label{lemma:sim diag}
  Let $C:\hil \to \hil$ self-adjoint and let $\func_1,\dots,\func_m \in
  \hil$. Denote $\func^*$ the element $\func$ acting as a linear
  functional. If
  \begin{equation*}
   (C + \sum_{j=1}^m \func_j\func_j^*) \func_k = \xi_k \func_k, k = 1,\dots,m
  \end{equation*}
  then $C$ and $\sum_{j=1}^m \func_j \func_j^*$ are simultaneously
  diagonalizable.
\end{restatable*}

Thus, if we take $\func_j^{*} = \meas_j$ and $C := (\fwd \prcov
\fwd^*)^{-1}$ in the above lemma, we conclude from the eigenproblem
\eqref{eq:mod conditions} that $\obs^*\obs$ has the same eigenvectors
as $\fwd \prcov \fwd^*$, namely $\{\ev_i\}_{i=1}^{\infty}$ (infinitely
many of them with zero eigenvalue, since $\obs^*\obs$ is
finite-rank). Denote the corresponding non-zero eigenvalues of
$\obs^*\obs$ by $\{\eta_i\}_{i=1}^{k}$ and let $\eta_i = 0$ for $i
\geq k+1$.

\begin{align}\label{eq:pre manipulations}
  \begin{split}
    \det \left (I + \sigma^{-2} \prcov^{1/2} \fwd ^* \obs^*
    \obs \fwd \prcov^{1/2}\right ) &=
    %
    \det \left (I + \sigma^{-2} \obs^*
    \obs \fwd \prcov\fwd^* \right ) \text{(Sylvester's determinant theorem)}\\
    %
    %
    %
    &= \det \left \{ \left ( (\fwd \prcov \fwd^*)^{-1} +
    \sigma^{-2}\obs^*\obs \right ) (\fwd \prcov \fwd^*)\right \}\\
  \end{split}
\end{align}


By definition of the objective \eqref{eq:objective} and using
\eqref{eq:pre manipulations}:
\begin{align}\label{eq:mod objective}
  \begin{split}
    \tar ( \obs ) &=\frac12 \log \det \left( I + \sigma^{-2}
    \prcov^{1/2} \fwd ^* \obs^* \obs \fwd \prcov^{1/2} \right )\\
    %
    %
    %
    %
    &=\frac12 \log \det \Big ( \left ( (\fwd \prcov \fwd^* )^{-1} +
    \sigma^{-2}\obs^* \obs \right ) \fwd \prcov \fwd^* \Big )\\
    %
    %
    %
    &=\frac12 \log \left (\prod_{i=1}^{\infty} ( \lambda_i^{-1} + \sigma^{-2} \eta_i ) \prod_{i=1}^{\infty} \lambda_i \right ) \\
    %
    %
    %
    &=\frac12 \log \left ( \prod_{i=1}^{k} ( \lambda_i^{-1} + \sigma^{-2} \eta_i )\prod_{i=1}^{k} \lambda_i \right ). 
  \end{split}
\end{align}
Therefore, an optimal design maximizes

\begin{equation}\label{eq:true target}
  \sum_{i=1}^{k}\log (\lambda_i^{-1} + \sigma^{-2} \eta_i),
\end{equation}

\pgfplotstableread{
  Label    prior  $o_1$  $o_2$   $o_3$   $o_4$   $o_5$         $o_6$    topper
  1         0.2    0.8    0.5     0.5    0.4    0.33333333  0.3333333  0.001
  2         0.8    0.2    0.5     0.5    0.4    0.33333333  0.3333333  0.001
  3         2.2    0      0       0      0.2    0.33333333  0.3333333  0.001
  4         3.5    0      0       0      0      0           0          0.001
}\testdata

\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[
        ybar stacked,
        ymin=0,
        ymax=4,
        xtick=data,
        legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
        reverse legend=false, % set to false to get correct display, but I'd like to have this true
        xticklabels from table={\testdata}{Label},
        xticklabel style={text width=2cm,align=center},
        legend plot pos=right,
        ylabel=precision --- prior and posterior,
        xlabel=eigenvector,
      ]
    
      
      \addplot [fill=green!80]  table [y=prior, meta=Label, x expr=\coordindex] {\testdata};
      \addplot [fill=blue!60]   table [y=$o_1$, meta=Label, x expr=\coordindex] {\testdata};
      \addplot [fill=red!60]    table [y=$o_2$, meta=Label, x expr=\coordindex] {\testdata};
      \addplot [fill=black!60]  table [y=$o_3$, meta=Label, x expr=\coordindex] {\testdata};
      \addplot [fill=orange!60] table [y=$o_4$, meta=Label, x expr=\coordindex] {\testdata};
      \addplot [fill=cyan!60]   table [y=$o_5$, meta=Label, x expr=\coordindex] {\testdata};
      \addplot [fill=purple!60] table [y=$o_6$, meta=Label, x expr=\coordindex] {\testdata};

      
      \addlegendentry{prior}
      \addlegendentry{$o_1$}
      \addlegendentry{$o_2$}
      \addlegendentry{$o_3$}
      \addlegendentry{$o_4$}
      \addlegendentry{$o_5$}
      \addlegendentry{$o_6$}   
    \end{axis}
  \end{tikzpicture}
  \caption{Posterior precision per eigenvector, after taking a
    measurement $\meas_i$. Each $\meas_i$ increases precision
    uniformly across the lowest precision eigenvectors. We see that
    $\meas_2$ and $\meas_3$ are repeated measurements, as well as
    $\meas_5$ and $\meas_6$.}
  \label{fig:clusterization}
\end{figure}
with the norm constraints on $\meas_j,j=1,\dots,m$. We show in the
appendix (Lemma \ref{lemma:free}) we can find $\meas_1,\dots,\meas_m$
such that $\{\eta_i\}_{i=1}^k$ have any value we desire, subject only
to the restriction that we do not change the trace of $\obs^*\obs$, so
that $\sum_{i=1}^k \eta_i = m$. By concavity of $\log$, the fastest
increase in the design criterion (equation \eqref{eq:true target}) is
gained by increasing $\lambda_i^{-1} + \sigma^{-2}\eta_i$ where it is
smallest. So, no weight should be given to (i.e. measurement taken of)
the $k$th eigenvector before $\lambda_i^{-1} + \sigma^{-2}\eta_i \geq
\lambda_k^{-1},\ \forall i \neq k$. Thus, we should choose
$\{\eta_i\}_{i=1}^k$ as follows. We would increase $\lambda_1^{-1} +
\sigma^{-2} \eta_1$ until it equals $\lambda_2^{-1}$. Then,
$\lambda_i^{-1} + \sigma^{-2} \eta_i,i=1,2$ are increased until
$\lambda_i^{-1} + \sigma^{-2} \eta_i = \lambda_3^{-1},i=1,2$ and so
forth.

The argument above has the important consequence that the eigenvalues
in \eqref{eq:eigenproblem} are all equal This makes perfect
sense. First, we know the Lagrange multipliers tell us how much we
gain by relaxing the constraints and there is no reason for any
measurement to give us more than any other (otherwise we would put
more weight on the better measurement, as can be understood from the
discussion in the following paragraph).%%  We are also used to symmetric
%% optimization problems having symmetric solutions \cite{Waterhouse83},
%% so symmetric and identical Lagrange multipliers definitely make sense
%% in this context.

\subsection{Sensor Clusterization}\label{subsec:clusterization}
The way clusterization can occur is best understood from figure
\ref{fig:clusterization}. We see that each measurement increases
precision (reduces uncertainty) only for the eigenvectors of lowest
precision. If the precision in these is lower than the precision for
the next eigenvector, a repeated measurement is optimal.

Results of section \ref{subsec:characterization} specifically, the
discussion at the end of said section, along with figure
\ref{figure:clusterization} and equation \eqref{eq:true target} show
how clusterization arises when an optimal design is saught.










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION Analysis of Optimal Designs --- Non-Vanishing Model Error
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis of Optimal Designs --- Non-Vanishing Model Error}
\subsection{Overview}
In this section we show how clusterization is avoided when model
error does not vanish.

\subsection{Derivation}
We rely on corollary \ref{cor:same meas} below, which is proved in the
appendix. Denote $\obs = (\meas_1,\dots,\meas_m)^t$ and $\obsm :=
(\meas_1,\dots,\meas_{m-1})^t$.
\begin{restatable*}{corollary}{samemeas}\label{cor:same meas}.
  If $\meas_m = \meas_j$ for some $1 \leq j \leq m-1$, then
  \begin{equation*}
    \tar(\obs) - \tar(\obsm) =
    \log \left ( 1 + \frac{\sigma^2
      \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1} e_j,
      \obsm^* \Sigmam^{-1}e_j \rangle
    }{
      2 - \sigma^2 e_j^t\Sigmam^{-1}e_j 
    }       
    \right ).
  \end{equation*}
\end{restatable*}
 In
contrast to the previous case of vanishing model error, we will see
that if $\modcov \neq 0$ clustering will not occur. From Corollary
\ref{cor:same meas} we can observe that

$$
\lim_{\sigma^2 \to 0} \tar(\obs) -\tar(\obsm) = 0
$$
Hence no increase in the design criterion is
achieved. This is clearly sub-optimal and $\obsm$ cannot be an optimal
design. We may conclude that for small measurement error levels, the
clusterization effect will be mitigated by the presence of a non-zero
model error. Since the design criterion is not defined for $\sigma^2 =
0$ and identical measurements, we cannot make a statement regarding
$\sigma^2 = 0$, except in the limiting sense described above.

\section{Conclusion}\label{section:conclusion}
Our relaxed model gives us insight to D-optimal designs. We see that
in our setting, uncertainty is not reduced in any direction
(eigenvector) before it is the largest uncertainty present. We show
how and why repeated measurements can give rise to a D-optimal
design. There is more work to be done in understanding exactly what
causes such designs to be strictly better than others.


\section{Acknowledgements}
This study is a part of my PhD thesis \cite{mine} which was written
under the instruction of Prof. Georg Stadler in New York University's
Courant Institute. I would like to thank him for his great mentorship.

\appendix
\section{Widely Applicable Lemmas}

The following lemma is generalized from \cite[Chapter 9,
  Theorem 4, pp. 127]{Lax97}
\lax
\begin{proof}
  Consider a differentiable operator-valued function $X(t)$ such that
  $X(0) = 0$ and $X(t)$ is positive, self-adjoint and trace-class for
  every $t\in \R$. We denote the eigenvalues of this operator by
  $\lambda_k(X(t))$ and sometimes drop the dependence on $X(t)$, so
  $\lambda_k = \lambda_k(X(t))$.  Then $\det (I+X(t)) =
  \prod_{k=1}^{\infty} (1+\lambda_k) < \infty$ where the bound holds
  by the arguments given in \cite{AlexanderianGloorGhattas14}. The
  full derivative is
  \begin{align*}
    \frac{\der \det (I+X(t))}{\der t} 
    % 
    % 
    % 
    &= \sum_{k=1}^{\infty} 
    \frac{\partial \det (I+X(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    % 
    %
    %
    &= \sum_{k=1}^{\infty} \frac{\partial \prod_{l=1}^{\infty}
      (1+\lambda_l(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    %
    %
    %
    &= \sum_{k=1}^{\infty} \frac{\prod_{l=1}^{\infty}
      (1+\lambda_l(s))}{(1+\lambda_k)}\Big |_{s=t}
    \dot{\lambda_k}(X(t)) \\
    % 
    % 
    % 
    &= \sum_{k=1}^{\infty} \frac{\det (I+X(t))}{1 +\lambda_k} \dot{\lambda_k}(X(t)).
  \end{align*}
  The assumption $X(0) = 0$ means $\lambda_k(X(0)) = 0,\ \forall k \geq 1$. Thus:
  \begin{align*}
    \frac{\der (I+\det X(t))}{\der t}\Big |_{t=0} 
    = \sum_{k=1}^{\infty} \dot{\lambda_k}(X(0)) 
    = \frac{\der }{\der t}\tr{X(0)}
    = \tr{\dot{X}(0)},
  \end{align*}
  where the second equality follows by monotone convergence. 
  Let $Y(t)$ a trace-class self-adjoint operator such that 
  $I+Y(t)$ is invertible.
  Define $X(t)$ via $I+X(t) = (I+Y(0))^{-1/2} (I+Y(t)) (I+Y(0))^{-1/2}$. 
  We show $X(t)$ satisfies the conditions above. It is trace-class:
  \begin{align*}
    \tr{X(t)} = \tr{(I+Y(0))^{-1} (I+Y(t)) - I}
    \leq \tr{I+Y(t) - I}< \infty,
  \end{align*}
  since $Y(t)$ is trace-class. It is also clear that
  $X(0) = 0$ and $X(t)$ is self-adjoint.
  $I+Y(t) = (I+Y(0))^{1/2}(I+X(t))(I+Y(0))^{1/2}$, so
  \begin{align*}
    \frac{\der \det (I+Y(t))}{\der t}|_{t=0} 
    &= \det (I+Y(0))\frac{\der \det (I+X(t))}{\der t}\Big |_{t=0} \\
    % 
    % 
    % 
    &= \det (I+Y(0)) \tr{\dot{X}(0)} \\
    % 
    % 
    % 
    &= \det (I+Y(0)) \tr{(I+Y(0))^{-1} \dot{Y}(0)}.
  \end{align*}
  Consequently, by the one-variable chain rule:
  \begin{align*}
    \frac{\der \log \det (I+Y(t))}{\der t}\Big |_{t=0} &=
    % 
    % 
    % 
    \frac{1}{\det (I+Y(0))}\frac{\der \det (I+Y(t))}{\der t}\Big |_{t=0} \\ 
    % 
    % 
    % 
    &= \tr{ (I+Y(t))^{-1} \dot{Y}(t)} \big |_{t=0}.
  \end{align*}
  Since there is nothing special about $t=0$, the relation holds for all $t$.
\end{proof}


\begin{lemma}[Matrix Determinant Lemma in Hilbert Spaces]\label{lemma:MDL}
  Let $\hil$ a separable Hilbert space, $u,v\in \hil$ and $A: \hil \to
  \hil$ an invertible linear operator such that $\tr{A-I} <
  \infty$. Then $\det A$ and $\det A + uv^*$ are well defined and
  \begin{equation*}
    \det (A + uv^*) = (1 + \langle A^{-1} u, v \rangle ) \det A,
  \end{equation*}
  where $(A + uv^*)w := Aw + \langle v,w \rangle u$.
\end{lemma}
\begin{proof}
  In this proof we rely on definitions and results from
  \cite{Simon77}. First, consider $B := I + xy^*$ for some $x,y \in
  \hil$. We construct an eigenbasis for $B$ and use that to show $\det
  B = 1 + \langle x, y \rangle$. First let $x_1 := x$.  Now, if $x
  \parallel y$, take $\{x_n \}_{n=2}^{\infty}$ an orthogonal basis for
  $span\{x_1\} ^{\perp}$. If, on the other hand, $x \nparallel y$, let
  \begin{equation*}
    x_2 := x - \frac{ \langle x, y\rangle}{\|y\|^2}y
  \end{equation*}
  and it is easy to verify that $x_2 \perp y$ and $span \{x,y\} = span
  \{x_1,x_2\}$. Take $\{x_n \}_{n=3}^{\infty}$ an orthogonal basis for
  $span\{x_1,x_2\} ^{\perp}$. In both cases,
  \begin{equation*}
    B x_n =
    \begin{cases}
      (1 + \langle x, y \rangle) x_n & n = 1 \\
      x_n                            & n \neq 1,
    \end{cases}
  \end{equation*}
  and so $\det B = 1 + \langle x, y \rangle$.
  
  It is easy to verify that $uv^*$ is trace-class and since $\tr{A-I}
  < \infty$, also $\tr{A + uv^* - I} < \infty$ (sum of two trace-class
  operators is trace-class). Thus $\det A$ and $\det (A+uv^*)$ are
  well defined. Let $x:=A^{-1}u$ and $y := v$:
  \begin{equation*}
    \det (A + uv^*) = \det A \ \det(I+A^{-1}uv^*) =
    (1 + \langle A^{-1}u, v \rangle) \det A .
  \end{equation*}
\end{proof}





\section{Specific Lemmas}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}\label{lemma:twice woodbury}
  Assume $\fwd \prcov \fwd^*$ is invertible. Then
\begin{align*}
  \begin{split}
    \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* \obs^* \obs \fwd )^{-1} \fwd^* 
    %
    %
    = \left ( (\fwd\prcov\fwd^*)^{-1} + \sigma^{-2}  \obs^* \obs \right )^{-1},
  \end{split}
\end{align*}  
\end{lemma}
\begin{proof}
  The proof is boring and amounts to using Woodbury's matrix identity
  twice and a regularization trick. The standard proof for this
  identity works in infinite dimensions, as long as all terms are well
  defined. Unfortunately, $\obs^*\obs$ is not invertible, so we force
  it to be. Throughout, we let $X := \fwd\prcov \fwd^*$ and $Y_{\eps}
  := \obs^*\obs + \eps I$, for some small $\eps > 0$. Note that now
  $Y_{\eps}$ is invertible. Recall Woodbury's matrix identity:
  $$
  (A + UCV)^{-1} = A^{-1} + A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}.
  $$

  Taking $A := \prcov^{-1}, U := \fwd^*, V := \fwd$ and $C :=
  \sigma^{-2} (\obs^*\obs+\eps I)$ we get:
  \begin{align*}
    \begin{split}
      \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\eps I) \fwd )^{-1}\fwd^* &=
      \fwd ( \prcov - \prcov \fwd^* ( \sigma^2(\obs^*\obs + \eps I)^{-1} + \fwd \prcov \fwd^* )^{-1} \fwd \prcov ) \fwd^* \\
      %
      %
      &= X - X(\sigma^2Y_{\eps}^{-1} + X)^{-1}X
    \end{split}
  \end{align*}
  Note that $X + \sigma^2 Y_{\eps}^{-1}$ is invertible, as the sum of
  a two positive definite operators. Now, taking $A := X^{-1}, C :=
  \sigma^2Y_{\eps}^{-1}, U := I$ and $V := I$ and using Wodbury's matrix
  identity in reverse order:
  \begin{align*}
    \begin{split}
      X - X(\sigma^2Y_{\eps}^{-1} + X)^{-1}X &= (X^{-1} + \sigma^{-2}Y_{\eps})^{-1} \\
      %
      %
      %
      &= (\fwd \prcov \fwd^* + \sigma^{-2} (\obs^*\obs + \eps I))^{-1}.
    \end{split}
  \end{align*}

  We conclude that $\forall \eps > 0$
  \begin{align*}
    \begin{split}
      \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\eps I) \fwd )^{-1}\fwd^* 
     &= (\fwd \prcov \fwd^* + \sigma^{-2} (\obs^*\obs + \eps I))^{-1}.
    \end{split}
  \end{align*}
  Letting $\eps \to 0$ completes the proof.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Increase due to a Measurement]\label{lemma:design increase}
  Let $\obs = (\meas_1,\dots,\meas_m)^t$ and $\obsm := (\meas_1,\dots,\meas_{m-1})^t$. Then
  \begin{align*}
    \tar( \obs ) - \tar (\obsm ) &=
    \frac12 \log \left ( 1 + \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m \rangle
    }{
      \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
    }       
    \right ).
  \end{align*}
\end{lemma}
\begin{proof}
  We use the Schur complement to write one inverse in terms of the other and
  introduce notations to make the derivation cleaner. Note that we think of
  $\obsm$ and $\obsm^*$ as column and row vectors (respectively).
  \begin{align*}
    \Sigma( \obs ) &= \Sigma = 
    \begin{bmatrix}
      \Sigma (\obsm )           & \obsm \modcov \meas_m \\
      \meas_m \modcov \obsm^*   & \sigma^2 + \meas_m \modcov \meas_m
    \end{bmatrix}
    =
    \begin{bmatrix}
      \Sigmam   & w \\
      w^t       & c
    \end{bmatrix}\\
    %
    %
    %
    \Sigma^{-1} &=
    \begin{bmatrix}
      \Sigmam^{-1} + \Sigmam^{-1} w ( c - w^t \Sigmam^{-1} w)^{-1} w^t \Sigmam^{-1} & - \Sigmam^{-1} w ( c - w^t \Sigmam^{-1} w)^{-1} \\
      -( c - w^t \Sigmam^{-1} w)^{-1} w^t \Sigmam^{-1}                            &  ( c - w^t \Sigmam^{-1} w)^{-1}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      \Sigmam^{-1} & 0 \\
      0           & 0 
    \end{bmatrix}
    + (c -w^t \Sigmam^{-1} w )^{-1}
    \begin{bmatrix}
      \Sigmam^{-1} w \\
      -1
    \end{bmatrix}
    \begin{bmatrix}
      w^t \Sigmam^{-1} & -1 
    \end{bmatrix}
  \end{align*}
  %
  Further, define
  %
  \begin{align*}
    \M (\obs ):&= \prcov^{\frac12}\fwd^* \obs^* \Sigma^{-1} \obs \fwd
    \prcov^{\frac12}    
  \end{align*}
  %
  and note that, using our understanding of what is a column vector and
  what is a row vector:
  %
  \begin{align*}
    \M(\obs) &= \prcov^{1/2} \fwd^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2} \\
    %
    %
    %
    &= \prcov^{1/2} \fwd^* \obs^* \left \{
    \begin{bmatrix}
      \Sigmam^{-1} & 0 \\
      0           & 0 
    \end{bmatrix}
    + (c -w^t \Sigmam^{-1} w )^{-1}
    \begin{bmatrix}
      \Sigmam^{-1} w \\
      -1
    \end{bmatrix}
    \begin{bmatrix}
      w^t \Sigmam^{-1} & -1 
    \end{bmatrix} 
    \right \} \obs \fwd \prcov^{1/2} \\
    %
    %
    %
    &= \M (\obsm) + (c -w^t \Sigmam^{-1} w )^{-1}
    \prcov^{1/2} \fwd^* \obs^*
    \begin{bmatrix}
      \Sigmam^{-1} w \\
      -1
    \end{bmatrix}
    \begin{bmatrix}
      w^t \Sigmam^{-1} & -1 
    \end{bmatrix} 
    \obs \fwd \prcov^{1/2}
  \end{align*}
  %
  Now, denote:
  %
  \begin{align}\label{eq:u}
    \begin{split}
      u :&= (c -w^t \Sigmam^{-1} w )^{-1/2}
      \prcov^{1/2} \fwd^* \obs^* 
      \begin{bmatrix}
        \Sigmam^{-1} w \\
        -1 
      \end{bmatrix} \\
      %
      %
      %
      & = (c -w^t \Sigmam^{-1} w )^{-1/2} ( \prcov^{1/2}\fwd^* \obsm^* \Sigmam^{-1} \obsm  \modcov \meas_m - \prcov^{1/2} \fwd^* \meas_m )\\
      %
      %
      %
      u^* :&=  (c -w^t \Sigmam^{-1} w )^{-1/2} (\meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \fwd \prcov^{1/2} - \meas_m \fwd \prcov^{1/2} ),
    \end{split}
  \end{align}
  %
  so that
  %
  \begin{equation}\label{eq:M plus I}
    I + \M( \obs ) = I + \M (\obsm ) + uu^*.
  \end{equation}
  %
  Note that
  \begin{equation}\label{eq:M postcov}
    \prcov^{1/2} \left (I + \M( \obsm ) \right )^{-1} \prcov^{1/2} = \postcovm.
  \end{equation}
  The increase in the design criterion gained by including $\meas_m$
  in the design is found by using Lemma \ref{lemma:MDL} the above
  results:
  %
  \begin{align*}
    \tar( \obs ) - \tar( \obsm )
    %
    %
    %
    &= \frac12 \log \det \Big ( I + \M ( \obs ) \Big ) / \det \Big ( I + \M (\obsm) \Big ) \\
    %
    %
    %
    &= \frac12  \log \det \left ( I + \M(\obsm) + uu^* \right ) / \det \Big ( I + \M (\obsm) \Big ) \\
    %
    %
    %
    &= \frac12 \log \left ( 1 + \left \langle \left ( I+\M(\obsm) \right )^{-1} u, u  \right \rangle \right ).
  \end{align*}
  Using \eqref{eq:u}:
  \begin{align*}
    &\left \langle \left (I+\M (\obsm)\right )^{-1}u, u \right \rangle\\
    &= \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m \rangle
    }{
      c- w^t \Sigmam^{-1} w
    }\\
    %
    %
    %
    &= 
    \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m \rangle
    }{
      \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
    }
  \end{align*}
  and the conclusion follows.
\end{proof}

Lemma \ref{lemma:design increase} implies the following two corollaries.
\begin{corollary}[Gain for No Model Error]\label{cor:zero mod err}
  If $\modcov = 0$, then
  \begin{equation*}
    \tar( \obs ) - \tar (\obsm )
    = -\frac12 \log (1 - \sigma^{-2} \langle \fwd \postcov \fwd^* \meas_m, \meas_m \rangle ).
  \end{equation*}
\end{corollary}
\begin{proof}
  Note that this is not immediate by substituting $\modcov = 0$ in the
  conclusion of Lemma \ref{lemma:design increase}, since we make a
  claim for $\postcov$, and not $\postcovm$. Let us first review
  \eqref{eq:u} and note that since $\modcov = 0$ the covariance
  $\Sigmam = \sigma^2I_{m-1}$ and $w = 0$, so $c - w^t\Sigmam^{-1}w =
  \sigma^2$:
  \begin{align*}
    u :&=-\sigma^{-1}\prcov^{1/2} \fwd^* \meas_m\\
    %
    %
    u^* :&= -\sigma^{-1} \meas_m \fwd \prcov^{1/2}.
  \end{align*}
  From \eqref{eq:M plus I}:
  \begin{equation*}
    I + \M(\obsm) = I +\M(\obs) - uu^*,
  \end{equation*}
  and thus:
  \begin{equation*}
    \left \langle \left ( I +\M(\obs) \right )^{-1} u, u \right \rangle
    = \sigma^{-2} \langle \fwd \postcov \fwd^* \meas_m, \meas_m \rangle.
  \end{equation*}
  Analogously to \eqref{eq:M postcov} we note that
  \begin{equation*}
    \prcov^{1/2} \left ( I + \M(\obs) \right )^{-1} \prcov^{1/2} = \postcov.
  \end{equation*}
  Using Lemma \ref{lemma:MDL} we conclude
  \begin{align*}
    \tar( \obs ) - \tar( \obs )
    &= \frac12 \log \det \left (I +\M(\obs) \right ) / \det \left (I + \M(\obsm) \right ) \\
    %
    %
    %
    &= \frac12 \log \det \left (I +\M(\obs) \right ) / \det \left (I + \M(\obs) - uu^* \right ) \\
    %
    %
    %
    &=-\frac12 \log (1 - \langle (I+\M(\obs))^{-1}u, u \rangle \\
    %
    %
    %
    &= -\frac12 \log (1 - \sigma^{-2} \langle \fwd \postcov \fwd^* \meas_m, \meas_m \rangle ).
  \end{align*}
\end{proof}

\samemeas
\begin{proof} \label{cor:same meas proof}
  Denote $A:= \obs \modcov \obs^*$ and $v_j$ the $j$th column of $A$.
  Note that $v_j = \obsm \modcov \meas_m$, since $(\obsm \modcov
  \obsm^*)_{ij} = \meas_i(\modcov \meas_j)$, as explained in
  \eqref{eq:modcov explained}. One can now verify that
  \begin{equation}\label{eq:observation}
    \Sigmam^{-1} \obsm \modcov \meas_m = \Sigmam^{-1}v_j = (A +\sigma^2I_{m-1})^{-1} v_j =
    e_j -\sigma^2 \Sigmam^{-1}e_j.
  \end{equation}
  %
  Using \eqref{eq:observation}:
  \begin{align}\label{eq:denominator}
    \begin{split}
      \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m
      &= \meas_m \modcov \obsm^* ( e^j - \sigma^2 \Sigmam^{-1} e_j )\\
      %
      %
      %
      &= \meas_m \modcov \meas_j - \sigma^2 \meas_m \modcov \obsm^* \Sigmam^{-1}e_j \\
      %
      %
      %
      &= \meas_m \modcov \meas_j -\sigma^2 (e_j - \sigma^2 \Sigmam^{-1}e_j)^t e_j \\
      %
      %
      %
      &= \meas_m \modcov \meas_m -\sigma^2 + \sigma^4 e_j^t\Sigmam^{-1}e_j.
    \end{split}
  \end{align}
  We use \eqref{eq:observation} to simplify the terms in the enumerator of
  the conclusion of Lemma \ref{lemma:design increase}:
  \begin{align}\label{eq:enumerator}
    \begin{split}
      (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m
      &= \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m - \meas_m \\
      %
      %
      %
      &= \obsm^* (e_j - \sigma^2 \Sigmam^{-1} e_j) -\meas_j \\ 
      %
      %
      %
      &= -\sigma^2 \obsm^* \Sigma^{-1}e_j. 
    \end{split}
  \end{align}
  %
  Substitute \eqref{eq:enumerator} and \eqref{eq:denominator} to
  the enumerator and denominator (respectively) of the conclusion of
  Lemma \ref{lemma:design increase}:
  %
  \begin{align*}
    \tar( \obs ) - \tar (\obsm ) &=
    \log \left ( 1 + \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m \rangle
    }{
      \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
    }       
    \right ) \\
    %
    %
    %
    &= \log \left ( 1 + \frac{\sigma^4
      \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1} e_j,
      \obsm^* \Sigmam^{-1}e_j \rangle
    }{
      2\sigma^2 - \sigma^4 e_j^t\Sigmam^{-1}e_j 
    }       
    \right ) \\
    %
    %
    %
    &= \log \left ( 1 + \frac{\sigma^2
      \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1} e_j,
      \obsm^* \Sigmam^{-1}e_j \rangle
    }{
      2 - \sigma^2 e_j^t\Sigmam^{-1}e_j 
    }       
    \right ).
  \end{align*}
\end{proof}

\begin{lemma}\label{lemma:free}
  Let $M \in \R^{k \times k}$ symmetric positive definite with $\tr{M} =
  m$, $m > k$. Then we can find $\func_j \in \R^k,j=1,\dots,m$ with
  $\|\func_j\|=1$ and $A = (\func_1,\dots,\func_m)$ such that $AA^t =
  M$.
\end{lemma}
\begin{proof}
  Let us diagonalize $M$, so that $M = U D U^t$ with $D =
  \diag(d_1,\dots,d_k)$ and $U \in \R^{k \times k }$ orthogonal. Let $S
  \in \R^{k \times m}$ with $S_{ii} = \sqrt{d_{i}}$ and zeros
  otherwise. Define $A:= U S V^t$, where $V \in \R^{m \times m}$ is
  orthogonal and will be further restricted later. Then $AA^t = U
  SV^tVS^t U^t = UDU^t$, so $AA^t$ has the required eigenvalues and
  eigenvectors by construction. If we can choose $V$ such that $A$
  also satisfies the unit norm constraints we are done. These
  constraints are, for $j=1,\dots,m$:
  \begin{equation}\label{eq:V constraints}
   1 = [A^tA]_{jj} = [V S^tS V^t]_{jj},
  \end{equation}
  and we can expect to do this since we assumed $\tr{D} = m$.

  Define $C = S^tS - I \in \R^{m \times m}$. Note that $\tr{C} = 0$ and
  $C$ is diagonal with non-zero entries $d_i-1,i=1,\dots,k$. It suffices
  to find $V$ orthogonal such that $V C V^t$ has zero diagonal. We
  construct such $V$ by sequentially inserting zeros in the diagonal
  and not destroying zeros we already introduced, starting from the
  last diagonal entry and moving to the first. If $c_{kk} \neq 0$ ,
  let $p < k$ such that $c_{pp}c_{kk} < 0$ (such $p$ exists because
  the trace is zero) and let $\theta \in (0,\pi)$. Define a Givens
  rotation $R^{(k)} \in \R^{m \times m}$ by
  \begin{equation*}
    r^{(k)}_{ab} :=
    \begin{cases}
      1 & a = b \neq p \text{ or } a = b \neq k \\
      \cos \theta & a = b = p  \\
     -\sin \theta & a = p, b = k\\
      \cos \theta & a = b = k \\
      \sin \theta & a = k, b = p \\ 
      0 & o.w
    \end{cases}
  \end{equation*}
  Note that conjugating a matrix by $R^{(k)}$ changes only its $k$ and
  $p$ rows and columns. Specifically, it leaves rows and columns
  $k+1,\dots,m$ unchanged. We want to choose $\theta$ such that
  \begin{equation}\label{eq:mm}
    0 = [R^{(k)} C (R^{(k)})^t]_{kk} = \cos^2 \theta c_{kk} + 2\cos \theta \sin
    \theta c_{kp} + \sin^2\theta c_{pp},
  \end{equation}
  and it suffices to choose $\theta$ such that
  \begin{equation*}
    c_{kk} \cot^2 \theta + 2 c_{kp} \cot \theta + c_{pp} = 0.
  \end{equation*}
  This quadratic in $\cot\theta$ has a real solution, since
  $c_{pp}c_{kk} < 0$ by assumption and we can find $\theta \in
  (0,\pi)$ such that \eqref{eq:mm} is satisfied. We continue to find
  $R^{(k-1)}$ that leaves rows and columns $k,\dots,m$ unchanged and
  continue introducing zeros to the diagonal. The assumption $\tr{D} =
  m \Rightarrow \tr{C} = 0$ guarantees we can do that. Taking $V:=
  R^{(1)} R^{(2)} \dots R^{(k-1)}R^{(k)}$ completes the proof.
\end{proof}

\begin{lemma}[Auxilliary Calculations]\label{lemma:aux calc}
  Let $T(\obs) := \obs^* \Sigma^{-1}(\obs)\obs$, with $\Sigma(\obs)$
  defined as in \eqref{eq:Sigma}. Then:
  \begin{align*}
    \delta T(\obs)V &= V^* \Sigma^{-1} \obs 
    - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
    &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs
    + \obs^* \Sigma^{-1} V.
  \end{align*}
\end{lemma}

\begin{proof}
  We need a few supplementary calculations. First:
  \begin{align}\label{eq:der sig}
    \begin{split}
      \frac{\der}{\der \tau} \Big |_{\tau=0} \Sigma( \obs + \tau V )
      &= \frac{\der}{\der \tau} \Big |_{\tau=0} 
      (\obs + \tau V ) \modcov (\obs + \tau V )^*  + \sigma^2I\\
      % 
      % 
      % 
      &= V \modcov \obs^* + \obs \modcov V^*.
    \end{split}
  \end{align}
  By the standard trick for the derivative of an operator: 
  \begin{align*}
    0 &= \frac{\der}{\der \tau} \Big |_{\tau=0} I \\
    % 
    % 
    % 
    &= \frac{\der}{\der \tau} \Big |_{\tau=0}
    \left (\Sigma(\obs+\tau V)^{-1} \Sigma(\obs+\tau V) \right ) \\
    % 
    % 
    % 
    &= \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0} \Sigma+
    \Sigma^{-1} \frac{\der \Sigma(\obs+\tau V)}{\der \tau} \Big |_{\tau=0}\\  
    %
    %
    %
    &= \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0} \Sigma+
    \Sigma^{-1} (V\modcov \obs^* + \obs \modcov V^*) 
    \text{, by \eqref{eq:der sig}. }
  \end{align*}
  Now we can write the variation of $\Sigma^{-1}$:
  \begin{align}\label{eq:der sig inv}
    \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0}  
      &= -\Sigma^{-1} (V \modcov \obs^* + \obs \modcov V^*) \Sigma^{-1}.
    \end{align}
  % 
  Finally, we can do the calculation we need to. By Leibnitz (product) rule
  and \eqref{eq:der sig inv}:
  % 
  \begin{align*}
    \delta T(\obs) V 
    &= \frac{\der T(\obs + \tau V)}{\der \tau} \Big |_{\tau=0} \\
    %
    %
    %
    &= V^* \Sigma^{-1} \obs 
    - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
    &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs
    + \obs^* \Sigma^{-1} V,
  \end{align*}
  as required.
\end{proof}

\simdiag
\begin{proof}
  First, enumerate the eigenvalues of $C + \sum_{j=1}^m
  \func_j\func_j^*$ as $\xi_1,\dots,\xi_\ell$. Denote the
  indices of the eigenvectors corresponding to $\xi_i$
  \begin{equation*}
    S_i := \{ 1 \leq k \leq m | (C + \sum_{j=1}^m \func_j\func_j^* )\func_k = \xi_i \func_k \}.
  \end{equation*}
  Define further
  \begin{equation*}
    A_i := \sum_{k \in S_i} \func_k \func_k^*,
  \end{equation*}
  which is self-adjoint. Two observations are in order. First,
  $\sum_{j=1}^m \func_j\func_j^* = \sum_{i=1}^\ell A_i$. Second, $A_i
  \func_k = 0$ if $k\not \in S_i$, since eigenvectors of different
  eigenvalue are orthogonal. For $k \in S_i$
  \begin{equation}\label{eq:on vi}
    \xi_i \func_k = (C + \sum_{j=1}^m \func_j \func_j^* ) \func_k = (C + A_i) \func_k.
  \end{equation}
  Let $V_i := span \{\func_k \}_{k\in S_i}$. Observe that $V_i$ is
  invariant under $A_i$, by definition, and under $C$, by \eqref{eq:on
    vi}. Using \eqref{eq:on vi} again, we conclude that on $V_i$, $A_i
  = \xi_iI - C$. This immediately implied $A_i$ and $C$ have the
  same eigenvectors on $V_i$. This holds for every $1 \leq i \leq
  \ell$ and we conclude that $C$ and $A$ have the same eigenvectors.
\end{proof}

%% \begin{proposition}\label{prop:bigger better}
%%   Let $\obs = (\meas_1,\dots,\meas_m)^t$, $j \in \{1,\dots,m\}$, $\sigma^2
%%   > 0$ and $\lambda > 1$. Then $\tar(\obs)$ increases if we use
%%   $\lambda \meas_j$ in $\obs$ instead of $\meas_j$.
%% \end{proposition}
\biggerbetter
\begin{proof} 
  Fix an arbitrary $j=1,\dots,m$ and take $V:= e_j e_j^t \obs$. We see
  that for $u \in \ban$
  \begin{equation*}
    Vu = e_je_j^t (\meas_1(u),\dots,\meas_m(u) )^t = e_j \meas_j(u)
    = (0,\dots,0,\meas_j(u),0,\dots,0)^t.
  \end{equation*}
  This way, $V$ has the same $j$th entry as $\obs$ while the rest are
  set to zero. We calculate the variation in this direction and show
  it is positive --- $\delta \tar(\obs)V > 0$. This means that
  increasing the magnitude of the $j$th measurement functional
  increases $\tar(\obs)$. We start with the last line of \eqref{eq:tar
    var}, and denote $\tmp : = \fwd \postcov \fwd^*: \ban^* \to \ban$:
  \begin{align*}
     \delta \tar(\obs) V 
    &= \tr{V ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}} \\
    % 
    %
    %
    &= \tr{e_je_j^t \obs ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}} \\
    %
    % 
    %
    &= e_j^t \obs ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}e_j \\
    %
    % 
    %
    &= e_j^t ( I - \obs \modcov \obs^*\Sigma^{-1})\obs \tmp \obs^* \Sigma^{-1}e_j \\  
    % 
    %
    %
    &=  e_j^t(\Sigma-\obs \modcov \obs^*) \Sigma^{-1}\obs \tmp \obs^* \Sigma^{-1}e_j \\
    %
    %
    %
    &=\sigma^2 e_j^t \Sigma^{-1}\obs \tmp \obs^* \Sigma^{-1}e_j
    \text{ by \eqref{eq:Sigma} }\\
    %
    % 
    %
    &=\sigma^2 e_j^t \Sigma^{-1}\obs \fwd \postcov \fwd^* \obs^* \Sigma^{-1}e_j.
  \end{align*} 
  Since $\postcov$ is positive definite, $\delta \tar(\obs) V > 0$.
\end{proof}



%% HERE
%% Since a measurement that takes into account $\ker \fwd$ is
%% necessarily sub-optimal, we may assume $\fwd$ is in fact
%% invertible (it is, on $\ker \fwd ^{\perp}$) and rewrite the
%% necessary condition \eqref{eq:eigenproblem} using
%% \eqref{eq:postcov} as \begin{equation*} \sigma^{-2} \left ( (\fwd
%% \prcov \fwd^*)^{-1} + \sigma^{-2}\obs\obs^* \right )^{-1} =
%% \obs^* \Xi.  \end{equation*} By Lemma \ref{lemma:sim diag},
%% taking $C = (\fwd \prcov \fwd^*)^{-1}$ we observe that $\meas_j,
%% j=1,\dots,m$ are eigenvectors of $\fwd \prcov \fwd^*$. If they have
%% different eigenvalue, they are orthogonal. If not, this means, by
%% the common interpretation of the Lagrange multipliers, that an
%% infinitesimal increase in one will cause the same increase in
%% design criterion as others with the same eigenvalue. So if these
%% eigenvectors do not have the same angle between every pair, an
%% infinitesimal increase in one will give more increase than
%% others, which is a contradiction. So in every subspace, vectors
%% will be restricted to these forms and if there will be more
%% vectors than the dimensionality of the subspace, clusterization
%% will occur.










%% \begin{figure}
%%   \begin{tikzpicture}[thick, scale=1.4, every node/.style={scale=0.99}]
%%     \begin{axis}
%%       [
%%         xmin = 0,
%%         xmax = 3.14,
%%         xlabel = {$x$},
%%         ylabel = posterior std,
%%         ymin   = 0,
%%         %compat = 1.3,
%%         % ymax   = 130,
%%         % ytick = \empty,
%%         legend cell align=left,
%%         % legend style={at={(0.45,0.2)}}
%%         legend pos= outer north east 
%%       ]
%%       % \draw[black!30!white, thin] (50,0) -- (50,130);
%%       % 
%%       \addplot [thin, black, mark=none] table{data/stdv-trunc-sens1-var1.txt};
%%       \addlegendentry{1 sensor};
      
%%       \addplot [thin, blue, mark=none] table{data/stdv-trunc-sens2-var1.txt};
%%       \addlegendentry{2 sensors};
      
%%       \addplot [thin, red, mark=none] table{data/stdv-trunc-sens3-var1.txt};
%%       \addlegendentry{3 sensors};
      
%%       \addplot [thin, green, mark=none] table{data/stdv-trunc-sens4-var1.txt};
%%       \addlegendentry{4 sensors};
      
%%       \addplot [thin, purple, mark=none] table{data/stdv-trunc-sens5-var1.txt};
%%       \addlegendentry{5 sensors};
      
%%       \addplot [thin, cyan, mark=none] table{data/stdv-trunc-sens6-var1.txt};
%%       \addlegendentry{6 sensors};

%%       \addplot [black,  only marks, mark=x, mark size=1.5] 
%%       table{data/locs-trunc-sens1-var1.txt}; 
%%       \addplot [blue,   only marks, mark=x, mark size=1.5]
%%       table{data/locs-trunc-sens2-var1.txt}; 
%%       \addplot [red,    only marks, mark=x, mark size=1.5]
%%       table{data/locs-trunc-sens3-var1.txt};
%%       \addplot [green,  only marks, mark=x, mark size=1.5] 
%%       table{data/locs-trunc-sens4-var1.txt}; 
%%       \addplot [purple, only marks, mark=x, mark size=1.5] 
%%       table{data/locs-trunc-sens5-var1.txt}; 
%%       \addplot [cyan,   only marks, mark=x, mark size=1.5] 
%%       table{data/locs-trunc-sens6-var1.txt}; 
%%     \end{axis}
%%   \end{tikzpicture}
%%   \caption{Analogously to figure \ref{fig:clusterization
%%       illustration}, shown are posterior pointwise standard
%%     deviations. Note the exact clustering. Here the dynamics is not
%%     the one governed by the 1D heat equation but a dynamics that sets
%%     to zero all modes except the first three. This dynamics simulates
%%     a finite-rank operator as per the conditions of Theorem
%%     \ref{theorem:finite rank}. Although the conditions of the theorem
%%     allow the use of a different constraint on measurements (see
%%     section \ref{subsec:unit length}), the conclusion of the theorem
%%     still seems to hold.}
%%   \label{fig:finite rank illustration}
%% \end{figure}




%% \section{Counterexample}\label{section:counterexample}
%% Applying Lemma \ref{lemma:sim diag} to
%% \begin{equation*}
%%   (\fwd \postcov \fwd^*)^{-1} = (\fwd \prcov \fwd^*)^{-1} +
%%   \sigma^{-2}\obs^*\obs
%% \end{equation*}
%% implies $(\fwd \prcov \fwd^*)^{-1}$ and $\obs^*\obs$ have the same
%% eigenspaces. Since $\meas_j$ are eigenvectors of $\fwd \postcov
%% \fwd^*$, we may expect this implies that $\meas_j$ are also
%% eigenvectors of $(\fwd \prcov \fwd^*)^{-1}$ and assuming $(\fwd \prcov
%% \fwd^*)^{-1}$ has simple eigenvalues completes the proof. Our
%% expectations, however, are let down.

%% Here is a counterexample. Let $e_1, e_2$ denote two
%% eigenvectors of $C := (\fwd \prcov \fwd^*)^{-1}$ of different
%% eigenvalues such that $C + \obs^*\obs$ is scalar on span
%% $V:=\{e_1,e_2\}$ (we took $\sigma^2 = 1$ for simplicity). Then,
%% on $V$, we have  
%% \begin{equation*}
%%   (\fwd \prcov \fwd^*)^{-1} = diag( d_1, d_2),
%% \end{equation*}
%% and we assume $d_1 > d_2$ without loss of generality. For $\meas \in
%% \hil$, we denote $\meas^*$ its Riesz representer, making a departure
%% from the standard notation employed, where $\meas$ is usually taken to
%% be a linear functional on $\hil$. Let $\obs = (\meas_1^*, \meas_2^*,
%% \meas_3^*)^t$. In this setup, we would like that $\meas_j,j=1,2,3$ are
%% either orthogonal or parallel. We now outline construction that shows
%% this is not necessarily true.

%% We know, from Corollary \ref{cor:sim diag} that $\meas_j \in V$, so
%% let
%% \begin{align*}
%%   \meas_1 &=  c_1e_1 + c_2e_2 \\
%%   \meas_2 &= -c_1e_1 + c_2e_2 \\
%%   \meas_3 &=     e_1.
%% \end{align*}
%% Then
%% \begin{align*}
%%   \meas_1 \meas_1^*
%%   &= c_1^2e_1e_1^* + c_1c_2( e_1e_2^* + e_2e_1^*) +
%%   c_2^2 e_2e_2^* \\
%%   \meas_2 \meas_2^* &= c_1^2e_1e_1^* - c_1c_2( e_1e_2^* +
%%   e_2e_1^*) + c_2^2 e_2e_2^*.
%% \end{align*}
%% Note that
%% \begin{equation*}
%%   \obs^* \obs = \meas_1 \meas_1^* + \meas_2\meas_2^* +
%%   \meas_3\meas_3^* = (2c_1^2+1) e_1e_1^* + 2c_2^2e_2e_2^*,
%% \end{equation*}
%% is diaonal in $\{e_1,e_2\}$. In order for $\meas_j,j=1,2,3$ to be
%% eigenvectors of $C + \obs^*\obs$ with the same eigenvalue, $C +
%% \obs^*\obs$ must be a scalar operator on $V$. Thus the following
%% must hold:
%% \begin{align*}
%%   d_1 + 2c_1^2 + 1 &= d_2 + 2c_2^2 \\
%%   c_1^2 + c_2^2 &= 1 \text{ (arises from $\| \meas_i \| = 1$).}
%% \end{align*}
%% Equivalently,
%% \begin{equation*}
%%   \begin{bmatrix}
%%     2 & -2 \\
%%     1 & 1
%%   \end{bmatrix}
%%   \begin{bmatrix}
%%     c_1^2 \\
%%     c_2^2
%%   \end{bmatrix}
%%   =
%%   \begin{bmatrix}
%%     d_2 - d_1 - 1 \\
%%     1
%%   \end{bmatrix}.
%% \end{equation*}
%% We can always find a solution for this system, for any $d_1,d_2$.
%% However, it is necessary that either $c_1^2 = 1$ or $c_2^2 = 1$ for
%% $\meas_j, j=1,2,3$ to all be either orthogonal or parallel. The first
%% conditiion implies $d_2 - d_1 = 3$, which is impossible, since $d_1 >
%% d_2$. The second requires $d_1 - d_2 = 1$, a condition that does not
%% hold generically.


%    Text of article.

%    Bibliographies can be prepared with BibTeX using amsplain,
%    amsalpha, or (for "historical" overviews) natbib style.
\bibliographystyle{amsplain}
\bibliography{refs.bib,georg_refs}

\end{document}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%    Templates for common elements of an article; for additional
%    information, see the file Author_Handbook_ProcColl.pdf, included
%    in every AMS author package, and the amsthm user's guide, linked
%    from http://www.ams.org/tex/amslatex.html .

%    Section headings
\section{}
\subsection{}

%    Ordinary theorem and proof
\begin{theorem}[Optional addition to theorem head]
% text of theorem
\end{theorem}

\begin{proof}[Optional replacement proof heading]
% text of proof
\end{proof}

%    Figure insertion; default placement is top; if the figure occupies
%    more than 75% of a page, the [p] option should be specified.
\begin{figure}
\includegraphics{filename}
\caption{text of caption}
\label{}
\end{figure}

%    Mathematical displays; for additional information, see the amsmath
%    user's guide:  texdoc amamath  or
%  http://mirror.ctan.org/tex-archive/macros/latex/required/amsmath/amsldoc.pdf

% Numbered equation
\begin{equation}
\end{equation}

% Unnumbered equation
\begin{equation*}
\end{equation*}

% Aligned equations
\begin{align}
  &  \\
  &
\end{align}

%-----------------------------------------------------------------------
% End of article-template.tex
%-----------------------------------------------------------------------
