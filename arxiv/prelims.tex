\section{Preliminaries and Notation}\label{section:prelim}

In this section we present the setup that will be used throughout this
article. The theoretical foundations for inverse problems over
function spaces can be found in \cite{Stuart10} and will not be
reviewed here.


\subsection{Bayesian Linear Inverse Problems}\label{subsec:abstract OED}
Let $\hilp$ and $\hilo$ be separable Hilbert spaces (the subscripts p
and o are for ``parameter'' and ``observation'', respectively), and
let $\fwd: \hilp \to \hilo$ the \emph{forward operator}. The forward
operator $\fwd$ is assumed linear and strongly smoothing (the heat
operator of the 1D heat equation introduced in Section
\ref{section:intro} is a prime example). Take a Gaussian prior $\param
\sim \pr = \normal(\prmean ,\prcov)$ with some appropriate covariance
operator $\prcov$ on $\hilp$ \cite{Stuart10}. Note that $\fwd \prcov
\fwd^*$ is the prior covariance in $\hilo$ \cite{Stuart10}, and as
such, $\fwd \prcov \fwd^*$ is assumed invertible --- an assumption
which will be used below (and if $\fwd$ has a nontrivial kernel we
utilize Occam's Razor and ignore said kernel). Measurements are taken
via the \emph{measurement operator}. It is common for the measurement
and forward operators to be merged $\tmp := \obs \fwd$
\cite{AlexanderianGloorGhattas14}, but the analysis carried out in the
following sections requires that $\fwd$ and $\obs$ are explicitly
separated as in \cite{attia2022stochastic, cvetkovic2023choosing}.
$\obs \in ( \hilo^* )^m$, where $m$ is the number of measurements
taken. Entries $\meas_j, j=1,\dots,m$ of the observation operator
$\obs$ are called \emph{measurements}:

\begin{equation*}%\label{eq:O}
  \obs u = (\meas_1(u), \dots, \meas_m(u) )^t \in \R^m,\ u \in \hilo.
\end{equation*}

Data is acquired via noisy observations, and we consider two types of
error terms: Spatially correlated model error $\eps' \sim
\normal(0,\modcov)$ with $\modcov$ a covariance operator. Observation
error is denoted $\eps \sim \normal(0, \sigma^2 I_m)$, with $I_m \in
\mathbb{R}^{m \times m}$ the identity. Both error terms and the prior
are assumed independent of each other. Thus, data is acquired via
\begin{align}\label{eq:inverse problem}
  \data := \obs (\fwd \param + \eps') + \eps = \obs \fwd \param + \obs \eps' + \eps.
\end{align}

It is easy to verify that $\obs \eps' + \eps \in \R^m$ is a centered
Gaussian random vector with covariance matrix

\begin{align}\label{eq:Sigma}
  \begin{split}
    \Sigma(\obs) :&= \mathbb{E}[ (\obs \eps' + \eps) (\obs \eps' +
      \eps)^t ]
    % 
    % 
    = \obs \modcov \obs^* + \sigma^2I_m , 
  \end{split}
\end{align}
where
\begin{align}\label{eq:modcov explained}
  \begin{split}
    [\obs \modcov \obs^*]_{ij} & = e_i^t \obs \modcov \obs^* e_j 
    %
    %
    %
    = \meas_i (\modcov \meas_j).% \text{ (by \eqref{eq:obs*})}.
  \end{split}
\end{align}
Taking $\modcov = 0$ is a common practice
\cite{tarantola2005,Kaipio2005,Vogel02} and then $\Sigma =
\sigma^2I_m$ is a scalar matrix which does not depend on $\obs$.

Finally, it is useful to record that the posterior measure $\post$ is
Gaussian in this setting and its covariance operator is does not
depend on data $\data$ \cite{Stuart10}:
\begin{align}\label{eq:postcov}
  \postcov = (\prcov^{-1} + \fwd^* \obs^* \Sigma^{-1} \obs \fwd
  )^{-1}.
\end{align}

\subsection{Bayesian D-Optimal Designs in Infinite Dimensions}\label{subsec:D optimal design} 
A Bayesian D-optimal design maximizes expected Kullback-Leibler (KL)
divergence between posterior $\post$ and prior measures $\pr$. It is
useful to first recall the definition of KL divergence \cite{CoverThomas91}:
$$
D_{KL}(\post||\pr) = \int \log \frac{\der \post}{\der \pr}(\param) \der \post(\param).
$$

The study of D-optimal designs for Bayesian linear inverse problems in
infinite dimensions was pioneered by \cite{AlexanderianGloorGhattas14,
  alexanderian2018efficient}. The main result we will make use of is
summarized in our notation below:

\begin{theorem}[Alexanderian, Gloor, Ghattas \cite{AlexanderianGloorGhattas14}]\label{thm:d optimality}
  Let $\pr = \normal(\prmean,\prcov)$ be a Gaussian prior on $\hilp$
  and let $\post = \normal(\postmean,\postcov)$ the posterior measure
  on $\hilp$ for the Bayesian linear inverse problem $\data = \obs
  \fwd\param + \obs \eps' + \eps$ discussed above. Then
  \begin{align}\label{eq:objective}
    \begin{split}
      \tar( \obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
      % 
      % 
      % 
      &= \frac12 \log \det 
      ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}).
    \end{split}
  \end{align}
\end{theorem}

Note that in \cite{AlexanderianGloorGhattas14,
  alexanderian2018efficient}, results are stated for $\Sigma=I$
(implied by $\modcov = 0,\sigma^2=1$), but the these results also hold
for more general covariance matrices
\cite[p. 681]{AlexanderianGloorGhattas14}.

%% It is important to note that since $\obs$ is finite-rank,
%% $\prcov^{1/2} \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}$ is
%% trace-class.
\begin{definition}\label{def:d optimality}
  We say $\obs^{\star}$ is \emph{D-optimal} if $\obs^{\star} =
  \argmax_{\obs} \tar(\obs)$, where entries of $\obs \in (\hilo^*)^m$
  are constrained to some allowed set of measurements in $\hilo^*$.
\end{definition}

For a Bayesian linear model in finite dimensions, with Gaussian prior
and Gaussian noise, a D-optimal design minimizes the determinant of
the posterior covariance matrix, and this turns out to be a
regularized version of the frequentist criterion
\cite{Chaloner1995}. Theorem \ref{thm:d optimality} and Definition
\ref{def:d optimality} carry a similar intuition:
\begin{align*}
  \begin{split}
    \tar(\obs) &= \frac12 \log \det ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}) \text{ by \eqref{eq:objective}}\\
    &= \frac12 \log \det \Big( \prcov ( \prcov^{-1} + \fwd ^* \obs^* \Sigma^{-1} \obs \fwd) \Big )\\
    &= \frac12 \log \det \prcov \postcov^{-1} \text{ by \eqref{eq:postcov}}.
    %%  &= \frac12 \log \det \prcov -\frac12 \log \det \postcov.
  \end{split}
\end{align*}
We think of $\prcov$ as constant, so a D-optimal design minimizes a
quantity analogous to the posterior covariance determinant, similarly
to the finite-dimensional case.


%% \subsection{Notation Summary}\label{subsec:notation}
%%   Our notation is summarized below. We let:
%%   \begin{itemize}
%%   \item $\hilp, \hilo$ Hilbert spaces.
%%   \item $\fwd:\hilp \to \hilo$ a linear compact operator.
%%   \item $\pr \sim \mathcal{N}(0, \prcov)$ prior Gaussian measure on
%%     $\hilp$, with prior covariance operator $\prcov:\hilp \to \hilp$.
%%   \item $\obs: \hilo \to \mathbb{R}^m$ measurement operator, where $m
%%     \in \mathbb{N}$ is the number of measurements taken.
%%   \item $\sigma^2 \in \mathbb{R}_{+}$ observation noise variance.
%%   \item $\modcov$ model error covariance operator.
%%     %% $\data = \obs \fwd \param + \eps$, where $\eps \in
%%     %% \mathbb{R}^m$ isiid $\mathcal{N}(0, \sigma^2)$ noise.
%%   \item $\Sigma(\obs) = \obs \modcov \obs^* + \sigma^2I$. 
%%   \item $\post$ the posterior measure, with covariance $\postcov$.
%%   \item A D-optimality design criterion
%%     \cite{AlexanderianGloorGhattas14}:
%%     \begin{align*}
%%       \begin{split}
%%         \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
%%         % 
%%         % 
%%         % 
%%         &= \frac12 \log \det ( I + \prcov^{1/2} \fwd ^* \obs^* \Sigma(\obs)^{-1} \obs
%%         \fwd \prcov^{1/2}).
%%       \end{split}
%%     \end{align*}
%%   %% \item $\{\lambda_i\}_{i=1}^\infty$ eigenvalues of $\fwd\prcov\fwd^*$
%%   %%   in decreasing order of magnitude.
%%   %% %% \item $\{\ev_i\}_{i=1}^\infty$ their corresponding eigenvectors.
%%   %% \item $\{\eta_i\}_{i=1}^\infty$ eigenvalues of $\obs^*\obs$.
%%   \end{itemize}


%% \subsection{Sequential vs Simultaneous Optimization}\label{subsec:seq vs sim}
%% From defintion \ref{def:d optimality} we wish to characterize solution(s) of the
%% following optimization problem for $\tar$. %%: (\hilo^*)^m \to \R$:
%% \begin{align}\label{eq:optimization}
%%   \obs^{\star} := \argmax_{\obs} \tar( \obs ) 
%%   = \argmax_{\obs} \frac12 \log \det 
%%   (I + \prcov^{1/2} \fwd^*\obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}),
%% \end{align}
%% where $\obs$ is constrained to some allowed set of observations. We
%% call this problem ``simultaneous optimization'', since all
%% observations are decided on simulatneously.

%% For computational reasons, one may prefer to find the best
%% observations in a sequential manner. Denote
%% \begin{equation}\label{eq:def obs_k}
%%   \obs_k := (\meas_1,\dots, \meas_k)^t,  k\leq m.
%% \end{equation}
%% Sequential optimal design proceeds as follows. Find $\meas_1$ by
%% maximizing $\tar(\obs_1)$. Then, keeping $\meas_1$ fixed --- find
%% $\meas_2$ as the maximizer of $\tar(\obs_2)$. Then, find $\meas_3$ by
%% keeping $\meas_1,\meas_2$ fixed and taking $\meas_3$ as the maximizer
%% of $\tar(\obs_3)$. Continue this way until $\obs_m = \obs$ is
%% found, where $m$ is the number of available observations. %% It is
%% %% important to notice that this scheme does not require actually
%% %% observing data --- in \eqref{eq:objective} data is averaged out.

%% The analysis in this paper is conducted for the general simultaneous
%% optimization case. The sequential optimization case is dealt with in
%% section \ref{subsec:clusterization sequential}. It is important to
%% note, however that all conclusions we arrive at for the simultaneous
%% case easily specialize to the sequential case by considering the
%% posterior as the next sequential step's prior.

