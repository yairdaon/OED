\section{Referee 2}\label{ref2}
\subsection{Measurement Clusterization vs Repetition}
\RC I will start by saying measurement clusterisation is not
well-defined in this paper. On page 1, it is referred to as two sets
of measurements that are almost indistinguishable from one
another. However, the setup for Corollary 11 implicitly defines
measurement clusterisation as repeated measurements. The latter
definition matches the references of \cite{fedorov1996} and
\cite{nyberg2012}. So I will assume for the rest of this review that,
measurement clusterisation refers to repeated measurements.

\AR I appreciate the comment, I fixed the relevant line in the
manuscript.


\RC Nearly all textbooks on classical design of experiments
(e.g.~\cite[Section 1.2.4]{morris2011}) will feature a section with
the three words randomisation, blocking and repetition: the important
principles of design of experiments.  In these texts, repeated
measurements are considered to be a beneficial property of a
design. Interestingly, when optimal designs are found, they are often
found to have repeated measurements. The paper does not explain why
the author thinks repeated measurements (measurement clusterisation)
is an undesirable property.

\AR\label{rep} I added the following to the Introduction.

\begin{quote} %% Replication
  Clusterization should not be confused with replication. Replication
requires that the experimentalist executes multiple trials under
circumstances that are \emph{nominally identical} \cite[Section
  1.2.4]{morris2011}. Replication is commonly viewed as a beneficial
and even necessary aspect of optimal experimental design
\cite{fisher1949design, morris2011, schafer2001replication}.
For example, \cite{fisher1949design}, suggested repeating his famous
milk and tea experiment in order "to be able to demonstrate the
predominance of correct classifications in spite of occasional
errors". Unfortunately, in the experiments we consider, replication is
impossible. For example, in the MRI problem, we cannot generate an
individual nominally identical to the one we wish to scan.
\newline
Similarly to a design implementing replication, a clustered design
reduces the signal-to-noise ratio of the repeated measurements
\cite{telford2007brief}. The difference is that a clustered design
takes repeated measurements at the expense of other quantities not
measured at all. For example, consider an experiment measuring the
effect of rainfall on grass growth \cite{fay2000rainfall}. The
experiment involved four rainfall manipulation "treatments"
(i.e.~simulating different timing and quantity of rainfall), each
replicated three times over different plots of land. Indeed, it seems
reasonable for researchers to replicate the phenomenon they are trying
to study. A clustered design in such an experiment would imply the
researchers should take a repeated measurement \emph{on the same
plot}, at the expense of measuring grass growth in other plots!
\newline
To conclude our short discussion on replication vs.~clusterization:
these are fundamentally different concepts and even though replication
is quite intuitive, it is inapplicable to the inverse problems we
consider in this manuscript.
\end{quote}



\RC If one looks at, for example, \cite{fedorov1996} and
\cite{nyberg2012}, it is an undesirable property because they are
interested in sensor location, and two or more sensors cannot be in
the same location. However, there are many experiments where repeated
measurements are possible, and are therefore optimal

\AR I believe that even in an application where repeated experiments
are allowed they may be unintuitive. For example, in a medical imaging
application --- repeatedly measuring a "slice" in an MRI scan might
seem suboptimal to some.


\RC (if the model is correct, which it won’t be).

\AR I think this is a great point. My analysis implies one of the
causes to clusterization is linearization:
\begin{quote} %% Linearization
Another potential cause of clusterization is linearization. While in a
real applications the model is not necessarily linear, some authors
consider a linearized version of their model when seeking optimal
designs \cite{fedorov1996, neitzel2019sparse}. Our analysis then shows
that the linearity of the forward problem is also an important
ingredient in giving rise to clustered desgins. Therefore, our advice
to practitioners is to avoid linearization, and find D-optimal designs
via other methods, e.g.~the sampling method of \cite{ryan2003}.
\end{quote}


\RC An example, is a chemical engineering experiment where one wishes
to learn the relationship between a series of variables and yield of a
chemical reaction. The experiment will involve specifying the
variables, and measuring the yield (the observation) after a specified
period of time. This process is then duplicated with a potentially
different set of variables. It is perfectly possible to have repeated
variables.

\AR I agree, but I believe what you are describing is replication. See
my comment on replication vs.~clusterization above.
 
\RC The author needs to make clear that they are considering
experiments where repeated measurements are impossible. Admittedly,
this consideration is implicit in the paper, for example, the examples
given on page 1. However, it needs to be made explicit in the Abstract
and the first paragraph of the paper.

\AR I do not refrain from considering experiments where repeated
measurements are possible.

  
\RC The paper explains why repeated measurements can be optimal
(answer to Question 3). The author should expand on this and draw the
link with how repeated measurements are, typically, regarded as
desirable in classical design of experiments for finite
dimensions.

\AR Great point, see my quote on replication vs.~clusterization
reproduced above.

\RC See for example, the discussion of the relationship between
Carath\'eodory’s Theorem and the number of support points in \cite[page
  139]{pronzatoPazman2013}. The support points are the unique design
points, so if this is less than $m$, then there are repeated design
points.
  
\AR I appreciate this insightful comment and reference. The arguments
presented there do not directly apply to the setup I suggest in my
paper. Thus, I have made some modifications:

\begin{quote} %% Caratheodory
  We can gain further insight to clusterization in our generic model,
  from Carath\'eodory's Theorem and the concentration on the first $k$
  eigenvectors of $\fwd \prcov \fwd^*$. We present a short discussion
  adapting arguments presented in \cite[Chapter 3]{silvey1980} and
  \cite[Section 5.2.3]{pronzatoPazman2013}.

  Consider $\opt$ a D-optimal design under our generic model.  As
  instructed by Theorem \ref{thm:char}, we ignore all but the first
  $k$ eigenvectors of $\fwd \prcov \fwd$. Thus, we replace $\hilo$
  with $\hilo^{(k)}$ --- the $k$-dimensional subspace spanned by the
  first $k$ eigenvectors of $\fwd^*\prcov\fwd$. Let
  \begin{equation*}
    \mathcal{M} := \conv \{\meas \meas^* : \meas\in \hilo^{(k)}, \|\meas\|=1\},
    %\hilo^{(k)}\},
  \end{equation*}
  where $\conv$ denotes the convex hull of a set. The set
  $\mathcal{M}$ contains only positive-definite operators on a
  $k$-dimensional vector space. Hence $\mathcal{M}$ lives in a
  $k(k+1)/2$-dimensional vector space. Since $\opt^*\opt =
  \sum_{i=1}^m \meas_i\meas_i^*$ for $\meas_i \in \hilo^{(k)}$, it is
  easy to verify that $\frac1m \opt^*\opt \in \mathcal{M}$.  Recall
  Carath\'eodory's Theorem:
  \begin{theorem*}[Carath\'eodory]
    Let $X \subseteq \mathbb{R}^n, X \neq \phi$ and denote $\conv (X)$
    the convex hull of $X$. For every $x \in \conv (X)$, $x$ is a convex
    combination of at most $n+1$ vectors in $X$.
  \end{theorem*}
  Carath\'eodory's Theorem implies that there exist $\meas_i$ and
  $\alpha_i$ such that
  \begin{equation*}
    \opt^*\opt = \sum_{i=1}^I \alpha_i \meas_i\meas_i^*,
  \end{equation*}
  where $\|\meas_i\|=1, \sum\alpha_i = m, \alpha_i \geq 0$ and $I =
  \frac{k(k+1)}{2} + 1$. We can thus write $\opt$ as:
  %(note that we do not require $\meas_i$ to be orthogonal to each
  %other):
  
  \[
  \opt =
  \left[
    \begin{array}{ccc}
      \horzbar & \sqrt{\alpha_i} \meas^*_1 & \horzbar \\
      \horzbar & \sqrt{\alpha_2} \meas_2^* & \horzbar \\
      & \vdots    &          \\
      \horzbar & \sqrt{\alpha_I} \meas_I^* & \horzbar \\
    \end{array}
    \right].
  \]
  
  Unfortunately, $\opt$ is not a valid design, since its rows do not
  have unit norm. Still, the above representation of $\opt$ is useful:
  If $m > \frac{k(k+1)}{2} + 1$, then $\alpha_i > 1$ for some $1\leq i
  \leq I$.  Thus, we can view $\opt$ as a clustered design, since it
  places weight $>1$ on a single measurement vector.
\end{quote}


\subsection{Context}
\RC The paper lacks discussion on context. I read the answers to the
three questions and thought “So what?”  What are the implications to
statistical practice, or experimental science, of these findings?

\AR I really appreciate this comment. I added a section highlighting
the implications of this study. Since it is too long to reproduce
here, see Section \ref{subsub:implications} in the revised
manuscript. I will just mention that one important takeaway is that
practitioners should avoid linearization and Gaussian priors when
seeking D-optimal designs.


\RC The presentation of the paper does not help. The results are
presented with no explanation or intuition. It might be better to
relegate much of the mathematical detail to Supplementary Material and
use the main manuscript for exposition.

\AR Thanks for this comment. I considerably expanded the
  Introduction and delegated many proofs to the Supplementary.


\subsection{Avoiding measurement clusterisation}
\RC The approaches to avoid measurement clusterisation, described on page
3, are described as pragmatic and "fundamentally altering the optimal
design problem".

\RC Firstly, this is not strictly true. Suppose measurements correspond to
time, it is not possible to take two or more observations at the same
time, and there should be a minimum time period between measurements.
Implementing this minimum time period is not pragmatic, rather it is
characterising the application correctly. One could imagine a similar
thing occurs with the electrode locations on the skin in impedance
tomography: the physical size of the sensor imposes a minimum
distance between sensors.

\AR To a certain extent I agree: the impossibility of taking identical
measurements is indeed natural. However, this is not encoded in the
mathematical formulation of the problem. One can then take one of two
approaches: either accept what the mathematics suggest, even if it
counterintuitive, or find ways to circumvent the mathematics because
the result seems non-intuitive. I expanded this discussion in the
revised manuscript. See paragraph below and the discussion preceding
it in Section \ref{subsub:implications} of the revised manuscript.

\begin{quote}%% Avoid
  Given the above discussion, it is our belief that the methods
  suggested by other authors to avoid clusterization are merely
  overlooking the problem. We do not view clustered designs as
  inherently bad on their own. Rather, we suggest that if a clustered
  design arises, then a practitioner should revisit their probabilistic
  and physical modeling assumptions. If the practitioner is confident in
  their assumptions, then clustered designs should be avoided only to
  the extent necessitated by the physical measuring apparatus.
\end{quote}


\RC Secondly, the paper seems to suggest “imposing correlations
between observations” as a solution. However, isn’t this imposition
pragmatic as well? I suppose it is argued that the $\obs \eps'$ term
is accounting for model error: but it is still pragmatic: the data
acquisition given by equation (1) is not the true data-generating
process.

\AR I apologize for this misunderstanding and appreciate the
comment. I have revised the manuscript to reflect the fact that I do
not support imposing correlations in the paper. I do argue against
linearization (see comment on linearization above) and the use of Gaussian priors (see below):

\begin{quote} %% Gaussian
One potential cause of clusterization is our choice of prior. Gaussian
priors, coupled with a Gaussian likelihood and a linear forward
problem give rise to a closed form solution for the posterior via
conjugacy. As we show in Section \ref{subsec:lemma_sims}, such
assumptions generically give rise to clusterization. While an
assumption of a Gaussian likelihood is standard, and rooted in the
central limit theorem, an assumption of Gaussian prior is merely a
matter of convenience. Therefore, we advise any practitioner who
encounters clusterization to replace their Gaussian priors with
non-Gaussian priors instead \cite{hosseini2017, hosseini2019}.
\end{quote}


\subsection{Minor comments}
\RC The author uses two different forms of asterisk: one for
optimality of the design (e.g. Definition 3) and one for the
adjoint. Suggest changing the one for optimality.

\AR I changed the notation for a D-optimal design from $\obs^\star$ to
$\opt$.
  
\RC I do not like the use of the term measurements as used in the
paper. The measurements are the $\data$’s since these are a result of
measuring a physical quantity. However, the paper refers to the
quantities that are controlled as the measurements.

\AR I was not able to find a better term to replace "measurements".
  
\RC Proof of Proposition 12 uses $\eps$ as part of the regularisation
trick but this has been used previously for error (e.g. equation (1)).
  
\AR I replaced $\eps$ with $\nu$ and moved the entire proof to the
Supplementary.






















%% Firstly, the argument in \cite{pronzatoPazman2013} relies on the
%% assumption that $\theta$ --- the parameter we seek to infer --- is
%% $p$-dimensional. However, the setup I suggest is
%% infinite-dimensional. E.g.~in the generic model I propose, the
%% parameter is in $\hilo$, which is assumed an infinite-dimensional
%% Hilbert space. Similarly, the initial condition $u_0$ we try to infer
%% in the inverse problem of the heat equation is in some function space
%% over $\Omega=[0,1]$. Even if we discretize $\Omega$ and consider a
%% finite-dimensional problem, the number of discretization points would
%% be at least a hundred. So even the bound $m \leq p=100$ suggested in
%% \cite[Section 5.3.1]{pronzatoPazman2013} is not very meaningful when
%% we allow only $\approx 6$ measurements. Indeed, discretization points
%% are cheap and measurements are expensive so the number of
%% discretization points will always be considerably larger than the
%% number of allowed measurements. Thus, I do not see how the bound
%% suggested will be informative. Respectfully I choose not to explore
%% your suggestion in the manuscript.

%% We can bypass the fact that $p$ tends to be large by recalling that
%% Theorem \ref{thm:char} implies
