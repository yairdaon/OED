\section{Reviewer 2}\label{ref2}
\subsection{Measurement Clusterization vs Repetition}
\RC I will start by saying measurement clusterisation is not
well-defined in this paper. On page 1, it is referred to as two sets
of measurements that are almost indistinguishable from one
another. However, the setup for Corollary 11 implicitly defines
measurement clusterisation as repeated measurements. The latter
definition matches the references of \cite{fedorov1996} and
\cite{nyberg2012}. So I will assume for the rest of this review that,
measurement clusterisation refers to repeated measurements.

\AR I appreciate the comment, I fixed the relevant line in the
manuscript.


\RC Nearly all textbooks on classical design of experiments
(e.g.~\cite[Section 1.2.4]{morris2011}) will feature a section with
the three words randomisation, blocking and repetition: the important
principles of design of experiments.  In these texts, repeated
measurements are considered to be a beneficial property of a
design. Interestingly, when optimal designs are found, they are often
found to have repeated measurements. The paper does not explain why
the author thinks repeated measurements (measurement clusterisation)
is an undesirable property.

\AR\label{rep} I do not view clusterization as an undesired property, I only
mentioned many authors consider it to be undesirable. I have added a
paragraph contrasting repetition and replication with clusterization
and why I believe they may be different:


\begin{quote}
  Clusterization should not be confused with replication. Replication
  requires that the experimentalist executes multiple trials under
  circumstances that are \emph{nominally identical} \cite[Section
    1.2.4]{morris2011}. Replication is commonly viewed as a beneficial
  and even necessary aspect of optimal experimental design
  \cite{fisher1949design, morris2011,
    schafer2001replication}. Similarly to a design implementing
  replication, a clustered design reduces the signal-to-noise ratio of
  the repeated measurements \cite{telford2007brief}. The difference is
  that a clustered design takes repeated measurements at the expense
  of other quantities not measured at all.

  For example, \cite{fisher1949design}, suggested repeating his famous
  milk and tea experiment in order "to be able to demonstrate the
  predominance of correct classifications in spite of occasional
  errors". Unfortunately, in the experiments we consider, replication
  is impossible. For example, in the MRI problem, we cannot generate
  an individual nominally identical to the one we wish to scan.

  To further illustrate the difference between replication and
  clusterization, consider an experiment measuring the effect of
  rainfall on grass growth \cite{fay2000rainfall}. The experiment
  involved four rainfall manipulation "treatments" (i.e.~simulating
  different timing and quantity of rainfall), each replicated three
  times over a different plot of land. Indeed, it seems reasonable for
  researchers to replicate the phenomenon they are trying to study. A
  clustered design in such an experiment would imply the researchers
  should take a repeated measurement \emph{on the same plot}, at the
  expense of measuring grass growth in other plots. To conclude our
  short discussion on replication vs.~clusterization: these are
  fundamentally different concepts and even though replication is
  quite intuitive, it is inapplicable to the inverse problems we
  consider in this manuscript.
\end{quote}



\RC If one looks at, for example, \cite{fedorov1996} and
\cite{nyberg2012}, it is an undesirable property because they are
interested in sensor location, and two or more sensors cannot be in
the same location. However, there are many experiments where repeated
measurements are possible, and are therefore optimal

\AR I believe that even in an application where repeated experiments
are allowed they may be unintuitive. For example, in a medical imaging
application --- repeatedly measuring a "slice" in an MRI scan might
seem suboptimal to some.


\RC (if the model is correct, which it won’t be).

\AR I think this is a great point. I believe clusterization may imply
a model is somehow inadequate:

\begin{quote}
  When clusterization arises, we believe it should serve as a warning
  signal to practitioners. In the inverse problem of the 1D heat
  equation, clusterization occurs primarily because Laplacian
  eigenvectors do not decay in $\Omega$. Consequently, measuring
  $u(x_1, T)$ at some point $x_1 \in \Omega$ provides information
  about $u(x_2,T)$ for distant points $x_2 \in \Omega$. Intuitively,
  this should not happen: for small $T$, the heat distribution at
  $x_1$ should give very little knowledge on the heat distribution at
  $x_2$. This behavior stems from a well-known property of the heat
  equation: it allows information to spread instantly across the
  computational domain \cite{renardy2006PDE}. In reality, heat (and
  information) propagate at finite speeds. Of course, the known
  physical barrier for information spread is the speed of light, but
  we expect heat and information to spread considerably slower. Thus,
  clusterization may suggest that our physical model itself is at
  fault.
  
  Our choice of prior is perhaps an even greater contributor to
  clusterization. Our choice of Gaussian prior similarly implies
  information is shared between distant locations in $\Omega$. Thus,
  we suggest refraining from choosing Gaussian priors with inverse
  Laplacian covariance operators. Rather, non-Gaussian priors should
  be employed instead \cite{hosseini2017, hosseini2019}. So,
  clusterization may suggest we give more attention to the choice of
  prior.
\end{quote}



\RC An example, is a chemical engineering experiment where one wishes
to learn the relationship between a series of variables and yield of a
chemical reaction. The experiment will involve specifying the
variables, and measuring the yield (the observation) after a specified
period of time. This process is then duplicated with a potentially
different set of variables.  It is perfectly possible to have repeated
variables.

\AR I agree that the problem with clustered designs depends on the
application. However, in the chemical engineering experiment
mentioned, it is possible that an optimal design will require two
simultaneous measurements of the same experiment. This is an example
of a clustered design that seems unintuitive.

 
\RC The author needs to make clear that they are considering
experiments where repeated measurements are impossible. Admittedly,
this consideration is implicit in the paper, for example, the examples
given on page 1. However, it needs to be made explicit in the Abstract
and the first paragraph of the paper.

\AR I do not refrain from considering experiments where repeated
measurements are possible. I am giving insight on why such designs
arise, since many authors view repeated measurements as non-intuitive.

  
\RC The paper explains why repeated measurements can be optimal
(answer to Question 3). The author should expand on this and draw the
link with how repeated measurements are, typically, regarded as
desirable in classical design of experiments for finite
dimensions.

\AR Thank you for this suggestion. I believe my previous answer above
also answers this comment. It is reproduced below:

\begin{quote}
  Clusterization should not be confused with replication. Replication
  requires that the experimentalist executes multiple trials under
  circumstances that are \emph{nominally identical} \cite[Section
    1.2.4]{morris2011}. Replication is commonly viewed as a beneficial
  and even necessary aspect of optimal experimental design
  \cite{fisher1949design, morris2011,
    schafer2001replication}. Similarly to a design implementing
  replication, a clustered design reduces the signal-to-noise ratio of
  the repeated measurements \cite{telford2007brief}. The difference is
  that a clustered design takes repeated measurements at the expense
  of other quantities not measured at all.

  For example, \cite{fisher1949design}, suggested repeating his famous
  milk and tea experiment in order "to be able to demonstrate the
  predominance of correct classifications in spite of occasional
  errors". Unfortunately, in the experiments we consider, replication
  is impossible. For example, in the MRI problem, we cannot generate
  an individual nominally identical to the one we wish to scan.

  To further illustrate the difference between replication and
  clusterization, consider an experiment measuring the effect of
  rainfall on grass growth \cite{fay2000rainfall}. The experiment
  involved four rainfall manipulation "treatments" (i.e.~simulating
  different timing and quantity of rainfall), each replicated three
  times over a different plot of land. Indeed, it seems reasonable for
  researchers to replicate the phenomenon they are trying to study. A
  clustered design in such an experiment would imply the researchers
  should take a repeated measurement \emph{on the same plot}, at the
  expense of measuring grass growth in other plots. To conclude our
  short discussion on replication vs.~clusterization: these are
  fundamentally different concepts and even though replication is
  quite intuitive, it is inapplicable to the inverse problems we
  consider in this manuscript.
\end{quote}


\RC See for example, the discussion of the relationship between
Carath\'eodory’s Theorem and the number of support points in \cite[page
  139]{pronzatoPazman2013}. The support points are the unique design
points, so if this is less than $m$, then there are repeated design
points.
  
\AR I appreciate this insightful comment and reference. The arguments
presented there do not directly apply to the setup I suggest in my
paper. Thus, I have made some modifications:

\begin{quote}
  We can gain further insight to clusterization in our generic model,
  from Carath\'eodory's Theorem and the concentration on the first $k$
  eigenvectors of $\fwd \prcov \fwd^*$. We present a short discussion
  adapting arguments presented in \cite[Chapter 3]{silvey1980} and
  \cite[Section 5.2.3]{pronzatoPazman2013}.

  Consider $\opt$ a D-optimal design under our generic model.  As
  instructed by Theorem \ref{thm:char}, we ignore all but the first
  $k$ eigenvectors of $\fwd \prcov \fwd$. Thus, we replace $\hilo$
  with $\hilo^{(k)}$ --- the $k$-dimensional subspace spanned by the
  first $k$ eigenvectors of $\fwd^*\prcov\fwd$. Let
  \begin{equation*}
    \mathcal{M} := \conv \{\meas \meas^* : \meas\in \hilo^{(k)}, \|\meas\|=1\},
    %\hilo^{(k)}\},
  \end{equation*}
  where $\conv$ denotes the convex hull of a set. The set
  $\mathcal{M}$ contains only positive-definite operators on a
  $k$-dimensional vector space. Hence $\mathcal{M}$ lives in a
  $k(k+1)/2$-dimensional vector space. Since $\opt^*\opt =
  \sum_{i=1}^m \meas_i\meas_i^*$ for $\meas_i \in \hilo^{(k)}$, it is
  easy to verify that $\frac1m \opt^*\opt \in \mathcal{M}$.  Recall
  Carath\'eodory's Theorem:
  \begin{theorem*}[Carath\'eodory]
    Let $X \subseteq \mathbb{R}^n, X \neq \phi$ and denote $\conv (X)$
    the convex hull of $X$. For every $x \in \conv (X)$, $x$ is a convex
    combination of at most $n+1$ vectors in $X$.
  \end{theorem*}
  Carath\'eodory's Theorem implies that there exist $\meas_i$ and
  $\alpha_i$ such that
  \begin{equation*}
    \opt^*\opt = \sum_{i=1}^I \alpha_i \meas_i\meas_i^*,
  \end{equation*}
  where $\|\meas_i\|=1, \sum\alpha_i = m, \alpha_i \geq 0$ and $I =
  \frac{k(k+1)}{2} + 1$. We can thus write $\opt$ as:
  %(note that we do not require $\meas_i$ to be orthogonal to each
  %other):
  
  \[
  \opt =
  \left[
    \begin{array}{ccc}
      \horzbar & \sqrt{\alpha_i} \meas^*_1 & \horzbar \\
      \horzbar & \sqrt{\alpha_2} \meas_2^* & \horzbar \\
      & \vdots    &          \\
      \horzbar & \sqrt{\alpha_I} \meas_I^* & \horzbar \\
    \end{array}
    \right].
  \]
  
  Unfortunately, $\opt$ is not a valid design, since its rows do not
  have unit norm. Still, the above representation of $\opt$ is useful:
  If $m > \frac{k(k+1)}{2} + 1$, then $\alpha_i > 1$ for some $1\leq i
  \leq I$.  Thus, we can view $\opt$ as a clustered design, since it
  places weight $>1$ on a single measurement vector.
\end{quote}

%% Firstly, the argument in \cite{pronzatoPazman2013} relies on the
%% assumption that $\theta$ --- the parameter we seek to infer --- is
%% $p$-dimensional. However, the setup I suggest is
%% infinite-dimensional. E.g.~in the generic model I propose, the
%% parameter is in $\hilo$, which is assumed an infinite-dimensional
%% Hilbert space. Similarly, the initial condition $u_0$ we try to infer
%% in the inverse problem of the heat equation is in some function space
%% over $\Omega=[0,1]$. Even if we discretize $\Omega$ and consider a
%% finite-dimensional problem, the number of discretization points would
%% be at least a hundred. So even the bound $m \leq p=100$ suggested in
%% \cite[Section 5.3.1]{pronzatoPazman2013} is not very meaningful when
%% we allow only $\approx 6$ measurements. Indeed, discretization points
%% are cheap and measurements are expensive so the number of
%% discretization points will always be considerably larger than the
%% number of allowed measurements. Thus, I do not see how the bound
%% suggested will be informative. Respectfully I choose not to explore
%% your suggestion in the manuscript.

%% We can bypass the fact that $p$ tends to be large by recalling that
%% Theorem \ref{thm:char} implies

\subsection{Context}
\RC The paper lacks discussion on context. I read the answers to the
three questions and thought “So what?”  What are the implications to
statistical practice, or experimental science, of these findings?

\AR I really appreciate this comment. I added a section highlighting
the implications of this study. Since it is too long to reproduce
here, see Section \ref{subsub:implications} in the revised manuscript.


\RC The presentation of the paper does not help. The results are
presented with no explanation or intuition. It might be better to
relegate much of the mathematical detail to Supplementary Material and
use the main manuscript for exposition.

\AR Thanks for this comment. I considerably expanded the
  introduction and delegated many proofs to the Supplementary.


\subsection{Avoiding measurement clusterisation}
\RC The approaches to avoid measurement clusterisation, described on page
3, are described as pragmatic and "fundamentally altering the optimal
design problem".

\RC Firstly, this is not strictly true. Suppose measurements correspond to
time, it is not possible to take two or more observations at the same
time, and there should be a minimum time period between measurements.
Implementing this minimum time period is not pragmatic, rather it is
characterising the application correctly. One could imagine a similar
thing occurs with the electrode locations on the skin in impedance
tomography: the physical size of the sensor imposes a minimum
distance between sensors.

\AR To a certain extent I agree: the impossibility of taking identical
measurements is indeed natural. However, this is not encoded in the
mathematical formulation of the problem. One can then take one of two
approaches: either accept what the mathematics suggest, even if it
counterintuitive, or find ways to circumvent the mathematics because
the result seems non-intuitive. I expanded this discussion in the
revised manuscript. See below and the discussion preceding it in
Section \ref{subsub:implications} of the revised manuscript. 

\begin{quote}
  Given the above discussion, it is our belief that the methods
  suggested by other authors to avoid clusterization are merely
  overlooking the problem. We do not view clustered designs as
  inherently bad on their own. Rather, we suggest that if a clustered
  design arises, then a practitioner should revisit their probabilistic
  and physical modeling assumptions. If the practitioner is confident in
  their assumptions, then clustered designs should be avoided only to
  the extent necessitated by the physical measuring apparatus.
\end{quote}


\RC Secondly, the paper seems to suggest “imposing correlations
between observations” as a solution. However, isn’t this imposition
pragmatic as well? I suppose it is argued that the $\obs \eps'$ term
is accounting for model error: but it is still pragmatic: the data
acquisition given by equation (1) is not the true data-generating
process.

\AR I do not support imposing correlations in the paper, I just
used correlations to show how my framework can be used to rigorously
prove what is widely accepted. Other referees also understood that the
paper endorses imposing correlations and I accept that this is my
error in writing. See text answering previous comment.


\subsection{Minor comments}
\RC The author uses two different forms of asterisk: one for
optimality of the design (e.g. Definition 3) and one for the
adjoint. Suggest changing the one for optimality.

\AR I changed $\obs^\star$ to $\opt$.

%% \begin{quote}
%%   \begin{definition}\label{def:d_optimality}
%%     We say \DIFdelbegin \DIFdel{\( \obs^{\star} \)}\DIFdelend
%%     \DIFaddbegin \DIFadd{alive}\DIFaddend

%%     \(\opt\) is \emph{D-optimal} if \(\opt =
%%     \argmax_{\obs} \tar(\obs)\), where entries of \(\obs \in (\hilo^*)^m\)
%%     are constrained to some allowed set of measurements in \(\hilo^*\).
%%   \end{definition}
%% \end{quote}
  
\RC I do not like the use of the term measurements as used in the
paper. The measurements are the $\data$’s since these are a result of
measuring a physical quantity. However, the paper refers to the
quantities that are controlled as the measurements.

\AR I was not able to find a better term to replace "measurements", so
I stuck to it.

  
\RC Proof of Proposition 12 uses $\eps$ as part of the regularisation
trick but this has been used previously for error (e.g. equation (1)).
  
\AR I changed it to $\nu$. You will not find the revised proof in the
manuscript since I moved it to the Supplementary.


