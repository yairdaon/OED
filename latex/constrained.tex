\section{The Constrained Optimization Problem of D-Optimal Design}\label{section:D and grad}
We seek a formulation of the D-optimal design problem via Lagrange
multipliers. We first find the gradient of $\tar$. Then we suggest
unit-norm constraints on $\obs$ and find the gradients of these
unit-norm constraints. Results of this section are summarized in
Theorem \ref{thm:constrained}. First, recall that:
\begin{definition}\label{def:var}
  Let $F$ a real valued function of $\obs$. The first variation of $F$
  at $\obs$ in the direction $V$ is:
  \begin{equation*}
    \delta F(\obs) V := \frac{\der}{\der \tau}\Big |_{\tau=0}  F( \obs + \tau V).
  \end{equation*}

  If
  \begin{equation*}
    \delta F(\obs) V = \tr{\nabla F(\obs) V},
  \end{equation*}
  then we call $\nabla F(\obs)$ the gradient of $F$ at $\obs$. 
\end{definition}

%% Gradients are best thought of as row vectors. This will prove
%% important in section \ref{subsec:necessary}.


%% \subsection{The gradient of $\tar$}\label{section:objective}

\begin{proposition}\label{prop:tar grad}
  The gradient of the D-optimality objective $\tar$ is
  \begin{equation*}
    %% \delta \tar(\obs) V = \tr{V ( I - \modcov \obs^* \Sigma^{-1}\obs )
    %%   \fwd \postcov \fwd^* \obs^* \Sigma^{-1}}.
    \nabla \tar(\obs) = ( I - \modcov \obs^* \Sigma^{-1}\obs ) \fwd
    \postcov \fwd^* \obs^* \Sigma^{-1}
  \end{equation*}
\end{proposition}

\begin{proof}  
  From the definition of $\Sigma(\obs)$ \eqref{eq:Sigma}: 

  \begin{align}\label{eq:der sig}
    \begin{split}
      \frac{\der}{\der \tau} \Big |_{\tau=0} \Sigma( \obs + \tau V )
      &= \frac{\der}{\der \tau} \Big |_{\tau=0} 
      (\obs + \tau V ) \modcov (\obs + \tau V )^*  + \sigma^2I\\
      % 
      % 
      % 
      &= V \modcov \obs^* + \obs \modcov V^*.
    \end{split}
  \end{align}

  Then, using \eqref{eq:der sig}: 
  \begin{align*}
    0 &= \frac{\der}{\der \tau} \Big |_{\tau=0} I \\
    % 
    % 
    % 
    &= \frac{\der}{\der \tau} \Big |_{\tau=0}
    \left (\Sigma(\obs+\tau V)^{-1} \Sigma(\obs+\tau V) \right ) \\
    % 
    % 
    % 
    &= \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0} \Sigma+
    \Sigma^{-1} \frac{\der \Sigma(\obs+\tau V)}{\der \tau} \Big |_{\tau=0}\\  
    %
    %
    %
    &= \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0} \Sigma+
    \Sigma^{-1} (V\modcov \obs^* + \obs \modcov V^*). 
    %\text{, by \eqref{eq:der sig}. }
  \end{align*}

  Thus:
  \begin{align}\label{eq:der sig inv}
    \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0}  
      &= -\Sigma^{-1} (V \modcov \obs^* + \obs \modcov V^*) \Sigma^{-1}.
    \end{align}
  % 

  Let
  \begin{equation*}
    T(\obs) = \obs^* \Sigma^{-1}(\obs)\obs.
  \end{equation*}
  
  By Leibniz (product) rule and \eqref{eq:der sig inv}:
  % 
  \begin{align}\label{eq:T}
    \begin{split}
    \delta T(\obs) V 
    &= \frac{\der T(\obs + \tau V)}{\der \tau} \Big |_{\tau=0} \\
    %
    %
    %
    &= V^* \Sigma^{-1} \obs 
    - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
    &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs
    + \obs^* \Sigma^{-1} V.
    \end{split}
  \end{align}


  We now record Lemma \ref{lemma:lax} which is a generalization of a
  Theorem 4 in Chapter 9 of \cite{Lax07}. Its proof is delegated to
  the appendix.
  \begin{restatable*}{lemma}{lax}\label{lemma:lax}
    Let $Y(t)$ be a differentiable operator-valued function. Assume 
    $I+Y(t)$ is invertible, $Y(t)$ self-adjoint and trace-class. Then
    \begin{equation*}
      \frac{\der \log \det (I+Y(t))}{\der t} = \tr{(I+Y(t))^{-1} \dot{Y}(t)}.
    \end{equation*}
  \end{restatable*}

  We now employ \ref{lemma:lax} and calculate the first variation of
  $\tar$:

  \begin{align*}
    \begin{split}
      \delta \tar(\obs) V 
      :&= \frac{\der}{\der\tau} \Big |_{\tau=0} \tar(\obs + \tau V) \text{ (Definition \ref{def:var})}\\
      % 
      % 
      % 
      &= \frac12 \frac{\der}{\der \tau} \Big |_{\tau=0} \log \det 
      (I + \prcov^{1/2} \fwd^* T(\obs+\tau V)\fwd \prcov^{1/2} ) \text{ (Theorem \ref{thm:d optimality})} \\
      % 
      % 
      % 
      &= \frac12 \tr{( I + \prcov^{1/2} \fwd^* \obs^* \Sigma^{-1}
        \obs\fwd \prcov^{1/2} )^{-1}
        \frac{\der}{\der \tau} \Big |_{\tau=0}
        \prcov^{1/2} \fwd^* T(\obs+\tau V) \fwd \prcov^{1/2}}\ \text{ (Lemma \ref{lemma:lax})} \\
      % 
      % 
      % 
      &= \frac12 \ttr\Big \{ \postcov \fwd^* (V^* \Sigma^{-1} \obs 
      - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
      &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs 
      + \obs^* \Sigma^{-1} V ) \fwd \Big \}  \text{ (by \eqref{eq:T})} \\
      %
      %
      %
      &= \tr{\postcov \fwd^* ( \obs^* \Sigma^{-1} V -
      \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs ) \fwd} \\
      %
      %
      % 
      &= \tr{\postcov \fwd^* \obs^* \Sigma^{-1} V 
      ( I - \modcov \obs^* \Sigma^{-1}\obs ) \fwd} \\
      % 
      %
      %
      &= \tr{V ( I - \modcov \obs^* \Sigma^{-1}\obs )
      \fwd \postcov \fwd^* \obs^* \Sigma^{-1}}.
    \end{split}
  \end{align*} 
\end{proof}

In a real-life optimal design problem we cannot choose any measurement
operator $\obs \in (\hilo^*)^m$. In order to facilitate analysis, we
seek reasonable constraints on $\obs$ for which finding a D-optimal
design is analytically tractable. The following proposition will guide
us in finding such constraints.

\begin{proposition}\label{prop:bigger better}
  Let $\obs = (\meas_1,\dots,\meas_m)^t$, $j \in \{1,\dots,m\}$,
  $\sigma^2 > 0$ and $|\zeta| > 1$. Then $\tar(\obs)$ increases if we
  use $\zeta \meas_j$ in $\obs$ instead of $\meas_j$.
\end{proposition}

\begin{proof} 
  Fix an arbitrary $j=1,\dots,m$ and take $V:= e_j e_j^t \obs$. For $u
  \in \hilo$:
  \begin{equation*}
    Vu = e_je_j^t (\meas_1(u),\dots,\meas_m(u) )^t = e_j \meas_j(u)
    = (0,\dots,0,\meas_j(u),0,\dots,0)^t.
  \end{equation*}
  %% This way, $V$ has the same $j$th entry as $\obs$ while the rest
  %% are set to zero.
  We now calculate the variation of $\tar$ at $\obs$ in the direction
  of $V$. Denote $\tmp: = \fwd \postcov \fwd^*$. From Proposition
  \ref{prop:tar grad}:
  \begin{align*}
     \delta \tar(\obs) V 
    &= \tr{V ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}} \\
    % 
    %
    %
    &= \tr{e_je_j^t \obs ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}} \\
    %
    % 
    %
    &= e_j^t \obs ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}e_j \\
    %
    % 
    %
    &= e_j^t ( I - \obs \modcov \obs^*\Sigma^{-1})\obs \tmp \obs^* \Sigma^{-1}e_j \\  
    % 
    %
    %
    &=  e_j^t(\Sigma-\obs \modcov \obs^*) \Sigma^{-1}\obs \tmp \obs^* \Sigma^{-1}e_j \\
    %
    %
    %
    &=\sigma^2 e_j^t \Sigma^{-1}\obs \tmp \obs^* \Sigma^{-1}e_j
    \text{ by \eqref{eq:Sigma} }\\
    %
    % 
    %
    &=\sigma^2 e_j^t \Sigma^{-1}\obs \fwd \postcov \fwd^* \obs^* \Sigma^{-1}e_j.
  \end{align*} 
  Since $\postcov$ is positive definite, we conclude that $\delta
  \tar(\obs) V > 0$. This means that increasing the magnitude of the
  $j$th observation functional increases $\tar(\obs)$.
\end{proof}

Proposition \ref{prop:bigger better} implies that it is a good idea to
bound the norm of obsevation vectors. If, for example, we can take
measurements in $span\{\meas\}$ for some $\meas \neq 0$, then a
D-optimal design does not exist because we can always take an
observation vector in $span\{\meas\}$ with larger norm. In contrast,
in any real-life problem where sensors are concerned, the norm of
measurements recorded by sensors is always one\footnote{Of course,
point evaluations are not in any Hilbert space of functions we wish to
consider.}:
\begin{align*}
  \| \delta_{\x} \| = \sup_{0 \neq u \in C(\Omega)} \frac{
    |\int_{\Omega}u(\y) \delta_{\x}(\y) \der \y| 
  }{
    \sup|u|}
  = \sup_{0 \neq u \in C(\Omega)} \frac{|u(\x)|}{ \sup|u|} = 1,
  \forall \x \in \Omega.
\end{align*}

Thus, it is reasonable to consider observation vectors with unit
$\hilo^*$ norm. The unit norm constraints can be written using
\eqref{eq:obs*} as a series of $m$ equality constraints (one for each
observation) on $\obs$. We define these and find their gradients in
Proposition \ref{prop:constraints grad} below:

\begin{proposition}\label{prop:constraints grad}
  Let
  \begin{align*}
    \phi_j(\obs) :=\frac12 \| \obs^* e_j\|_{\hilp}^2 - \frac12 = 0,\ j=1,\dots,m.
  \end{align*}
  Then
  \begin{equation*}
    %% \delta \phi_j(\obs)V = \tr{V \obs^* e_je_j^t}.
    \nabla \phi_j(\obs) = \obs^* e_je_j^t.
  \end{equation*}
\end{proposition}

\begin{proof}
  \begin{align*}
    \delta \phi_j(\obs)V  
    &= \frac12\lim_{\tau \to 0}\tau^{-1}
    ( \|(\obs + \tau V)^*e_j \|_{\hilp}^2 - \|\obs ^*e_j \|_{\hilp}^2  ) \\
    %
    %
    %
    &= \frac12\lim_{\tau \to 0}\tau^{-1}
    ( \langle (\obs + \tau V)^*e_j, (\obs + \tau V)^*e_j \rangle_{\hilp} - 
    \langle \obs^*e_j, \obs^*e_j \rangle_{\hilp} ) \\
    % 
    % 
    %
    &= \frac12\lim_{\tau \to 0}\tau^{-1}
    (2\tau \langle \obs^*e_j,V^*e_j \rangle_{\hilp} 
    +\tau^2 \langle V^*e_j, V^*e_j \rangle_{\hilp} ) \\
    %
    %
    % 
    &= \langle \obs^*e_j,V^*e_j \rangle_{\hilp} \\
    %
    %
    % 
    &= \langle V \obs^*e_j,e_j \rangle_{\R^m} \\
    %
    %
    %
    &= e_j^t V \obs^* e_j \\
    % 
    %
    %
    &= \tr{V \obs^* e_je_j^t}.
  \end{align*}
\end{proof}


%% The same arguments justifying \eqref{eq:tar grad} hold here, and thus:

%% \begin{align}\label{eq:grad constraints}
%% \nabla \phi_j(\obs) = \obs^* e_j e_j^t = \meas_j e_j^t , j=1,\dots,m,
%% \end{align}
%% where $\nabla \phi_j(\obs) \in \hilo^m$. As noted at the end of
%% Section \ref{section:objective},

%% The gradients $\nabla \tar(\obs)$ and $\nabla \phi_j(\obs)$ are best
%% thought of as row vectors.


Necessary first-order conditions for D-optimality are found via
Lagrange multipliers:
\begin{align}
  &\nabla \tar(\obs) = \sum_{j=1}^m \xi_j \nabla \phi_j (\obs)
  \label{eq:Lagrange mult1} \\
    &\phi_j(\obs) = 0, j = 1,\dots,m. \label{eq:Lagrange mult2}
\end{align}

Substitute the gradients calculated in Propositions \ref{prop:tar
  grad} and \ref{prop:constraints grad} into \eqref{eq:Lagrange
  mult1}:
\begin{equation}\label{eq:constrained}
  (I - \modcov \obs^* \Sigma^{-1} \obs) \fwd \postcov \fwd^* \obs^*\Sigma^{-1}
  = \sum_{j=1}^m \xi_j \obs^* e_je_j^t = (\xi_1 \meas_1,\dots,\xi_m \meas_m).
\end{equation} 
Letting $\Xi := \diag(\xi_j)$, \eqref{eq:constrained} and
\eqref{eq:Lagrange mult2} are written more compactly as:

\begin{theorem}[Necessary conditions for D-Optimality]\label{thm:constrained}
  Let:
  \begin{equation*}
    \obs = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs).
  \end{equation*}
  
  Then:
  \begin{equation*}
    ( I - \modcov \obs^* \Sigma^{-1} \obs) \fwd \postcov \fwd^* \obs^*  \Sigma^{-1}
    = \obs^* \Xi, 
  \end{equation*}
  where $\Xi \in \mathbb{R}^{m \times m}$ is diagonal.
\end{theorem}




