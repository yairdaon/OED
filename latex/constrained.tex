\section{The Constrained Optimization Problem of D-Optimal Design}\label{section:D and grad}
We seek a formulation of the D-optimal design problem
\cite{AlexanderianGloorGhattas14} via Lagrange multipliers. In Section
\ref{section:objective} we find the gradient of $\tar$. In Section
\ref{subsec:unit norm} we suggest unit-norm constraints on rows of
$\obs$ and find their. Finally, in Section \ref{subsec:necessary} we
formulate the D-optimal design problem as a Lagrange multipliers
problem.


\subsection{The Objective and its Gradient}\label{section:objective}
Recall that:


\begin{definition}\label{def:var}
  Let $F$ a real valued function of $\obs$. The first variation of $F$
  at $\obs$ in the direction $V$ is:
  \begin{equation*}
    \delta F(\obs) V := \frac{\der}{\der \tau}\Big |_{\tau=0}  F( \obs + \tau V).
  \end{equation*}
\end{definition}

We now record two Lemmas whose proofs are delgated to the appendix.

\begin{restatable*}[Generalized from \cite{Lax07}]{lemma}{lax}\label{lemma:lax}
  Let $Y(t)$ be a differentiable operator-valued function. Assume 
  $I+Y(t)$ is invertible, $Y(t)$ self-adjoint and trace-class. Then
  \begin{equation*}
    \frac{\der \log \det (I+Y(t))}{\der t} = \tr{(I+Y(t))^{-1} \dot{Y}(t)}.
  \end{equation*}
\end{restatable*}

\begin{restatable*}[Auxilliary Calculations]{lemma}{aux}\label{lemma:aux calc}
  Let
  \begin{align*}
    \begin{split}
      \Sigma(\obs) &= \obs\modcov\obs^* + \sigma^2, \\
      T(\obs) &= \obs^* \Sigma^{-1}(\obs)\obs.
    \end{split}
  \end{align*}

  Then
  \begin{align*}
    \delta T(\obs)V &= V^* \Sigma^{-1} \obs 
    - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
    &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs
    + \obs^* \Sigma^{-1} V.
  \end{align*}
\end{restatable*}

The first variation of $\tar (\obs)$ in the direction $V$ is:
\begin{align}\label{eq:tar var}
  \begin{split}
    \delta \tar(\obs) V 
    :&= \frac{\der}{\der\tau} \Big |_{\tau=0} \tar(\obs + \tau V)\  \text{ (Definition \ref{def:var})}\\
    % 
    % 
    % 
    &= \frac12 \frac{\der}{\der \tau} \Big |_{\tau=0} \log \det 
    (I + \prcov^{1/2} \fwd^* T(\obs+\tau V)\fwd \prcov^{1/2} ) \text{ (by \eqref{eq:objective})}\\
    % 
    % 
    % 
    &= \frac12 \tr{( I + \prcov^{1/2} \fwd^* \obs^* \Sigma^{-1}
    \obs\fwd \prcov^{1/2} )^{-1}
    \frac{\der}{\der \tau} \Big |_{\tau=0}
    \prcov^{1/2} \fwd^* T(\obs+\tau V) \fwd \prcov^{1/2}}\ \text{ (Lemma \ref{lemma:lax})}\\
    % 
    % 
    % 
    &= \frac12 \ttr\Big \{ \postcov \fwd^* (V^* \Sigma^{-1} \obs 
    - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
    &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs 
    + \obs^* \Sigma^{-1} V ) \fwd \Big \}  \text{ (Lemma \ref{lemma:aux calc})}\\
    %
    %
    %
    &= \tr{\postcov \fwd^* ( \obs^* \Sigma^{-1} V -
    \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs ) \fwd} \text{
        (cyclic property of trace)} \\
    %
    %
    % 
    &= \tr{\postcov \fwd^* \obs^* \Sigma^{-1} V 
    ( I - \modcov \obs^* \Sigma^{-1}\obs ) \fwd} \\
    % 
    %
    %
    &= \tr{V ( I - \modcov \obs^* \Sigma^{-1}\obs )
    \fwd \postcov \fwd^* \obs^* \Sigma^{-1}}.
  \end{split}
\end{align} 

Denote
\begin{align}\label{eq:tar grad}
  \nabla \tar(\obs) &:= (I - \modcov \obs^* \Sigma^{-1} \obs) \fwd
  \postcov \fwd^* \obs^*\Sigma^{-1}.
\end{align}
We refer to $\nabla \tar(\obs)$ as the gradient of $\tar (\obs )$, and
we will now justify this name. Since the trace of $A := V \nabla
\tar(\obs) \in \R^{m \times m}$ is just $\ttr A = \sum_{j=1}^m e_j^t A
e_j$ (with $e_j$ the $j$th standard basis vector), we see that
\begin{align*}
  \delta \tar(\obs)V = \tr{V \nabla \tar(\obs)} = \sum_{j=1}^m
  V_j(\nabla \tar(\obs)_j),
\end{align*}
with $V_j \in \hilo^*$ and $\nabla \tar(\obs)_j \in \hilo^{**} =
\hilo, j=1,\dots,m$. Thus, $\nabla \tar( \obs ) \in \hilo^m$ is indeed
the correct gradient and the notation \eqref{eq:tar grad} is
justified. The gradient $\nabla \tar(\obs)$ should be viewed as a row
vector, as the product $V \nabla \tar$ is in $\R^{m \times m}$. This
will prove important in section \ref{subsec:necessary}.

\subsection{Unit Norm Constraints and their Gradients}\label{subsec:unit norm}
In a real-life optimal design problem we cannot choose any measurement
operator $\obs$. In order to facilitate analysis, we seek reasonable
constraints on $\obs$ for which finding a D-optimal design is
analytically tractable. We start with a proposition, whose proof is
delegated to the appendix.

\begin{restatable*}{proposition}{biggerbetter}\label{prop:bigger better}
  Let $\obs = (\meas_1,\dots,\meas_m)^t$, $j \in \{1,\dots,m\}$,
  $\sigma^2 > 0$ and $|\zeta| > 1$. Then $\tar(\obs)$ increases if we
  use $\zeta \meas_j$ in $\obs$ instead of $\meas_j$.
\end{restatable*}

%% the kind of sensors we have at our disposal and the properties of
%% $\hilo$, as discussed in section \ref{subsec:dynamics}.
 %% Viewed another way, it increases the Signal to Noise Ratio
%% (SNR) --- which is always desirable when taking any observation
%% prone to error.  This can be understood easily from the formulation
%% of the inverse problem $\data = \obs \fwd \param + \obs \eps' +
%% \eps$, where $\eps'$ can be taken as model error and $\eps$ is iid
%% observation error.

From Proposition \ref{prop:bigger better} we understand that the norm
of obsevation vectors must be bounded. Otherwise, vectors whose norms
approach infinity are optimal. This absurd situation is equivalent to
ignoring the iid observation noise term $\eps$. The norm of point
evaluations, for example, is always one\footnote{Of course, point
evaluations are not in any Hilbert space of functions we wish to
consider.}:
\begin{align*}
  \| \delta_{\x} \| = \sup_{0 \neq u \in C(\Omega)} \frac{
    |\int_{\Omega}u(\y) \delta_{\x}(\y) \der \y| 
  }{
    \sup|u|}
  = \sup_{0 \neq u \in C(\Omega)} \frac{|u(\x)|}{ \sup|u|} = 1,
  \forall \x \in \Omega.
\end{align*}

Thus, it is reasonable to consider observation vectors with unit
$\hilo^*$ norm. The unit norm constraints can be written using
\eqref{eq:obs*} as a series of $m$ equality constraints (one for each
observation) on $\obs$ as
\begin{align}\label{eq:constraints}
  \phi_j(\obs) :=\frac12 \| \obs^* e_j\|_{\hilp}^2 - \frac12 = 0,j=1,\dots,m.
\end{align}
The first variations of the unit norm constraints are:
\begin{align*}
  \delta \phi_j(\obs)V  
  &= \frac12\lim_{\tau \to 0}\tau^{-1}
  ( \|(\obs + \tau V)^*e_j \|_{\hilp}^2 - \|\obs ^*e_j \|_{\hilp}^2  ) \\
  %
  %
  %
  &= \frac12\lim_{\tau \to 0}\tau^{-1}
  ( \langle (\obs + \tau V)^*e_j, (\obs + \tau V)^*e_j \rangle_{\hilp} - 
  \langle \obs^*e_j, \obs^*e_j \rangle_{\hilp} ) \\
  % 
  % 
  %
  &= \frac12\lim_{\tau \to 0}\tau^{-1}
  (2\tau \langle \obs^*e_j,V^*e_j \rangle_{\hilp} 
  +\tau^2 \langle V^*e_j, V^*e_j \rangle_{\hilp} ) \\
  %
  %
  % 
  &= \langle \obs^*e_j,V^*e_j \rangle_{\hilp} \\
  %
  %
  % 
  &= \langle V \obs^*e_j,e_j \rangle_{\R^m} \\
  %
  %
  %
  &= e_j^t V \obs^* e_j \\
  % 
  %
  %
  &= \tr{V \obs^* e_je_j^t}.
\end{align*}
The same arguments justifying \eqref{eq:tar grad} hold here, and thus:

\begin{align}\label{eq:grad constraints}
\nabla \phi_j(\obs) = \obs^* e_j e_j^t = \meas_j e_j^t , j=1,\dots,m,
\end{align}
where $\nabla \phi_j(\obs) \in \hilo^m$. As noted at the end of
Section \ref{section:objective}, the gradient $\nabla \phi_j(\obs)$ is
best thought of as a row vector.

\subsection{Necessary Conditions for D-Optimality}\label{subsec:necessary}
We find necessary conditions for D-optimality via Lagrange
multipliers:
\begin{align}
  &\nabla \tar(\obs) = \sum_{j=1}^m \xi_j \nabla \phi_j (\obs)
  \label{eq:Lagrange mult1} \\
    &\phi_j(\obs) = 0, j = 1,\dots,m. \label{eq:Lagrange mult2}
\end{align}

%% Recall where the objects involved are defined: the observation
%% operator, $\obs \in (\hilo^*)^m$. The objective $\tar: (\hilo^*)^m \to
%% \R$ and its gradient is $\nabla \tar(\obs) \in \hilo^m$. Similarly,
%% $\phi_j: (\hilo^*)^m \to \R$ and $\nabla \phi_j(\obs) \in \hilo^m$.

Substituting \eqref{eq:tar grad} and \eqref{eq:grad constraints} in
\eqref{eq:Lagrange mult1}:
\begin{equation}\label{eq:constrained}
  (I - \modcov \obs^* \Sigma^{-1} \obs) \fwd \postcov \fwd^* \obs^*\Sigma^{-1}
  = \sum_{j=1}^m \xi_j \obs^* e_je_j^t = (\xi_1 \meas_1,\dots,\xi_m \meas_m).
\end{equation}
Let $\Xi := \diag(\xi_j)$. We can write \eqref{eq:constrained} more
compactly as:

\begin{theorem}[Necessary conditions for D-Optimality]\label{thm:constrained}
  Let:
  \begin{equation*}
    \obs = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs).
  \end{equation*}
  
  Then:
  \begin{equation*}
    ( I - \modcov \obs^* \Sigma^{-1} \obs) \fwd \postcov \fwd^* \obs^*  \Sigma^{-1}
    = \obs^* \Xi.
  \end{equation*}
\end{theorem}




