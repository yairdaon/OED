\section{The Constrained Optimization Problem of D-Optimal Design}\label{section:D and grad}
dfasdfasdaWe seek a formulation of the D-optimal design problem via Lagrange
multipliers. We first find the gradient of $\tar$, then we suggest
unit-norm constraints on $\obs$ and find their gradients. Results of
this section are summarized in Theorem \ref{thm:constrained}. First,
recall that:
\begin{definition}\label{def:var}
  Let $F$ a real valued function of $\obs$. The first variation of $F$
  at $\obs$ in the direction $V$ is:
  \begin{equation*}
    \delta F(\obs) V := \frac{\der}{\der \tau}\Big |_{\tau=0}  F( \obs + \tau V).
  \end{equation*}

  Moreover, if
  \begin{equation*}
    \delta F(\obs) V = \tr{\nabla F(\obs) V},
  \end{equation*}
  then we call $\nabla F(\obs)$ the gradient of $F$ at $\obs$. 
\end{definition}

%% Gradients are best thought of as row vectors. This will prove
%% important in section \ref{subsec:necessary}.


%% \subsection{The gradient of $\tar$}\label{section:objective}

\begin{proposition}\label{prop:tar grad}
  The gradient of the D-optimality objective $\tar$ is
  \begin{equation*}
    %% \delta \tar(\obs) V = \tr{V ( I - \modcov \obs^* \Sigma^{-1}\obs )
    %%   \fwd \postcov \fwd^* \obs^* \Sigma^{-1}}.
    \nabla \tar(\obs) = ( I - \modcov \obs^* \Sigma^{-1}\obs ) \fwd
    \postcov \fwd^* \obs^* \Sigma^{-1}
  \end{equation*}
\end{proposition}

The proof amounts to calculating the variational derivative of $\tar$
at $\obs$ for any direction $V$ (by Definition \ref{def:var}) and is
delegated to the Supplementary:
\begin{equation*}
  \delta \tar(\obs) V := \frac{\der}{\der\tau} \Big |_{\tau=0} \tar(\obs + \tau V).
\end{equation*}


In a real-life optimal design problem we cannot choose any measurement
operator $\obs \in (\hilo^*)^m$. In order to facilitate analysis, we
seek reasonable constraints on $\obs$ for which finding a D-optimal
design is analytically tractable. The following proposition will guide
us in finding such constraints.

\begin{proposition}\label{prop:bigger better}
  Let $\obs = (\meas_1,\dots,\meas_m)^t$, $j \in \{1,\dots,m\}$,
  $\sigma^2 > 0$ and $|\zeta| > 1$. Then $\tar(\obs)$ increases if we
  use $\zeta \meas_j$ in $\obs$ instead of $\meas_j$.
\end{proposition}

\begin{proof} 
  Fix $j=1,\dots,m$ and take $V:= e_j e_j^t \obs$. For $u
  \in \hilo$:
  \begin{equation*}
    Vu = e_je_j^t (\meas_1(u),\dots,\meas_m(u) )^t = e_j \meas_j(u)
    = (0,\dots,0,\meas_j(u),0,\dots,0)^t.
  \end{equation*}
  %% This way, $V$ has the same $j$th entry as $\obs$ while the rest
  %% are set to zero.
  We now calculate the variation of $\tar$ at $\obs$ in the direction
  of $V$. Denote $\tmp: = \fwd \postcov \fwd^*$. From Proposition
  \ref{prop:tar grad}:
  \begin{align*}
     \delta \tar(\obs) V 
    &= \tr{V ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}} \\
    % 
    %
    %
    &= \tr{e_je_j^t \obs ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}} \\
    %
    % 
    %
    &= e_j^t \obs ( I - \modcov \obs^*\Sigma^{-1}\obs) \tmp \obs^* \Sigma^{-1}e_j \\
    %
    % 
    %
    &= e_j^t ( I - \obs \modcov \obs^*\Sigma^{-1})\obs \tmp \obs^* \Sigma^{-1}e_j \\  
    % 
    %
    %
    &=  e_j^t(\Sigma-\obs \modcov \obs^*) \Sigma^{-1}\obs \tmp \obs^* \Sigma^{-1}e_j \\
    %
    %
    %
    &=\sigma^2 e_j^t \Sigma^{-1}\obs \tmp \obs^* \Sigma^{-1}e_j
    \text{ by \eqref{eq:Sigma} }\\
    %
    % 
    %
    &=\sigma^2 e_j^t \Sigma^{-1}\obs \fwd \postcov \fwd^* \obs^* \Sigma^{-1}e_j.
  \end{align*} 
  Since $\postcov$ is positive definite, we conclude that $\delta
  \tar(\obs) V > 0$. This means that increasing the magnitude of the
  $j^{\text{th}}$ measurement functional increases $\tar(\obs)$.
\end{proof}

Proposition \ref{prop:bigger better} implies that it is a good idea to
bound the norm of measurements. If, for example, we can take
measurements in $span\{\meas\}$ for some $\meas \neq 0$, then a
maximum for the D-optimality criterion does not exist, as it is not
bounded. In contrast, in any real-life problem where sensors are
concerned, the norm of measurements recorded by sensors is always one
(of course, point evaluations are not in any Hilbert space of
functions we wish to consider):

\begin{equation}
  \| \delta_{\x} \| = \sup_{0 \neq u \in C(\Omega)}
  \frac{
    |\int_{\Omega}u(\y) \delta_{\x}(\y) \der \y|
  }{
    \sup|u|
  } = \sup_{0 \neq u \in C(\Omega)} \frac{|u(\x)|}{\sup|u|} = 1,
  \forall \x \in \Omega.
\end{equation}

Thus, it is reasonable to consider measurements with unit $\hilo^*$
norm. We can write the unit norm constraints as a series of $m$
equality constraints (one for each measurement) on $\obs$. We define
them and find their gradients in Proposition \ref{prop:constraints
  grad} below:

\begin{proposition}\label{prop:constraints grad}
  Let
  \begin{align*}
    \phi_j(\obs) :=\frac12 \| \obs^* e_j\|_{\hilp}^2 - \frac12 = 0,\ j=1,\dots,m.
  \end{align*}
  Then
  \begin{equation*}
    %% \delta \phi_j(\obs)V = \tr{V \obs^* e_je_j^t}.
    \nabla \phi_j(\obs) = \obs^* e_je_j^t.
  \end{equation*}
\end{proposition}

The proof is straightforward and we delegate it to the Supplementary.

%% The same arguments justifying \eqref{eq:tar grad} hold here, and thus:

%% \begin{align}\label{eq:grad constraints}
%% \nabla \phi_j(\obs) = \obs^* e_j e_j^t = \meas_j e_j^t , j=1,\dots,m,
%% \end{align}
%% where $\nabla \phi_j(\obs) \in \hilo^m$. As noted at the end of
%% Section \ref{section:objective},

%% The gradients $\nabla \tar(\obs)$ and $\nabla \phi_j(\obs)$ are best
%% thought of as row vectors.


We find necessary first-order conditions for D-optimality via Lagrange
multipliers:

\begin{align}
  &\nabla \tar(\obs) = \sum_{j=1}^m \xi_j \nabla \phi_j (\obs)
  \label{eq:Lagrange mult1} \\
    &\phi_j(\obs) = 0, j = 1,\dots,m. \label{eq:Lagrange mult2}
\end{align}

We now substitute the gradients calculated in Propositions
\ref{prop:tar grad} and \ref{prop:constraints grad} into
\eqref{eq:Lagrange mult1}:
\begin{equation}\label{eq:constrained}
  (I - \modcov \obs^* \Sigma^{-1} \obs) \fwd \postcov \fwd^* \obs^*\Sigma^{-1}
  = \sum_{j=1}^m \xi_j \obs^* e_je_j^t = (\xi_1 \meas_1,\dots,\xi_m \meas_m).
\end{equation} 
Letting $\Xi := \diag(\xi_j)$, we can write \eqref{eq:constrained} and
\eqref{eq:Lagrange mult2} more compactly as:

\begin{theorem}[Necessary conditions for D-Optimality]\label{thm:constrained}
  Let:
  \begin{equation*}
    \opt = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs).
  \end{equation*}
  
  Then:
  \begin{equation*}
    ( I - \modcov \opt^* \Sigma^{-1} \obs) \fwd \postcov \fwd^* \opt^*  \Sigma^{-1}
    = \obs^* \Xi, 
  \end{equation*}
  where $\Xi \in \mathbb{R}^{m \times m}$ is diagonal.
\end{theorem}




