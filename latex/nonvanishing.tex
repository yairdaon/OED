%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION Analysis of Optimal Designs --- Non-Vanishing Model Error
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sensor Clusterization --- Non-Vanishing Model Error}\label{section:non vanishing}
We now show that if $\modcov \neq 0$ clusterization will not
occur. Previous authors have demonstrated that model error mitigates
the clusterization phenomenon \cite{ucinski, koval}, but we prove this
rigorously. Let $\obs = (\meas_1,\dots,\meas_m)^t$ and $\obsm :=
(\meas_1,\dots,\meas_{m-1})^t$. Denote $\Sigmam := \Sigma (\obsm)$ and
$\postcovm$ the posterior covariance that arises when $\obsm$ is
utilized as an observation operator.

\begin{lemma}[Increase due to an observation]\label{lemma:design increase}
  Let $\obs = (\meas_1,\dots,\meas_m)^t$ and $\obsm :=
  (\meas_1,\dots,\meas_{m-1})^t$. Then
  \begin{align*}
    \tar( \obs ) - \tar (\obsm ) &=
    \frac12 \log \left ( 1 + \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m \rangle
    }{
      \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
    }       
    \right ).
  \end{align*}
\end{lemma}

\begin{proof}
  Let
  \begin{align*}
    \Sigma( \obs ) &= 
    \begin{bmatrix}
      \Sigma (\obsm )           & \obsm \modcov \meas_m \\
      \meas_m \modcov \obsm^*   & \sigma^2 + \meas_m \modcov \meas_m
    \end{bmatrix}
    : =
    \begin{bmatrix}
      \Sigmam   & w \\
      w^t       & c
    \end{bmatrix}\\
    %
    %
  \end{align*}

  The Schur complement implies:
  \begin{align*}
    \Sigma^{-1} &=
    \begin{bmatrix}
      \Sigmam^{-1} + \Sigmam^{-1} w ( c - w^t \Sigmam^{-1} w)^{-1} w^t \Sigmam^{-1} & - \Sigmam^{-1} w ( c - w^t \Sigmam^{-1} w)^{-1} \\
      -( c - w^t \Sigmam^{-1} w)^{-1} w^t \Sigmam^{-1}                            &  ( c - w^t \Sigmam^{-1} w)^{-1}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      \Sigmam^{-1} & 0 \\
      0           & 0 
    \end{bmatrix}
    + (c -w^t \Sigmam^{-1} w )^{-1}
    \begin{bmatrix}
      \Sigmam^{-1} w \\
      -1
    \end{bmatrix}
    \begin{bmatrix}
      w^t \Sigmam^{-1} & -1 
    \end{bmatrix}
  \end{align*}
  %
  Denote:
  %
  \begin{align*}
    \M (\obs ):&= \prcov^{\frac12}\fwd^* \obs^* \Sigma^{-1} \obs \fwd
    \prcov^{\frac12}.
  \end{align*}
  
  Note that:
  \begin{align*}
    \M(\obs) &= \prcov^{1/2} \fwd^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2} \\
    %
    %
    %
    &= \prcov^{1/2} \fwd^* \obs^* \left \{
    \begin{bmatrix}
      \Sigmam^{-1} & 0 \\
      0           & 0 
    \end{bmatrix}
    + (c -w^t \Sigmam^{-1} w )^{-1}
    \begin{bmatrix}
      \Sigmam^{-1} w \\
      -1
    \end{bmatrix}
    \begin{bmatrix}
      w^t \Sigmam^{-1} & -1 
    \end{bmatrix} 
    \right \} \obs \fwd \prcov^{1/2} \\
    %
    %
    %
    &= \M (\obsm) + (c -w^t \Sigmam^{-1} w )^{-1}
    \prcov^{1/2} \fwd^* \obs^*
    \begin{bmatrix}
      \Sigmam^{-1} w \\
      -1
    \end{bmatrix}
    \begin{bmatrix}
      w^t \Sigmam^{-1} & -1 
    \end{bmatrix} 
    \obs \fwd \prcov^{1/2}
  \end{align*}
  %
  Now, denote:
  %
  \begin{align}\label{eq:u}
    \begin{split}
      u :&= (c -w^t \Sigmam^{-1} w )^{-1/2}
      \prcov^{1/2} \fwd^* \obs^* 
      \begin{bmatrix}
        \Sigmam^{-1} w \\
        -1 
      \end{bmatrix} \\
      %
      %
      %
      & = (c -w^t \Sigmam^{-1} w )^{-1/2} ( \prcov^{1/2}\fwd^* \obsm^* \Sigmam^{-1} \obsm  \modcov \meas_m - \prcov^{1/2} \fwd^* \meas_m )\\
      %
      %
      %
      u^* :&=  (c -w^t \Sigmam^{-1} w )^{-1/2} (\meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \fwd \prcov^{1/2} - \meas_m \fwd \prcov^{1/2} ),
    \end{split}
  \end{align}
  %
  so that
  %
  \begin{equation}\label{eq:M plus I}
    I + \M( \obs ) = I + \M (\obsm ) + uu^*.
  \end{equation}
  %
  Note that
  \begin{equation}\label{eq:M postcov}
    \prcov^{1/2} \left (I + \M( \obsm ) \right )^{-1} \prcov^{1/2} = \postcovm.
  \end{equation}
  Next, use an analogue of the matrix determinant lemma (Lemma
  \ref{lemma:MDL}), whose statement and proof are recorded in the
  appendix:
  %
  \begin{align}\label{eq:diffs}
    \begin{split}
      \tar( \obs ) - \tar( \obsm )
      %
      %
      %
      &= \frac12 \log \det \Big ( I + \M ( \obs ) \Big ) / \det \Big ( I + \M (\obsm) \Big ) \\
      %
      %
      %
      &= \frac12  \log \det \left ( I + \M(\obsm) + uu^* \right ) / \det \Big ( I + \M (\obsm) \Big ) \\
      %
      %
      %
      &= \frac12 \log \left ( 1 + \left \langle \left ( I+\M(\obsm) \right )^{-1} u, u  \right \rangle \right ) \text{ (Lemma \ref{lemma:MDL})}.
    \end{split}
  \end{align}
  From \eqref{eq:u} and \eqref{eq:M postcov}:
  \begin{align}\label{eq:final}
    \begin{split}
      &\left \langle \left (I+\M (\obsm)\right )^{-1}u, u \right \rangle\\
      &= \frac{
        \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m,
        (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m \rangle
      }{
        c- w^t \Sigmam^{-1} w
      }\\
      %
      %
      %
      &= 
      \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m \rangle
      }{
        \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
      }
    \end{split}
  \end{align}
  and the conclusion follows by substituting \eqref{eq:final} into
  \eqref{eq:diffs}.
\end{proof}



\begin{corollary}\label{cor:same meas}
  If $\meas_m = \meas_j$ for some $1 \leq j \leq m-1$, then
  \begin{equation*}
    \tar(\obs) - \tar(\obsm) =
    \log \left ( 1 + \frac{\sigma^2
      \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1} e_j,
      \obsm^* \Sigmam^{-1}e_j \rangle
    }{
      2 - \sigma^2 e_j^t\Sigmam^{-1}e_j 
    }       
    \right ),
  \end{equation*}
  where $e_j\in \mathbb{R}^{m-1}$ is the $j^{\text{th}}$ standard unit
  vector.
\end{corollary}

\begin{proof} \label{cor:same meas proof}
  Denote $A:= \obs \modcov \obs^*$ and $v_j$ the $j$th column of $A$.
  Note that $v_j = \obsm \modcov \meas_m$, since $(\obsm \modcov
  \obsm^*)_{ij} = \meas_i(\modcov \meas_j)$, as explained in
  \eqref{eq:modcov explained}. One can now verify that
  \begin{equation}\label{eq:observation}
    \Sigmam^{-1} \obsm \modcov \meas_m = \Sigmam^{-1}v_j = (A +\sigma^2I_{m-1})^{-1} v_j =
    e_j -\sigma^2 \Sigmam^{-1}e_j.
  \end{equation}
  %
  Using \eqref{eq:observation}:
  \begin{align}\label{eq:denominator}
    \begin{split}
      \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m
      &= \meas_m \modcov \obsm^* ( e^j - \sigma^2 \Sigmam^{-1} e_j )\\
      %
      %
      %
      &= \meas_m \modcov \meas_j - \sigma^2 \meas_m \modcov \obsm^* \Sigmam^{-1}e_j \\
      %
      %
      %
      &= \meas_m \modcov \meas_j -\sigma^2 (e_j - \sigma^2 \Sigmam^{-1}e_j)^t e_j \\
      %
      %
      %
      &= \meas_m \modcov \meas_m -\sigma^2 + \sigma^4 e_j^t\Sigmam^{-1}e_j.
    \end{split}
  \end{align}
  We use \eqref{eq:observation} to simplify the terms in the enumerator of
  the conclusion of Lemma \ref{lemma:design increase}:
  \begin{align}\label{eq:enumerator}
    \begin{split}
      (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m
      &= \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m - \meas_m \\
      %
      %
      %
      &= \obsm^* (e_j - \sigma^2 \Sigmam^{-1} e_j) -\meas_j \\ 
      %
      %
      %
      &= -\sigma^2 \obsm^* \Sigma^{-1}e_j. 
    \end{split}
  \end{align}
  %
  Substitute \eqref{eq:enumerator} and \eqref{eq:denominator} to
  the enumerator and denominator (respectively) of the conclusion of
  Lemma \ref{lemma:design increase}:
  %
  \begin{align*}
    \tar( \obs ) - \tar (\obsm ) &=
    \log \left ( 1 + \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m \rangle
    }{
      \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
    }       
    \right ) \\
    %
    %
    %
    &= \log \left ( 1 + \frac{\sigma^4
      \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1} e_j,
      \obsm^* \Sigmam^{-1}e_j \rangle
    }{
      2\sigma^2 - \sigma^4 e_j^t\Sigmam^{-1}e_j 
    }       
    \right ) \\
    %
    %
    %
    &= \log \left ( 1 + \frac{\sigma^2
      \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1} e_j,
      \obsm^* \Sigmam^{-1}e_j \rangle
    }{
      2 - \sigma^2 e_j^t\Sigmam^{-1}e_j 
    }       
    \right ).
  \end{align*}
\end{proof}


Recall from \eqref{eq:Sigma} that $\Sigma(\obs) = \obs
\modcov \obs^* + \sigma^2I$ and let $u := \obsm^*
\Sigmam^{-1}e_j$. Then

\begin{align*}
  \begin{split}
    \lim_{\sigma^2 \to 0} u &= \obsm^*(\obsm \modcov \obsm^*)^{-1}e_j\\
    \lim_{\sigma^2 \to 0} \postcovm &= (\prcov^{-1} + \fwd^* \obsm^* (\obsm \modcov \obsm^*)^{-1} \obsm \fwd)^{-1} \text{ (From \eqref{eq:postcov})}.
  \end{split}
\end{align*}

Consequently, 
\begin{equation*}
   \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1}
    e_j, \obsm^* \Sigmam^{-1}e_j \rangle 
  %
  %
  = \langle \fwd \postcovm \fwd^* u, u \rangle
\end{equation*}

is bounded, and

\begin{equation*}
\lim_{\sigma^2 \to 0} \tar(\obs) -\tar(\obsm) = 0.
\end{equation*}

We have shown that in the limit $\sigma^2 \to 0$, no increase in
$\tar$ is achieved by repeating an observation, so designs that
exhibit sensor clusterization are not D-optimal. In conclusion, for
small observation error levels, sensor clusterization is mitigated by
the presence of a non-zero model error\footnote{Since the design
criterion is not defined for $\sigma^2 = 0$ and identical
observations, we cannot make a statement regarding $\sigma^2 = 0$,
except in the limiting sense described above.}.
