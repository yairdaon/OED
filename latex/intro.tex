\section{Introduction}\label{section:OED intro}
Measurements of physical processes are indespensable when considering
an inverse problem, choosing an optimal set of measurements is of
utmost importance. In applications, one may have to specify electrode
locations on the skin (e.g.\ in electricat impedance tomography
\cite{horesh2010impedance}), certain wavelengths (e.g.\ in MRI
\cite{horesh2008mri}), or location of sensors for waves reflecting
from the ground (e.g.\ searching for oil
\cite{horesh2008borehole}). Whatever the allowed set of measurements
is, one should select the optimal measurements to take, in order to
increase accuracy, reduce costs, or both. A set of measurements is
called a \emph{design}, and an \emph{optimal design} is found by
optimizing some \emph{design criterion}. Two of the most popular
desgin criteria are A- and D-optimality (see \cite{Chaloner1995} for a
thorough review).

Surprisingly, optimal designs sometimes result in measuremets that are
very similar \cite{fedorov1996, hooker2009, fedorov2012, Ucinski05,
  neitzel2019sparse}. As an example, consider the inverse problem of
inferring the initial condition of the 1D heat equation (details in
Section \ref{subsec:example}). In Figure \ref{fig:clusterization
  illustration} we see that an optimal design results in two sets of
sensors that are almost indistinguishable from each other. Following
\cite{Ucinski05}, we refer to this phenomenon as \emph{sensor
clusterization}.

\begin{figure}
  \begin{tikzpicture}[thick, scale=1.3, every node/.style={scale=0.99}]
    \begin{axis}
      [
      title={Posterior Pointwise Standard Deviations and D-Optimal Sensor Locations},  
      xmin = 0,
      xmax = 3.14,
      xlabel = {$x \in \Omega$},
      ylabel = posterior std,
      ymin   = 0,
      %compat = 1.3,
      % ymax   = 130,
      % ytick = \empty,
      legend cell align=left,
      % legend style={at={(0.45,0.2)}}
      legend pos= outer north east 
      ]
      % \draw[black!30!white, thin] (50,0) -- (50,130);
      % 
      %% \addplot [thin, black, mark=none] table{stdv-heat-sens1-var1.txt};
      %% \addlegendentry{1 sensors};
      
      %% \addplot [thin, blue, mark=none] table{stdv-heat-sens2-var1.txt};
      %% \addlegendentry{2 sensors};
      
      %% \addplot [thin, red, mark=none] table{stdv-heat-sens3-var1.txt};
      %% \addlegendentry{3 sensors};
      
      \addplot [thin, green, mark=none] table{stdv-heat-sens4-var1.txt};
      \addlegendentry{4 sensors};
      
      \addplot [thin, purple, mark=none] table{stdv-heat-sens5-var1.txt};
      \addlegendentry{5 sensors};
      
      \addplot [thin, cyan, mark=none] table{stdv-heat-sens6-var1.txt};
      \addlegendentry{6 sensors};

  
      %% \addplot [black,  only marks, mark=x, mark size=1.5] 
      %% table{locs-heat-sens1-var1.txt}; 
      %% \addplot [blue,   only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens2-var1.txt}; 
      %% \addplot [red,    only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens3-var1.txt};
      \addplot [green,  only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens4-var1.txt}; 
      \addplot [purple, only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens5-var1.txt}; 
      \addplot [cyan,   only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens6-var1.txt}; 
  
      
    \end{axis}
  \end{tikzpicture}
  \caption{Sensor clusterization for inferring the initial condition
    of the 1D heat equation. Posterior pointwise standard deviations
    (lines) are plotted over the computatoinal domain $\Omega = [0,
      \pi]$, for varying numbers of sensors. Sensor locations
    (circles) were chosen to optimize the D-optimality
    criterion. Sensor clusterization occurs for six sensors: Only four
    observation locations are visible, since two pairs of sensors are
    so close together they are indistinguishable.}
  \label{fig:clusterization illustration}
\end{figure}

There is a consensus among researchers that sensor clusterization is
undesirable \cite{fedorov1996, hooker2009, fedorov2012, Ucinski05,
  neitzel2019sparse} and several remedies to sensor clusterization
were suggeted through the years. Merging ``close'' measurements is
possible, \cite{fedorov2012}, but this practice ignores sensor
clusterization altogether. In a \emph{clusterization-free design},
sensor locations are forced to be far from each other
\cite{Ucinski05}. In a frequentist and finite-dimensional setting,
senesor clusterization is avoided by either imposing distance
constraints between measurements, or by introducing correlated errors
which account for both observation error and model error
\cite{Ucinski05}. In time-series analysis for pharmacokinetic
experiments, sensor clusterization is mitigated by modeling
auto-correlation time in the noise \cite{hooker2009}, and this
approach is similar in spirit to assuming errors are correlatied
\cite{Ucinski05}.

%% Sometimes, imposing a correlation structure on observation noise is
%% reasonable indeed \cite{koval2020, Alexanderian2021}. For example,
%% Koval \cite{koval2020} imposes a Gaussian prior on ocean bathytermy
%% when inferring BLA. However, in the (un)fortunate event that ocean
%% bathytermy is known, there is no use to modeling it as an unknown
%% and sensor clusterization will rear its ugly head.

In spatial problems, where sensors are placed in some domain $\Omega
\subseteq \mathbb{R}^d, d=1,2,3$, many scientists bypass the sensor
clusterization problem by restricting measurements to a coarse grid in
$\Omega$ \cite{koval2020, alexanderian2021, attia2020,
  alexanderian2014, alexanderian2016,
  alexanderian2018efficient}. Unfortunately, this practice requires
solvilg a hard combinatorial optimization problem for choosing optimal
sensor locations from a finite set. A solution typically involves
relaxing the problem to choosing optimal measurement ``weight'' in
$[0,1]$ for each potential sensor location. Some authors then add a
sparsifying $\ell_1$ penalty term to the design criterion, which is
subsequently thresholded \cite{horesh2008borehole}, thus resulting in
the desired binary design. Others successively relax the $\ell_1$
penalty to a $\ell_0$ penalty via a continuation method
\cite{alexanderian2016, alexanderian2014}. A clever alternative is to
cast the problem of finding optimal sensor weights as a stochastic
optimization problem, which in turn is solved by stochastic gradient
descent \cite{attia2022stochastic}????. While all of the above
mentioned methods are valid solutions to the prohibitive computational
cost of finding a binary optimal design restricted to a coarse grid,
none treats the elephant in the room: restricting sensor locations to
a coarse grid in $\Omega \subseteq \mathbb{R}^d, d=1,2,3$ necessarily
gives rise to a sub-optimal design.

Avoiding sensor clusterization is pragmatical: we feel sensor
clusterization is bad (but do not know why), and try to avoid
it. However, all of the abovementioned techniques that avoid sensor
clusterization do so by somehow restricting sensor locations, thus
fundamentaly altering the optimal design problem. To the best of my
knowledge, no study tried to answer the following seemingly trivial
and basic questions: why do sensor clusterization occurs? Should
sensor clusterization really be avoided? How imposing a correlation
structure between obsevations mitigates sensor clusterization? Can we
find truly optimal designs that do not exhibit sensor clusterization?

%% However, as far as I am aware, no study concerned the fact that
%% restricting sensors to a coarse grid is, by definition,
%% sub-optimal.


%% Attia \cite{attia2020} focused on QoI, Koval considered model
%% uncertainty and focused on reducing forward and adjoint solves, using
%%  randomized linear algebra \cite{koval2020}.

%% Today, including model errors are a bona-fide????  standard in the
%% inverse problems community: \cite{attia2020}, \cite{koval2020},
%% \cite{Alexanderian2021} all empower their models.  Both studies
%% present numerical techniques for finding optimal designs when model
%% error is present. Both are restricted to linear inverse problems
%% (although in the latter the authors use their method on a nonlinear
%% problem by taking a Laplace approximation for the posterior). Both
%% find an optimal design by first solving a continuous problem for
%% sensor weights. Said solution is then sparsified to give a binary
%% design. Both studies are successful in the task of Bayesian
%% inversion. Neither \cite{attia2020}, nor \cite{koval2020} mention any
%% effect model errors can have on sensor clusterization.

%% Considerable efforts focused on various computational aspects of
%% finding optimal designs in infinite-dimensional Bayesian inverse
%% problems \cite{attia2020, koval2020,
%% AlexanderianPetraStadlerEtAl16, attia2023pyoed, koval2021optimal,
%% attia2020stochastic}, the subject of sensor clusterization was less
%% investigated, and


%% Second, restricting sensors to a such a small subset of the computatoinal
%% domain $\Omega \subseteq \mathbb{R}^d, d=1,2,3$ necessarily gives rise
%% to sub-optimal designs.


\subsection{Contribution}
In this study, we seeks to understand sensor clusterization. We focus
our attention on the Bayesian \emph{D-optimality} criterion: a
D-optimal design maximizes the expected Kullback-Leibler divergence
\cite{CoverThomas91} between posterior and prior
\cite{Chaloner1995}. We study Bayesian D-optimal designs in linear
inverse problems that are (potentially) of infinte dimensions. We
propose and study a relaxed and analytically tractable model for such
D-optimal designs. Under this model, D-optimal designs are solutions
of a constrained optimization problem, formulated using Lagrange
multipliers. The Lagrange multipliers formulation allows us to
rigorously show why model error mitigates clusterization (Section
\ref{section:non vanishing}), like many authors note, and why designs
that exhibit clusterization are indeed optimal when no model error is
considered (Section \ref{section:vanishing}).

the Lagrange multipliers problem for vanishing model error results in
a characterization of D-optimal designs. The key idea is that a
D-optimal design reduces uncertainties where they are highest
first. This is the conclusion of part (5) of Theorem \ref{thm:char},
stated below and proved in Section \ref{section:vanishing}.

\begin{restatable}[D-optimal designs with vanishing model error]{theorem}{char}\label{thm:char}
  %% \begin{theorem}[D-optimal designs with vanishing model error]\label{thm:char}
  %% Let Notation \ref{not:setup}. Further, let

  Let:
  \begin{itemize}
  \item $\hilp, \hilo$ Hilbert spaces.
  \item $\fwd:\hilp \to \hilo$ a linear compact operator.
  \item $\pr \sim \mathcal{N}(0, \prcov)$ prior Gaussian measure on $\hilp$,
    where $\prcov:\hilp \to \hilp$ is the prior covariance operator.
  \item $\obs: \hilo \to \mathbb{R}^m$ observation operator, where $m
    \in \mathbb{N}$ is the number of observations taken. 
  \item $\sigma^2 \in \mathbb{R}_{+}$ observation noise variance,
    $\data = \obs \fwd \param + \eps$, where $\eps \in \mathbb{R}^m$
    is iid $\mathcal{N}(0, \sigma^2)$ noise.
  \item $\post$ the posterior measure, with covariance $\postcov$.
  \item A D-optimality design criterion
    \cite{AlexanderianGloorGhattas14}:
    \begin{align*}
      \begin{split}
        \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det ( I + \sigma^{-2} \prcov^{1/2} \fwd ^*
        \obs^* \obs \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  \item $\{\lambda_i\}_{i=1}^\infty$ eigenvalues of $\fwd\prcov\fwd^*$
    in decreasing order of magnitude.
  %% \item $\{\ev_i\}_{i=1}^\infty$ their corresponding eigenvectors.
  \item $\{\eta_i\}_{i=1}^\infty$ eigenvalues of $\obs^*\obs$.
  \item $\obs$ a D-optimal design operator
    \begin{equation*}
      \obs = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs).
    \end{equation*}
  \end{itemize}

  Then:
  \begin{enumerate}
  \item  $\tr{\obs^*\obs} = m$.
  \item $\obs^*\obs$ and $\fwd\prcov\fwd^*$ are simultaneously
    diagonalizable.
  \item $k := \rank \obs^*\obs \leq m$ and
    \begin{equation*}
      \tar(\obs) = \frac12 \sum_{i=1}^{k} \log (1 + \sigma^{-2}\lambda_i\eta_i). %= \frac12 \sum_{i=1}^{m} \log (1 + \sigma^{-2}\lambda_i\eta_i).
    \end{equation*}
  %% \item 
  %%   \begin{equation*}
  %%     k = \argmax \left \{ k:\lambda_k^{-1} < \sigma^{-2}\frac{m}{k} + \frac{1}{k} \sum_{j=1}^{k}
  %%     \lambda_j^{-1} \right \}.
  %%   \end{equation*}
  \item
    \begin{equation*}
        \eta_i = \begin{cases}
          \frac{m}{k} - \sigma^2 \lambda_i^{-1} + \sigma^2 \frac{1}{k} \sum_{j=1}^k \lambda_j^{-1} & 1 \leq i \leq k \\
          0 & i > k 
        \end{cases}.
    \end{equation*}
  \item The covariance of the pushforwad $\fwd_{*} \post$ is $\left (
    (\fwd \prcov \fwd^*)^{-1} + \sigma^{-2} \obs^*\obs \right )^{-1}$
    and has eigenvalues
    \begin{equation*}
      \theta_i =
      \begin{cases}
        \left(\frac{\sum_{j=1}^k \lambda_j^{-1} + \sigma^{-2}m}{k} \right )^{-1} & i \leq k \\
        \lambda_i &  i > k 
      \end{cases}
    \end{equation*}
  \end{enumerate}
%\end{theorem}
\end{restatable}

An interesting byproduct of the proof of Theorem \ref{thm:char} is a
novel lemma in linear algebra: For a symmetric positive definite
matrix $M \in \mathbb{R}^{k \times k}$ with $\ttr M = m \in
\mathbb{N}$, we decompose $M = AA^t$, where $A$ has unit norm columns
(Lemma \ref{lemma:free}). We also generalize two lemmas from linear
algebra to infinite-dimensional settings, see Lemmas \ref{lemma:MDL}
and \ref{lemma:lax}.

%%  we prove a matrix determinant lemma in Hilbert spaces, and in
%%  Lemma \ref{lemma:lax} we generalize a lemma for calculating
%%  $\frac{\der}{\der t} \log \det (I + X(t))$, where $X(t)$ is an
%%  operator valued function \cite{Lax07}.

\subsection{Limitations}\label{subsec:limitations}
The main limitation of this study is that our generic model does not
correspond to any specific real-life problem. It is generic enough to
be analytically tractable, but one may argue it is too far removed
from any real application. To these claims I would argue that
Mathematicians have a long history of studying models that are
bare-bones simplifications of real systems. E.g. the Ising model
\cite{cipra1987}, the Lorenz system \cite{brin}, and the SIR model
\cite{keeling2008}, to name a few.

%% The second drawback of our model is that we do not show rigorously
%% that clusterization necessarily occurs, we only show that it does
%% not pose any obstruction to D-optimality.


\subsection{An Example of Clusterization}\label{subsec:example}
For the purpose of illustration, consider a toy inverse problem ---
inferring the initial condition of the 1D heat equation in $\Omega :=
[0,\pi]$ with a homogeneous Dirichlet boundary condition. Sensor
clusterization is clearly evident, see Figure \ref{fig:clusterization
  illustration}.

The 1D heat equation with homogeneous Dirichlet boundary condition is:
\begin{subequations}\label{eq:heat equation}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,\pi] \times [0,\infty),\\
      u &= 0 &&\qquad \text{on } \{0, \pi\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,\pi] \times \{0\}.
  \end{alignat}
\end{subequations}


The goal is to infer the initial condition $u_0$. Model the initial
condition as $u_0 \sim \normal(0,\prcov)$, for $\prcov =
(-\Delta)^{-1}$ with a homogeneous Dirichlet boundary condition. Fix
some final time $T$ and denote by $\fwd$ the forward operator, so that
$u(\cdot, T) = \fwd u_0$. Let $\obs$ the observation operator, so that
$u(\x_j,T) = (\obs u)_j, j=1,\dots,m$. Assume iid centered Gaussian
observation error, so observations are given by $u(\x_j,T) +
\eps(\x_j)$ with $\eps(\x_j) \sim \normal(0, \sigma^2)$ iid, $\sigma^2
> 0$. The posterior covariance is known and depends only on $\prcov,
\fwd, \obs$ and $\sigma^2$ \cite{Stuart10} (see Section
\ref{section:prelim} and \eqref{eq:postcov} below). Finally, choose
$\x_j,j=1,\dots,m$ to optimize the D-optimality criterion, see Theorem
\ref{thm:d optimality} \cite{AlexanderianGloorGhattas14}.

Sensor clusterization is illustrated in Figure \ref{fig:clusterization
  illustration}. The posterior covariance does not depend on data, so
the plot has no reference to actual data observed. The posterior
covariance does depend on location of the observation taken, which
were chosen to optimize the D-optimality criterion. Sensor
clusterization is clearly seen for six sensors; it looks like only
four observations were taken. Two pairs of sensors are clustered so
close together that they are indistinguishable from each other.


%% A D-optimal design
%% is found via a generalization of the D-optimality criterion to
%% infinite dimension(also, Section
%% \ref{subsec:D optimal design} below).
