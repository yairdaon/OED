\section{Introduction}\label{section:OED intro}
Experimental design is an important part of many scientific
investigations. When considering an inverse problem, one can often
specify sensor locations (e.g.\ in electricat impedance tomography
\cite{horesh2010impedance}), certain wavelengths (e.g.\ in MRI
\cite{horesh2008mri}), or location of sensors for waves reflecting
from the ground (e.g.\ searching for oil
\cite{horesh2008borehole}). Whatever the allowed set of measurements
is, one should select the optimal measurements to take, in order to
increase accuracy, reduce costs, or both. A set of measurements is
called a \emph{design}, and an \emph{optimal design} is found by
optimizing some \emph{design criterion}. The most popular desgin
criteria are A- and D-optimality

Surprisingly, optimal designs sometimes result in measuremets that are
very similar \cite{fedorov1996, hooker2009, fedorov2012, Ucinski05,
  neitzel2019sparse}. As an example, consider the inverse problem of
inferring the initial condition of the 1D heat equation (details in
Section \ref{subsec:example}). In Figure \ref{fig:clusterization
  illustration} we see that an optimal design results in two sets of
sensors that are almost indistinguishable from each other. Following
\cite{Ucinski05}, we refer to this phenomenon as \emph{sensor
clusterization}.

\begin{figure}
  \begin{tikzpicture}[thick, scale=1.3, every node/.style={scale=0.99}]
    \begin{axis}
      [
      title={Posterior Pointwise Standard Deviations and D-Optimal Sensor Locations},  
      xmin = 0,
      xmax = 3.14,
      xlabel = {$x \in \Omega$},
      ylabel = posterior std,
      ymin   = 0,
      %compat = 1.3,
      % ymax   = 130,
      % ytick = \empty,
      legend cell align=left,
      % legend style={at={(0.45,0.2)}}
      legend pos= outer north east 
      ]
      % \draw[black!30!white, thin] (50,0) -- (50,130);
      % 
      %% \addplot [thin, black, mark=none] table{stdv-heat-sens1-var1.txt};
      %% \addlegendentry{1 sensors};
      
      %% \addplot [thin, blue, mark=none] table{stdv-heat-sens2-var1.txt};
      %% \addlegendentry{2 sensors};
      
      %% \addplot [thin, red, mark=none] table{stdv-heat-sens3-var1.txt};
      %% \addlegendentry{3 sensors};
      
      \addplot [thin, green, mark=none] table{stdv-heat-sens4-var1.txt};
      \addlegendentry{4 sensors};
      
      \addplot [thin, purple, mark=none] table{stdv-heat-sens5-var1.txt};
      \addlegendentry{5 sensors};
      
      \addplot [thin, cyan, mark=none] table{stdv-heat-sens6-var1.txt};
      \addlegendentry{6 sensors};

  
      %% \addplot [black,  only marks, mark=x, mark size=1.5] 
      %% table{locs-heat-sens1-var1.txt}; 
      %% \addplot [blue,   only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens2-var1.txt}; 
      %% \addplot [red,    only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens3-var1.txt};
      \addplot [green,  only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens4-var1.txt}; 
      \addplot [purple, only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens5-var1.txt}; 
      \addplot [cyan,   only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens6-var1.txt}; 
  
      
    \end{axis}
  \end{tikzpicture}
  \caption{Sensor clusterization for inferring the initial condition
    of the 1D heat equation. Posterior pointwise standard deviations
    (lines) are plotted over the computatoinal domain $\Omega = [0,
      \pi]$, for varying numbers of sensors. Sensor locations
    (circles) were chosen to optimize the D-optimality
    criterion. Sensor clusterization occurs for six sensors: Only four
    observation locations are visible, since two pairs of sensors are
    so close together they are indistinguishable.}
  \label{fig:clusterization illustration}
\end{figure}

There is a consensus among researchers that sensor clusterization is
undesirable \cite{fedorov1996, hooker2009, fedorov2012, Ucinski05,
  neitzel2019sparse} and several remedies to sensor clusterization
were suggeted through the years. In a \emph{clusterization-free
design}, sensor locations are forced to be far from each other
\cite{Ucinski05}. In a frequentist and finite-dimensional setting,
senesor clusterization is avoided by either imposing distance
constraints between measurements, or by introducing correlated errors
which account for both observation error and model error
\cite{Ucinski05}. Merging close measurements is also possible
\cite{fedorov2012}, but this practice ignores sensor clusterization
altogether. In time-series analysis for pharmacokinetic experiments,
sensor clusterization is mitigated by modeling auto-correlation time
in the noise \cite{hooker2009}, and this approach is similar in spirit
to assuming errors are correlatied as in \cite{Ucinski05}.

%% Sometimes, imposing a correlation structure on observation noise is
%% reasonable indeed \cite{koval2020, Alexanderian2021}. For example,
%% Koval \cite{koval2020} imposes a Gaussian prior on ocean bathytermy
%% when inferring BLA. However, in the (un)fortunate event that ocean
%% bathytermy is known, there is no use to modeling it as an unknown
%% and sensor clusterization will rear its ugly head.

Many scientists bypass the sensor clusterization problem by
restricting measurements to a coarse grid of the computational domain
$\Omega \subseteq \mathbb{R}^d, d=1,2,3$ \cite{koval2020,
  alexanderian2021, attia2020, alexanderian2014sparsification,
  AlexanderianPetraStadlerEtAl16, alexanderian2018efficient}, but this
practice requires solvilg a hard combinatorial optimization problem
for choosing optimal sensor locations from a finite set. A solution
typically involves relaxing the problem to that of choosing optimal
measurement ``weight'' in $[0,1]$ for each potential sensor
location. Some authors then add a sparsifying $\ell_1$ penalty term to
the design criterion, which is subsequently thresholded
\cite{horesh2008borehole}, thus resulting in a sparse binary
solution. Others successively relax the $\ell_1$ penalty to a $\ell_0$
penalty via a continuation method
\cite{AlexanderianPetraStadlerEtAl16,
  alexanderian2014sparsification}. A clever alternative is to cast the
problem of finding optimal sensor weights as a stochastic optimization
problem, which in turn is solved by stochastic gradient descent
\cite{attia2022stochastic}. While all of the above mentioned methods
are valid solutions to the prohibitive computational cost of finding
an binary optimal design restricted to a coarse grid, none treats the
elephant in the room: restricting sensors to a coarse grid in $\Omega
\subseteq \mathbb{R}^d, d=1,2,3$ necessarily gives rise to a
sub-optimal design.

Avoiding sensor clusterization is pragmatical: we feel sensor
clusterization is bad (but do not know why), and try to avoid
it. However, all of the abovementioned techniques that avoid sensor
clusterization do so by somehow restricting sensor locations, thus
fundamentaly altering the optimal design problem. To the best of my
knowledge, no study tried to answer the following seemingly trivial
and basic questions: why do sensor clusterization occurs? Is sensor
clusterization really bad? Can we find optimal designs BLABLA?

%% However, as far as I am aware, no study concerned the fact that
%% restricting sensors to a coarse grid is, by definition,
%% sub-optimal.


%% Attia \cite{attia2020} focused on QoI, Koval considered model
%% uncertainty and focused on reducing forward and adjoint solves, using
%%  randomized linear algebra \cite{koval2020}.

%% Today, including model errors are a bona-fide????  standard in the
%% inverse problems community: \cite{attia2020}, \cite{koval2020},
%% \cite{Alexanderian2021} all empower their models.  Both studies
%% present numerical techniques for finding optimal designs when model
%% error is present. Both are restricted to linear inverse problems
%% (although in the latter the authors use their method on a nonlinear
%% problem by taking a Laplace approximation for the posterior). Both
%% find an optimal design by first solving a continuous problem for
%% sensor weights. Said solution is then sparsified to give a binary
%% design. Both studies are successful in the task of Bayesian
%% inversion. Neither \cite{attia2020}, nor \cite{koval2020} mention any
%% effect model errors can have on sensor clusterization.

%% Considerable efforts focused on various computational aspects of
%% finding optimal designs in infinite-dimensional Bayesian inverse
%% problems \cite{attia2020, koval2020,
%% AlexanderianPetraStadlerEtAl16, attia2023pyoed, koval2021optimal,
%% attia2020stochastic}, the subject of sensor clusterization was less
%% investigated, and


%% Second, restricting sensors to a such a small subset of the computatoinal
%% domain $\Omega \subseteq \mathbb{R}^d, d=1,2,3$ necessarily gives rise
%% to sub-optimal designs.


\subsection{Contribution}
In this study, we seeks to understand sensor clusterization. We focus
our attention on the Bayesian \emph{D-optimality} criterion: a
D-optimal design maximizes the expected Kullback-Leibler divergence
\cite{CoverThomas91} between posterior and prior
\cite{Chaloner1995}. We study Bayesian D-optimal designs in linear
inverse problems that are (potentially) of infinte dimensions. We
propose and study a relaxed and analytically tractable model for such
D-optimal designs. Under this model, D-optimal designs are solutions
of a constrained optimization problem, formulated using Lagrange
multipliers. The Lagrange multipliers formulation allows us to
rigorously show why model error mitigates clusterization (Section
\ref{section:non vanishing}), like many authors note, and why designs
that exhibit clusterization are indeed optimal when no model error is
considered (Section \ref{section:vanishing}).

As a byproduct, the Lagrange multipliers problem for no model error
results in a characterization of D-optimal designs. The key idea is
that a D-optimal design reduces uncertainties where they are highest
first. This is the conclusion of part (5) of Theorem \ref{thm:char},
which we state below and prove in Section \ref{section:vanishing}.

\begin{restatable}[D-optimal designs with vanishing model error]{theorem}{char}\label{thm:char}
  %% \begin{theorem}[D-optimal designs with vanishing model error]\label{thm:char}
  %% Let Notation \ref{not:setup}. Further, let

  Let:
  \begin{itemize}
  \item $\hilp, \hilo$ Hilbert spaces.
  \item $\fwd:\hilp \to \hilo$ a linear compact operator.
  \item $\pr \sim \mathcal{N}(0, \prcov)$ prior Gaussian measure on $\hilp$,
    where $\prcov:\hilp \to \hilp$ is the prior covariance operator.
  \item $\obs: \hilo \to \mathbb{R}^m$ observation operator, where $m
    \in \mathbb{N}$ is the number of observations taken. 
  \item $\sigma^2 \in \mathbb{R}_{+}$ observation noise variance,
    $\data = \obs \fwd \param + \eps$, where $\eps \in \mathbb{R}^m$
    is iid $\mathcal{N}(0, \sigma^2)$ noise.
  \item $\post$ the posterior measure, with covariance $\postcov$.
  \item A D-optimality design criterion
    \cite{AlexanderianGloorGhattas14}:
    \begin{align*}
      \begin{split}
        \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det ( I + \sigma^{-2} \prcov^{1/2} \fwd ^*
        \obs^* \obs \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  \item $\{\lambda_i\}_{i=1}^\infty$ eigenvalues of $\fwd\prcov\fwd^*$
    in decreasing order of magnitude.
  %% \item $\{\ev_i\}_{i=1}^\infty$ their corresponding eigenvectors.
  \item $\{\eta_i\}_{i=1}^\infty$ eigenvalues of $\obs^*\obs$.
  \item $\obs$ a D-optimal design operator
    \begin{equation*}
      \obs = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs).
    \end{equation*}
  \end{itemize}

  Then:
  \begin{enumerate}
  \item  $\tr{\obs^*\obs} = m$.
  \item $\obs^*\obs$ and $\fwd\prcov\fwd^*$ are simultaneously
    diagonalizable.
  \item $k := \rank \obs^*\obs \leq m$ and
    \begin{equation*}
      \tar(\obs) = \frac12 \sum_{i=1}^{k} \log (1 + \sigma^{-2}\lambda_i\eta_i). %= \frac12 \sum_{i=1}^{m} \log (1 + \sigma^{-2}\lambda_i\eta_i).
    \end{equation*}
  %% \item 
  %%   \begin{equation*}
  %%     k = \argmax \left \{ k:\lambda_k^{-1} < \sigma^{-2}\frac{m}{k} + \frac{1}{k} \sum_{j=1}^{k}
  %%     \lambda_j^{-1} \right \}.
  %%   \end{equation*}
  \item
    \begin{equation*}
        \eta_i = \begin{cases}
          \frac{m}{k} - \sigma^2 \lambda_i^{-1} + \sigma^2 \frac{1}{k} \sum_{j=1}^k \lambda_j^{-1} & 1 \leq i \leq k \\
          0 & i > k 
        \end{cases}.
    \end{equation*}
  \item The covariance of the pushforwad $\fwd_{*} \post$ is $\left (
    (\fwd \prcov \fwd^*)^{-1} + \sigma^{-2} \obs^*\obs \right )^{-1}$
    and has eigenvalues
    \begin{equation*}
      \theta_i =
      \begin{cases}
        \left(\frac{\sum_{j=1}^k \lambda_j^{-1} + \sigma^{-2}m}{k} \right )^{-1} & i \leq k \\
        \lambda_i &  i > k 
      \end{cases}
    \end{equation*}
  \end{enumerate}
%\end{theorem}
\end{restatable}

Another byproduct of the proof of Theorem \ref{thm:char} is a novel
linear algebra lemma: We show how to decompose a symmetric positive
definite matrix $M$ as $M = AA^t$, where $A$ has unit norm columns
(Lemma \ref{lemma:lax}). We also generalize two lemmas from linear
algebra to infinite-dimensional settings: In Lemma \ref{lemma:MDL} we
prove a matrix determinant lemma in Hilbert spaces, and in Lemma
\ref{lemma:lax} we generalize a lemma \cite{Lax07} for calculating
$\frac{\der}{\der t} \log \det (I + X(t))$, where $X(t)$ is an
operator valued function.

\subsection{Limitations}\label{subsec:limitations}
The study presented here is not free from drawbacks. First, our
relaxed model does not correspond to any real-life problem. It is
generic enough to be analytically tractable, but one may argue it is
to far removed from any real application. To these claims I would
argue that Mathematicians have a long history of studying models that
are bare-bones simplifications of real systems, e.g. the Ising model
\cite{cipra1987} the Lorenz system \cite{brin}, and the SIR model
\cite{keeling2008}, to name a few. The second drawback of our model is
that we do not show rigorously that clusterization necessarily occurs,
we only show that it does not pose any obstruction to D-optimality.


\subsection{An Example of Clusterization}\label{subsec:example}
For the purpose of illustration, consider a toy inverse problem ---
inferring the initial condition of the 1D heat equation in $\Omega :=
[0,\pi]$ with a homogeneous Dirichlet boundary condition. Sensor
clusterization is clearly evident, see Figure \ref{fig:clusterization
  illustration}.

The 1D heat equation with homogeneous Dirichlet boundary condition is:
\begin{subequations}\label{eq:heat equation}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,\pi] \times [0,\infty),\\
      u &= 0 &&\qquad \text{on } \{0, \pi\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,\pi] \times \{0\}.
  \end{alignat}
\end{subequations}


The goal is to infer the initial condition $u_0$. Model the initial
condition as $u_0 \sim \normal(0,\prcov)$, for $\prcov =
(-\Delta)^{-1}$ with a homogeneous Dirichlet boundary condition. Fix
some final time $T$ and denote by $\fwd$ the forward operator, so that
$u(\cdot, T) = \fwd u_0$. Let $\obs$ the observation operator, so that
$u(\x_j,T) = (\obs u)_j, j=1,\dots,m$. Assume iid centered Gaussian
observation error, so observations are given by $u(\x_j,T) +
\eps(\x_j)$ with $\eps(\x_j) \sim \normal(0, \sigma^2)$ iid, $\sigma^2
> 0$. The posterior covariance is known and depends only on $\prcov,
\fwd, \obs$ and $\sigma^2$ \cite{Stuart10} (see Section
\ref{section:prelim} and \eqref{eq:postcov} below). Finally, choose
$\x_j,j=1,\dots,m$ to optimize the D-optimality criterion, see Theorem
\ref{thm:d optimality} \cite{AlexanderianGloorGhattas14}.

Sensor clusterization is illustrated in Figure \ref{fig:clusterization
  illustration}. The posterior covariance does not depend on data, so
the plot has no reference to actual data observed. The posterior
covariance does depend on location of the observation taken, which
were chosen to optimize the D-optimality criterion. Sensor
clusterization is clearly seen for six sensors; it looks like only
four observations were taken. Two pairs of sensors are clustered so
close together that they are indistinguishable from each other.


%% A D-optimal design
%% is found via a generalization of the D-optimality criterion to
%% infinite dimension(also, Section
%% \ref{subsec:D optimal design} below).
