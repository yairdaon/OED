\section{Introduction}\label{section:intro}
Measurements play a fundamental role in generating observations and
are indispensable to any inference process. In the realm of data
science, selecting the optimal set of measurements holds particular
significance when inferring parameters of a physical process. Unlike
many other fields, where measurements are fixed or limited, in this
context, we have the freedom to choose which measurements to take. We
would like to take advantage of this freedom to choose measurements
that enhance accuracy, reduce costs, or achieve both objectives
simultaneously. Whether measurements involves specifying electrode
locations on the skin in electric impedance tomography
\cite{horesh2010impedance}, determining certain wavelengths in MRI
\cite{horesh2008mri}, or positioning sensors for detecting
ground-reflected waves in the search for oil
\cite{horesh2008borehole}, the selection of optimal measurements,
referred to as the problem of \emph{optimal design}, becomes
crucial. To achieve this, researchers optimize specific \emph{design
criteria}, with A- and D-optimality being two of the most widely
recognized and extensively studied design criteria
\cite{Chaloner1995}.


Interestingly, A- and D-optimal designs have been observed to yield
remarkably similar measurements in certain cases \cite{fedorov1996,
  hooker2009, fedorov2012, Ucinski05, neitzel2019sparse}. To
illustrate this point, we consider a toy inverse problem: inferring
the initial condition of the 1D heat equation (details in Section
\ref{section:example}). In Figure \ref{fig:clusterization
  illustration} D-optimal measurement locations are shown for
different numbers of allowed measurements. Notably, for six
measurement locations, the D-optimal design yields two sets of
measurements that are almost indistinguishable from one another. This
intriguing phenomenon is known as \emph{measurement clusterization}
\cite{Ucinski05}, and we refer to a design that exhibits measurement
clusterization as a \emph{clustered design}.


\begin{figure}
  \begin{tikzpicture}[thick, scale=1.3, every node/.style={scale=0.99}]
    \begin{axis}
      %% title={Posterior Pointwise Standard Deviations and D-Optimal
      %% Measurement Locations}
      [xmin = 0, xmax = 3.14, xlabel = {$x \in \Omega$}, ylabel =
        posterior pointwise std, ymin = 0,
      %compat = 1.3,
      % ymax   = 130,
      % ytick = \empty,
      legend cell align=left,
      % legend style={at={(0.45,0.2)}}
      legend pos= outer north east 
      ]
      % \draw[black!30!white, thin] (50,0) -- (50,130);
      % 
      %% \addplot [thin, black, mark=none] table{stdv-heat-sens1-var1.txt};
      %% \addlegendentry{1 measurements};
      
      %% \addplot [thin, blue, mark=none] table{stdv-heat-sens2-var1.txt};
      %% \addlegendentry{2 measurements};
      
      %% \addplot [thin, red, mark=none] table{stdv-heat-sens3-var1.txt};
      %% \addlegendentry{3 measurements};
      
      \addplot [thin, green, mark=none] table{stdv-heat-sens4-var1.txt};
      \addlegendentry{4 measurements};
      
      \addplot [thin, purple, mark=none] table{stdv-heat-sens5-var1.txt};
      \addlegendentry{5 measurements};
      
      \addplot [thin, cyan, mark=none] table{stdv-heat-sens6-var1.txt};
      \addlegendentry{6 measurements};

  
      %% \addplot [black,  only marks, mark=x, mark size=1.5] 
      %% table{locs-heat-sens1-var1.txt}; 
      %% \addplot [blue,   only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens2-var1.txt}; 
      %% \addplot [red,    only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens3-var1.txt};
      \addplot [green,  only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens4-var1.txt}; 
      \addplot [purple, only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens5-var1.txt}; 
      \addplot [cyan,   only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens6-var1.txt}; 
  
      
    \end{axis}
  \end{tikzpicture}
  \caption{Measurement clusterization when inferring the initial
    condition of the 1D heat equation. Posterior pointwise standard
    deviations (lines) are plotted over the computational domain
    $\Omega = [0, \pi]$, for varying numbers of
    measurements. Measurement locations (circles) were chosen to
    according to the D-optimality criterion. Measurement
    clusterization occurs for six measurements: Only four measurement
    locations are visible, since two pairs of measurements are so
    close together they are indistinguishable.}
  \label{fig:clusterization illustration}
\end{figure}


Researchers widely agree that measurement clusterization is undesirable
\cite{fedorov1996, hooker2009, fedorov2012, Ucinski05,
  neitzel2019sparse}, prompting the exploration of various remedies to
address this issue. One approach involves merging ``close''
measurements \cite{fedorov2012}; however, this strategy merely
overlooks the phenomenon of measurement clusterization. An alternative
solution lies in \emph{clusterization-free design}s, where measurement
locations are deliberately chosen to be distant from one another. This
can be achieved by imposing distance constraints between measurements
or by introducing correlated errors that account for both observation
error and model misspecification \cite{Ucinski05}. For instance, in
the context of time-series analysis for pharmacokinetic experiments,
measurement clusterization can be mitigated by incorporating the modeling
of auto-correlation time within the noise terms
\cite{hooker2009}. %% This methodology shares a similar rationale with
%% the earlier approach, which assumed correlation between errors
%% \cite{Ucinski05}.


In spatial problems involving choice of measuremnts within a domain
$\Omega \subseteq \mathbb{R}^d, d=1,2,3$, many researchers circumvent
the problem of measurement clusterization by choosing measurements from a
coarse grid in $\Omega$ \cite{koval2020, alexanderian2021, attia2020,
  alexanderian2014, alexanderian2016,
  alexanderian2018efficient}. However, this approach incurs a
significant computational cost as it requires solving a difficult
combinatorial optimization problem to select optimal measurement
locations from a finite set. Typically, the optimization problem is
relaxed by first assigning optimal measurement "weights" in
$\mathbb{R}_+$ to potential measurement locations. Some authors
incorporate a sparsifying $\ell_1$ penalty term into the design
criterion, which is subsequently thresholded to achieve the desired
binary design \cite{horesh2008borehole}. Others progressively relax
the $\ell_1$ penalty to an $\ell_0$ penalty via a continuation method
\cite{alexanderian2016, alexanderian2014}. A clever alternative is to
cast the problem of finding optimal measurement weights as a stochastic
optimization problem \cite{attia2022stochastic}. All of the
aforementioned methods may indeed find a binary optimal design
restricted to a given coarse grid. However, none addresses one
fundamental issue: the restriction of measurement locations to a coarse
grid in $\Omega \subseteq \mathbb{R}^d, d=1,2,3$ inherently results in
a sub-optimal design.

Avoiding measurement clusterization is a pragmatic approach:
intuitively, we recognize that measurement clusterization is
undesirable, even though the underlying reasons may not be fully
clear. Consequently, we strive to prevent it and devise various
methodologies to avoid measurement clusterization. Yet each and every
one of these methodologies achieves this objective by imposing
restrictions on measurement locations, thereby fundamentally altering
the optimal design problem. Surprisingly, to the best of my knowledge,
no previous study has tried to address some seemingly simple yet
fundamental questions:
%
Why does measurement clusterization occur?
%
Why does imposing correlations between observations alleviate
measurement clusterization?
%
Should we aim to avoid measurement clusterization?
%
Can an optimal clustered design be relpaced with an equally optimal
non-clustered design?design?


\subsection{Contribution}
The primary objective of this study is to provide a comprehensive
understanding of measurement clusterization by addressing the
aforementioned questions. Our focus centers around investigating the
Bayesian D-optimality criterion, which involves maximizing the
expected Kullback-Leibler divergence between the posterior and prior
measures \cite{CoverThomas91, Chaloner1995}. To delve deeper into this
subject, we conduct an analysis of Bayesian D-optimal designs within
the context of linear inverse problems in Hilbert spaces. We propose a
novel relaxed model for D-optimality that maintains analytical
tractability and enables the identification of D-optimal designs using
Lagrange multipliers. This analytical framework facilitates the
exploration of the following questions posed at the end of the
previous paragraph:


\begin{enumerate}
\item \label{q:why} \textbf{Why does measurement clusterization
  occur?} In Section \ref{section:vanishing}, we provide an insightful
  explanation for the optimality of clustered designs when no model
  error is present. We demonstrate that measurement clusterization
  poses no obstruction to D-optimality and that clustered design are
  preferred by numerical algorithms.

  %% We conjecture that the prevalence of measurement clusterization
  %% arises due to the ease of discovering clustered designs.

\item \label{q:mitigate} \textbf{Why does imposing correlations
  between observations alleviate measurement clusterization?} In
  Section \ref{section:non vanishing}, we rigorously demonstrate the
  role of model error in mitigating clusterization, thereby
  corroborating earlier observations made by various researchers.

\item \label{q:avoid} \textbf{Should we aim to avoid measurement
  clusterization?} Based on the analysis conducted in this study, we
  did not find any compelling reason to explicitly avoid optimal
  clustered designs.

\item \label{q:replace} \textbf{Can an optimal clustered design be
  relpaced with an equally optimal non-clustered design?} In Section
  \ref{section:vanishing}, we answer this question in the affirmative,
  although we show that numerical experiments conducted using our
  model indicate a strong preference for clustered designs.
\end{enumerate}

The cornerstone of our investigation into measurement clusterization is
Theorem \ref{thm:char}, presented below and proven in Section
\ref{section:vanishing}. The key insight of Theorem \ref{thm:char}
lies in its fifth component, which highlights that a D-optimal design
aims to uniformly reduce posterior uncertainties across all posterior
covariance eigenvectors.

\begin{restatable}[D-optimal designs with vanishing model error]{theorem}{char}\label{thm:char}
  Let:
  \begin{itemize}
  \item $\hilp, \hilo$ Hilbert spaces.
  \item $\fwd:\hilp \to \hilo$ a linear compact operator.
  \item $\pr \sim \mathcal{N}(0, \prcov)$ prior Gaussian measure on $\hilp$,
    where $\prcov:\hilp \to \hilp$ is the prior covariance operator.
  \item $\obs: \hilo \to \mathbb{R}^m$ measurement operator, where $m
    \in \mathbb{N}$ is the number of measurements taken.
  \item $\sigma^2 \in \mathbb{R}_{+}$ observation noise variance,
    $\data = \obs \fwd \param + \eps$, where $\eps \in \mathbb{R}^m$
    is iid $\mathcal{N}(0, \sigma^2)$ noise.
  \item $\post$ the posterior measure, with covariance $\postcov$.
  \item A D-optimality design criterion
    \cite{AlexanderianGloorGhattas14}:
    \begin{align*}
      \begin{split}
        \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det ( I + \sigma^{-2} \prcov^{1/2} \fwd ^*
        \obs^* \obs \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  \item $\{\lambda_i\}_{i=1}^\infty$ eigenvalues of $\fwd\prcov\fwd^*$
    in decreasing order of magnitude.
  %% \item $\{\ev_i\}_{i=1}^\infty$ their corresponding eigenvectors.
  \item $\{\eta_i\}_{i=1}^\infty$ eigenvalues of $\obs^*\obs$.
  \item $\obs$ a D-optimal design operator
    \begin{equation*}
      \obs = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs).
    \end{equation*}
  \end{itemize}

  Then:
  \begin{enumerate}
  \item  $\tr{\obs^*\obs} = m$.
  \item $\obs^*\obs$ and $\fwd\prcov\fwd^*$ are simultaneously
    diagonalizable.
  \item $k := \rank \obs^*\obs \leq m$ and
    \begin{equation*}
      \tar(\obs) = \frac12 \sum_{i=1}^{k} \log (1 + \sigma^{-2}\lambda_i\eta_i). %= \frac12 \sum_{i=1}^{m} \log (1 + \sigma^{-2}\lambda_i\eta_i).
    \end{equation*}
  %% \item 
  %%   \begin{equation*}
  %%     k = \argmax \left \{ k:\lambda_k^{-1} < \sigma^{-2}\frac{m}{k} + \frac{1}{k} \sum_{j=1}^{k}
  %%     \lambda_j^{-1} \right \}.
  %%   \end{equation*}
  \item
    \begin{equation*}
        \eta_i = \begin{cases}
          \frac{m}{k} - \sigma^2 \lambda_i^{-1} + \sigma^2 \frac{1}{k} \sum_{j=1}^k \lambda_j^{-1} & 1 \leq i \leq k \\
          0 & i > k 
        \end{cases}.
    \end{equation*}
  \item The covariance of the pushforwad $\fwd_{*} \post$ is $\left (
    (\fwd \prcov \fwd^*)^{-1} + \sigma^{-2} \obs^*\obs \right )^{-1}$
    and has eigenvalues
    \begin{equation*}
      \theta_i =
      \begin{cases}
        \left(\frac{\sum_{j=1}^k \lambda_j^{-1} + \sigma^{-2}m}{k} \right )^{-1} & i \leq k \\
        \lambda_i &  i > k 
      \end{cases}
    \end{equation*}
  \end{enumerate}
%\end{theorem}
\end{restatable}

Other contributions include three useful lemmas: In Lemma
\ref{lemma:free} we decompose a symmetric positive definite matrix $M
\in \mathbb{R}^{k \times k}$ with $\ttr M = m \in \mathbb{N}$ as $M =
AA^t$, where $A$ has unit norm columns. In Lemma \ref{lemma:MDL} we
prove a matrix determinant lemma in Hilbert spaces. Finally in Lemma
\ref{lemma:lax} we generalize a lemma for calculating
$\frac{\der}{\der t} \log \det (I + X(t))$, where $X(t)$ is an
operator valued function \cite{Lax07}.


\subsection{Limitations}\label{subsec:limitations}
The main limitation of this study is that our generic model does not
correspond to any specific real-life problem. It is generic enough to
be analytically tractable, but one may argue it is too far removed
from any real application. To these claims I would argue that
scientists have a long history of studying models that are bare-bones
simplifications of real systems, e.g. the Ising model
\cite{cipra1987}, the Lorenz system \cite{brin}, the Lotka-Volterra
equations \cite{logan2006}, the Carnot engine \cite{kardar2007}, and
many others.


%% \subsection{An Example of Measurement Clusterization}\label{subsec:example}
