\section{Introduction}\label{section:OED intro}
Experimental design is an important part of many scientific
investigations. When considering an inverse problem, one can often
specify sensor locations (e.g.\ in geophysics and oceanography
applications), certain wavelengths (e.g.\ in MRI) or wave reflections
from the ground (e.g.\ searching for oil or using a radar). Whatever
the allowed set of observations is, one should select the optimal
observations to take, in order to increase accuracy, reduce costs, or
both.

The problem of choosing the best set of measurements is known as the
problem of \emph{optimal experimental design}. Optimal designs are
found by optimizing some \emph{design criterion}. Many design criteria
exist, and in this study we focus on the Bayesian \emph{D-optimality
criterion}. The D-optimality criterion has a simple and appealing
motivation \cite{Chaloner1995}: a D-optimal design maximizes the
expected information gain (KL divergence \cite{CoverThomas91}) between
posterior and prior. For a linear model in finite dimensions, with
Gaussian prior and iid Gaussian noise, a D-optimal design minimizes
the determinant of the posterior covariance matrix\footnote{In a
frequentist setting, a D-optimal design minimizes the determinant of
the Fisher information matrix \cite[page 16]{Ucinski05}, so the
Bayesian criterion is just a regularized version of the frequentist
criterion \cite{Chaloner1995}.}.

Bayesian D-optimal designs for inverse problems on function spaces
were first studied by \cite{AlexanderianGloorGhattas14,
  AlexanderianPetraStadlerEtAl16,
  AlexanderianPetraStadlerEtAl14}. Similarly to the finite dimensional
case, a D-optimal design maximizes the KL divergence between posterior
and prior measures. For a linear inverse problem with Gaussian prior
and iid Gaussian noise, a D-optimal design minimizes an expression
analogous to the determinant of the posterior covariance operator,
understood as a product of its eigenvalues.
%% Some difficulties arise in the process, but remedies can be found
%% as shown in \cite{AlexanderianGloorGhattas14}.

When one computes an optimal design using the D-optimality criterion,
the optimization process sometimes results in observations that are
very similar. For example, if an observation is thought of as
measuring some function value at $\x \in \Omega \subseteq \R^d,
d=1,2,3$ (with added error) then the optimization procedure sometimes
places sensors in very close proximity to each other (as can be seen
in Figure \ref{fig:clusterization illustration}). Following
\cite{Ucinski05}, we refer to this phenomenon as \emph{sensor
clusterization}.

Sensor clusterization is intuitively undesirable. Selecting a small
set of sensors may lead to incomplete or biased data, as certain parts
of the system or environment may be overlooked or
underrepresented. This can result in inaccurate or unreliable
estimates of the unknown parameters of interest, potentially leading
to incorrect conclusions or decisions.

%% \subsection{Related Work}
Several remedies are suggested in the literature to phenomenon of
sensor clusterization. In a frequentist and finite-dimensional
context, \cite{fedorov1996} and \cite[chapter 2.4.3]{Ucinski05}
discuss the clusterization phenomenon and suggest an approach called
clusterization-free design. In such designs, the user enforces
observation locations to be far from each other. One way to do this is
by introducing correlated errors which account for both observation
error and model error. Another method considered is imposing distance
constraints between observations. A somewhat different approach is
suggested in \cite[page 49]{fedorov2012}, where close observations are
merged --- a procedure which obviously does not avoid
clusterization. The same problem arises in time-series analysis for
pharmacokinetic experiments. The authors of \cite{hooker2009} suggest
modeling auto-correlation time in the noise model, which is equivalent
to the correlated errors mentioned above. Any of the above mentioned
approaches might serve as a remedy and push sensors away from each
other. Yet, none offers any insight as to why clusterization
occurs. Also, as better models are employed, model error is decreased
and the clusterization phenomenon will eventually reappear. While
these approaches are practical and help us avoid sensor
clusterization, they do not provide insight as to why sensors are
clustering.

Model errors were also considered for inverse problems in
\cite{attia2020, koval2020}. The former study is focused on inferring
a Quantity of Interest (QoI). The focus of the latter is reducing
forward solves, using randomized linear algebra. Both studies present
numerical techniques for finding optimal designs when model error is
present. Both are restricted to linear inverse problems (although in
the latter the authors use their method on a nonlinear problem by
taking a Laplace approximation for the posterior). Both find an
optimal design by first solving a continuous problem for sensor
weights. Said solution is then sparsified to give a binary
design. Both studies are successful in the task of Bayesian
inversion. Neither \cite{attia2020}, nor \cite{koval2020} mention any
effect model errors can have on sensor clusterization.


\subsection{Contribution}
The current study aims to fill the gap of understanding sensor
clusterization in infinte dimensional Bayesian linear inverse
problems. We propose and study a relaxed and analytically tractable
model for D-optimal designs. Under this model, D-optimal designs are
solutions of a constrained optimization problem, formulated using
Lagrange multipliers. The Lagrange multipliers formulation allows us
to rigorously show how model error mitigates clusterization (Section
\ref{section:non vanishing}), and why designs that exhibit
clusterization are indeed optimal when no model error is considered.

As a byproduct, the Lagrange multipliers problem for no model error
results in a characterization of D-optimal designs. The key idea is
that a D-optimal design reduces uncertainties where they are highest
first. This is the conclusion of part (5) of Theorem \ref{thm:char},
which we state below and prove in Section \ref{section:vanishing}:


\begin{restatable}[D-optimal designs with vanishing model error]{theorem}{char}\label{thm:char}
  %% \begin{theorem}[D-optimal designs with vanishing model error]\label{thm:char}
  %% Let Notation \ref{not:setup}. Further, let

  Let:
  \begin{itemize}
  \item $\hilp, \hilo$ Hilbert spaces.
  \item $\fwd:\hilp \to \hilo$ a linear compact operator.
  \item $\pr \sim \mathcal{N}(0, \prcov)$ prior Gaussian measure on $\hilp$,
    where $\prcov:\hilp \to \hilp$ is the prior covariance operator.
  \item $\obs: \hilo \to \mathbb{R}^m$ observation operator, where $m
    \in \mathbb{N}$ is the number of observations taken. 
  \item $\sigma^2 \in \mathbb{R}_{+}$ observation noise variance,
    $\data = \obs \fwd \param + \eps$, where $\eps \in \mathbb{R}^m$
    is iid $\mathcal{N}(0, \sigma^2)$ noise.
  \item $\post$ the posterior measure, with covariance $\postcov$.
  \item A D-optimality utility function
    \cite{AlexanderianGloorGhattas14}:
    \begin{align*}
      \begin{split}
        \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det ( I + \sigma^{-2} \prcov^{1/2} \fwd ^*
        \obs^* \obs \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  \item $\{\lambda_i\}_{i=1}^\infty$ eigenvalues of $\fwd\prcov\fwd^*$
    in decreasing order of magnitude.
  %% \item $\{\ev_i\}_{i=1}^\infty$ their corresponding eigenvectors.
  \item $\{\eta_i\}_{i=1}^\infty$ eigenvalues of $\obs^*\obs$.
  \item $\obs$ a D-optimal design operator
    \begin{equation*}
      \obs = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs).
    \end{equation*}
  \end{itemize}

  Then:
  \begin{enumerate}
  \item  $\tr{\obs^*\obs} = m$.
  \item $\obs^*\obs$ and $\fwd\prcov\fwd^*$ are simultaneously
    diagonalizable.
  \item $k := \rank \obs^*\obs \leq m$ and
    \begin{equation*}
      \tar(\obs) = \frac12 \sum_{i=1}^{k} \log (1 + \sigma^{-2}\lambda_i\eta_i). %= \frac12 \sum_{i=1}^{m} \log (1 + \sigma^{-2}\lambda_i\eta_i).
    \end{equation*}
  %% \item 
  %%   \begin{equation*}
  %%     k = \argmax \left \{ k:\lambda_k^{-1} < \sigma^{-2}\frac{m}{k} + \frac{1}{k} \sum_{j=1}^{k}
  %%     \lambda_j^{-1} \right \}.
  %%   \end{equation*}
  \item
    \begin{equation*}
        \eta_i = \begin{cases}
          \frac{m}{k} - \sigma^2 \lambda_i^{-1} + \sigma^2 \frac{1}{k} \sum_{j=1}^k \lambda_j^{-1} & 1 \leq i \leq k \\
          0 & i > k 
        \end{cases}.
    \end{equation*}
  \item The covariance of the pushforwad $\fwd_{*} \post$ is $\left (
    (\fwd \prcov \fwd^*)^{-1} + \sigma^{-2} \obs^*\obs \right )^{-1}$
    and has eigenvalues
    \begin{equation*}
      \theta_i =
      \begin{cases}
        \left(\frac{\sum_{j=1}^k \lambda_j^{-1} + \sigma^{-2}m}{k} \right )^{-1} & i \leq k \\
        \lambda_i &  i > k 
      \end{cases}
    \end{equation*}
  \end{enumerate}
%\end{theorem}
\end{restatable}

In the process of proving Theorem \ref{thm:char}, we prove a novel
lemma in linear algebra: We show how to decompose a symmetric positive
definite matrix $M$ as $M = AA^t$, where $A$ has unit norm columns
(Lemma \ref{lemma:lax}). We also generalize two lemmas from linear
algebra to infinite-dimensional settings: In Lemma \ref{lemma:MDL} we
prove a matrix determinant lemma in Hilbert spaces, and in Lemma
\ref{lemma:lax} we generalize a lemma \cite{Lax07} for calculating
$\frac{\der}{\der t} \log \det (I + X(t))$, where $X(t)$ is an
operator valued function.

\subsection{Limitations}\label{subsec:limitations}
The study presented here is not free from drawbacks. First, our
relaxed model does not correspond to any real-life problem. It is
generic enough to be analytically tractable, but one may argue it is
to far removed from any real application. To these claims I would
argue that Mathematics has a long history of studying models that are
bare-bones simplifications of real systems, e.g. the Ising model
\cite{cipra1987} the Lorenz system \cite{brin}, the SIR model
\cite{keeling2008}, to name a few. The second drawback of our model is
that we do not show rigorously that clusterization necessarily occurs,
we only show that it does not pose any obstruction to D-optimality.


\subsection{An Example of Clusterization}\label{subsec:example}
\begin{figure}
  \begin{tikzpicture}[thick, scale=1.3, every node/.style={scale=0.99}]
    \begin{axis}
      [
      title={Posterior Pointwise Standard Deviations and D-Optimal Sensor Locations},  
      xmin = 0,
      xmax = 3.14,
      xlabel = {$x$},
      ylabel = posterior std,
      ymin   = 0,
      %compat = 1.3,
      % ymax   = 130,
      % ytick = \empty,
      legend cell align=left,
      % legend style={at={(0.45,0.2)}}
      legend pos= outer north east 
      ]
      % \draw[black!30!white, thin] (50,0) -- (50,130);
      % 
      %% \addplot [thin, black, mark=none] table{stdv-heat-sens1-var1.txt};
      %% \addlegendentry{1 sensors};
      
      %% \addplot [thin, blue, mark=none] table{stdv-heat-sens2-var1.txt};
      %% \addlegendentry{2 sensors};
      
      %% \addplot [thin, red, mark=none] table{stdv-heat-sens3-var1.txt};
      %% \addlegendentry{3 sensors};
      
      \addplot [thin, green, mark=none] table{stdv-heat-sens4-var1.txt};
      \addlegendentry{4 sensors};
      
      \addplot [thin, purple, mark=none] table{stdv-heat-sens5-var1.txt};
      \addlegendentry{5 sensors};
      
      \addplot [thin, cyan, mark=none] table{stdv-heat-sens6-var1.txt};
      \addlegendentry{6 sensors};

  
      %% \addplot [black,  only marks, mark=x, mark size=1.5] 
      %% table{locs-heat-sens1-var1.txt}; 
      %% \addplot [blue,   only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens2-var1.txt}; 
      %% \addplot [red,    only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens3-var1.txt};
      \addplot [green,  only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens4-var1.txt}; 
      \addplot [purple, only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens5-var1.txt}; 
      \addplot [cyan,   only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens6-var1.txt}; 
  
      
    \end{axis}
  \end{tikzpicture}
  \caption{The clusterization effect for the 1D heat equation
    described in Section \ref{subsec:example}. Posterior pointwise
    standard deviations (lines) are plotted over the domain $[0,
      \pi]$, for varying numbers of sensors. Sensor locations
    (circles) were chosen to minimize (an expression analogous to) the
    determinant of the posterior covariance. The clusterization effect
    can be clearly seen for six sensors. Only four observation
    locations are used --- two pairs of sensors are so close they are
    indistinguishable.}
  \label{fig:clusterization illustration}
\end{figure}

In Section \ref{section:prelim}, we present a more abstract and general
formulation of the inverse problem we consider. But, for the purpose of
illustration, we present clusterization via a toy model --- the 1D heat
equation in $[0,\pi]$ with a homogeneous Dirichlet boundary condition.

The 1D heat equation is:
\begin{subequations}\label{eq:heat equation}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,\pi] \times [0,\infty),\\
      u &= 0 &&\qquad \text{on } \{0, \pi\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,\pi] \times \{0\}.
  \end{alignat}
\end{subequations}

The goal is to infer the initial condition $u_0$. For that purpose, we
measure $u$ at some set of locations $\x_j \in [0,\pi], j=1, \dots,m$
and a final time $T > 0$. We assume centered Gaussian observation
error, so we observe $v(\x_j,T) = u(\x_j,T) + \eps(\x_j)$ with
$\eps(\x_j) \sim \normal(0, \sigma^2)$ iid, $\sigma^2 > 0$. We model
the initial condition as $u_0 \sim \normal(0,\prcov)$, for $\prcov =
(-\Delta)^{-1}$ with a homogeneous Dirichlet boundary condition. It is
known \cite{Tarantola05} that for linear problems, with Gaussian prior
and error, the posterior is also Gaussian with a covariance that does
not depend on the observed data. The posterior covariance $\postcov$
has a closed form formula, even in infinite dimensions
\cite{Stuart10}. We denote by $\fwd$ the dynamics operator, so that
$u( \cdot,T) = \fwd u_0$, and the observation operator $\obs$ so that
$u(\x_j,T) = (\obs u)_j, j=1,\dots,m$. The posterior covariance is
known and depends only on $\prcov, \fwd, \obs$ and $\sigma^2$ (see
Section \ref{section:prelim} and \eqref{eq:postcov} specifically).

We consider generalization of the information-theoretic design
criterion presented in the introduction to infinite dimensions
(Section \ref{subsec:D optimal design} below). We choose
$\x_j,j=1,\dots,m$ to minimize (an expression analogous to) the
determinant of the posterior covariance operator. We will see later
how this corresponds to maximizing expected information gain.

The clusterization effect is illustrated in Figure
\ref{fig:clusterization illustration}. Posterior pointwise standard
deviations are plotted over the domain $[0, \pi]$. Since the posterior
covariance does not depend on data, the plot has no reference to
actual data observed. The posterior covariance does, however, depend
on location of the observation taken. In Figure
\ref{fig:clusterization illustration}, observation locations are
marked by circles. These were chosen to minimize (an expression
analogous to) the determinant of the posterior covariance. The
clusterization effect can be clearly seen for six sensors. It looks
like only four observations were taken. The reason is that two pairs
of sensors are so close they are indistinguishable.


