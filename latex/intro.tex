\section{Introduction}\label{section:intro}
The practice of experimental design entails specifying every detail of
running an experiment \cite{chaloner1995}. At its simplest form,
experimental design involves choosing a set of measurements to be
taken. For example, consider the process of searching for oil:
measurements require digging holes in the ground and a \emph{design}
simply indicates which holes should be dug
\cite{horesh2008borehole}. Since measurements are expensive, only a
limited number of them can be taken, and thus carefully choosing
measurement locations is crucial.

Specifying the right set of measurements holds particular significance
when solving an \emph{inverse problem} --- i.e.~making inference of
the physical world utilizing a physical model of a phenomenon of
interest \cite{tarantola2005,kaipio2005}. In an inverse problem, we
seek to infer physical quantities, based on observations and a model
of the physical world. Models are typically phrased in the language of
ordinary or partial differential equations.

Inverse problems are ubiquitous in science and engineering. For
example, in electrical impedance tomography (EIT) we seek to infer the
structure of the inside of a human body. Inference is conducted based
on observations of electrical current impedance measured at specific
electrode locations on the skin, leveraging known equations of
electric current flow \cite{horesh2010impedance}. Magnetic Resonance
Imaging (MRI) also entails solving an inverse problem. There,
radio-frequency pulses are sent through the human body in the presence
of a strong magnetic field. The response of one's body contents to
these radio-frequency pulses is measured, allowing a radiologist to
view the internals of a body in a noninvasive manner
\cite{horesh2008mri}. An inverse problem also arises in oil and gas
exploration, where acoustic waves are sent through "boreholes" ---
deep cylindrical wells drilled into the ground. Data of travel and
return times of the acoustic waves are recorded. Wave travel and
return time is influenced by the properties of the subsurface
materials, such as density, elasticity, and the presence of fluids or
voids. Combining travel time data with a geophysical models of
contents of the earth's crust facilitates reconstructing the structure
of the subsurface in a process called \emph{borehole tomography}
\cite{horesh2008borehole}. Inverse problems also arise in many other
areas of seismology and geology \cite{rabinowitz1990, steinberg1995}
and medical imaging \cite{tarantola2005}. In many inverse problems the
goal is to infer some numerical parameter. However, in the formulation
we consider in this study, as well as in the examples above, the goal
is to conduct inference over some \emph{function} over $\Omega$, where
\(\Omega \subseteq \mathbb{R}^d, d=1,2,3\) is a spatial domain of
interest.

\subsection{D-optimal Designs}\label{subsec:D}
In all of the examples of inverse problems we have seen, as well as in
many other applications, only a limited number of measurements are
allowed. For example, in borehole tomography, a measurement involves
drilling a deep well in the ground --- a very costly endeavor. Even in
a seemingly simpler application such as MRI, a patient cannot occupy
the MRI machine for too long, hence only a limited number of
measurements can be taken. Therefore, measurements should be chosen to
most benefit the user. Typically, a user will consider some utility,
called a \emph{design criterion} and take measurements that maximize
this utility. Two of the most widely recognized and extensively
studied design criteria are the A- and \emph{D-optimality} criteria.

In this study, we focus on the Bayesian D-optimality
criterion. Bayesian D-optimality carries a simple and intuitive
meaning: measurements chosen according to the D-optimality criterion
maximize the expected Kullback-Leibler (KL) divergence from posterior
to prior \cite{chaloner1995, AlexanderianGloorGhattas14,
  CoverThomas91}. Utilizing KL divergence for quantifying differences
between distributions is standard practice and we will not motivate it
further here. For a discrete parameter $\param$ and data $\data$, the
KL divergence is defined as
\begin{equation}\label{eq:basic_KL}
  D_{KL}\left (\Pr(\param|\data)||\Pr(\param)\right ) =  \sum_{\param} \log
  \frac{\Pr(\param|\data)}{\Pr(\param)} \Pr(\param|\data). 
\end{equation}
Of course, data $\data$ is not known before the experiment is
conducted, so we average over it to define the D-optimality criterion
as:
\begin{equation*}
  \mathbb{E}_{\data}\left [ D_{KL}\left (\Pr(\param|\data)||\Pr(\param)\right ) \right ].
\end{equation*}
We refer to a set of measurements that maximizes the D-optimality
criterion as a \emph{D-optimal design}.

For a linear model with Gaussian prior in finite dimensions, a
D-optimal design minimizes the determinant of the posterior covariance
\cite{chaloner1995}. In Section \ref{subsec:D_optimal_design} we give
a more general definition of the D-optimality criterion that applies
to arbitrary measures. We also show how the D-optimality criterion
generalizes to linear models over infinite-dimensional Hilbert spaces
(i.e.~function spaces).

\subsection{A toy model: the 1D heat equation}\label{subsec:toy}
For concreteness, let us now consider a toy inverse problem: inferring
the initial condition for partial differential equation known as the
\emph{heat equation} in one dimension (1D heat equation
henceforth). Readers less familiar with partial differential equations
can think of the following setup: before us there is a perfectly
insulated metal rod. At each point on the rod the temperature is
different and unknown, and we call this temperature distribution the
\emph{initial condition}. We wait for a short time $T$ and let the
heat dissipate a bit inside the rod. The heat equation determines the
heat distribution inside the rod at time $t=T$.

The full time evolution of heat in the rod $\Omega=[0,1]$ is formally
described by the following three equations:
\begin{subequations}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,1] \times [0,\infty), \label{eq:heat1}\\
    u &= 0 &&\qquad \text{on } \{0, 1\} \times [0,\infty), \label{eq:heat2}\\
    u &= u_0 &&\qquad \text{on }[0,1] \times \{0\}. \label{eq:heat3}
  \end{alignat}
\end{subequations}
As time passes, heat dissipates across the rod as hotter regions,
e.g.~local maxima of the heat distribution, become cooler. This
behavior is captured by eq.~\eqref{eq:heat1}: the Laplacian at local
maxima is negative, so $u_t = \Delta u$ implies $u$ decreases at its
maxima, and the reverse happens at local minima. Eq.~\eqref{eq:heat2}
describes the \emph{boundary condition}, which dictates how heat
interacts with the outside world at the two edges of the rod
$\{0,1\}$. Specifically, eq.~\eqref{eq:heat2} implements an absorbing
boundary, i.e.~heat that interacts with the boundary immediately
disappears. This type of boundary condition is known as a
\emph{homogeneous Dirichlet boundary condition}. Eq.~\eqref{eq:heat3}
describes the \emph{initial condition}, i.e.~the heat distribution
inside the rod at time $t=0$.

Solving the 1D heat equation is straightforward. Any initial heat
distribution $u_0\in L^2([0,1])$ that satisfies the homogeneous
Dirichlet boundary condition eq.~\eqref{eq:heat2} is a linear
combination of sines $\ev_n(x) = \sin(\pi n x)$, so $u_0 = \sum_{n\geq
  1} a_n \ev_n$ for some $\{a_n\}_{n\geq 1}$. Now, note that $\ev_n$
are in fact eigenvectors of the Laplacian: $\Delta \ev_n = -\pi^2
n^2\ev_n$. It should not be hard to believe that $u(\cdot, T) =
\sum_{n\geq 1} a_n \exp(-\pi^2 n^2T ) \ev_n$
\cite{renardy2006PDE}. Thus, eigenvectors of the linear operator that
describes the time evolution of heat are $\ev_n(x) = \sin(\pi n x)$
with corresponding eigenvalues
\begin{equation}\label{eq:decay}
  \exp(-\pi^2 n^2T ).
\end{equation}

In the \emph{inverse problem of the 1D heat equation}, our goal is to
infer the initial condition $u_0 = u(\cdot, 0)$ from observations of
the final state $u(\cdot, T)$. However, we are not able to measure
$u(\cdot, T)$ at every point in the domain $\Omega$. Rather, we take
some number of temperature measurements on the rod once our waiting
time $T$ has passed and try to infer $u(\cdot, 0)$ from these
measurements.

The inverse problem of the 1D heat equation is difficult ("ill-posed")
since heat spreads in a diffusive manner and any roughness in the
initial condition is quickly smoothed, as implied by the squared
exponential decay of eigenvalues in eq.~\eqref{eq:decay} with
$n\to\infty$. See Supplementary movies S1 and S2 for an illustration
of this phenomenon (code generating these movies is located in module
\texttt{movies.py} in the accompanying
\href{https://github.com/yairdaon/OED}{repository}). For ill-posed
problems regularization is required, and we implement such
regularization via a fully Bayesian formulation of the inverse
problem.

We choose a prior for the initial condition $u_0 \sim \mathcal{N}(0,
(-\Delta)^{-1})$, with homogeneous Dirichlet boundary condition. This
choice of prior covariance operator ensures the posterior is
well-defined and prior realizations are well-behaved, see
\cite[Theorem 3.1 and Lemma 6.25]{Stuart10} for details. The second
ingredient of a fully Bayesian formulation of the inverse problem is
likelihood. We simply assume centered iid Gaussian measurement error
model, giving rise to a the widely applicable Gaussian likelihood
function.


\subsection{Measurement Clusterization}
Surprisingly, A- and D-optimal designs for inverse problems have been
observed to yield remarkably similar measurements in certain cases
\cite{fedorovDesignSpatialExperiments1996, nyberg2012, fedorov1997,
  Ucinski05, neitzel2019sparse}. This phenomenon is illustrated for
our toy inverse problem in Fig.~\ref{fig:clusterization_illustration},
where D-optimal measurement locations are shown for different numbers
of measurements. Notably, for six measurements, a D-optimal design
yields two sets of measurements that are identical. Following
\cite{Ucinski05}, we refer to this intriguing phenomenon as
\emph{measurement clusterization}. We consider a design to be
\emph{clustered} when two or more of its constituent measurements are
identical.

\begin{figure}
    \centering
    \includegraphics[height=0.5\textwidth]{figs/dst_modelError0.png}
    \caption{Measurement clusterization in D-optimal designs for the
      inverse problem of the 1D heat equation. Measurement locations
      were chosen according to the Bayesian D-optimality criterion of
      Theorem \ref{thm:d_optimality}. Measurement locations are
      plotted over the computational domain \(\Omega = [0, 1]\)
      (x-axis), for varying numbers of measurements (y-axis). The
      colored numbers are measurement indices, plotted for visual
      clarity. Measurement clusterization already occurs for three
      measurements: the second measurement (red) is overlaid on the
      third (green). For five measurements, first (blue) and second
      (red) measurements are clustered, as well as the fourth (black)
      and the fifth (magenta).}
  \label{fig:clusterization_illustration}
\end{figure}


Clusterization should not be confused with either repetition nor with
replication, which are commonly viewed as beneficial and even
necessary aspects of an optimal design \cite{fisher1949design,
  morris2011, schafer2001replication}. For example,
\cite{fisher1949design} in his famous milk and tea experiment,
suggested that repetition is a "way of enlarging the experiment and,
thereby, increasing its sensitiveness". In another example,
\cite{fay2000rainfall} measured the effect of rainfall on grass growth
in plots of land. The experiment involved fifteen "rainfall
manipulation shelters", where "Four rainfall manipulation treatments
(three replicates) then were assigned to 12 of the plots". While it
seems reasonable for the researchers to replicate the phenomenon they
are trying to study, clusterization is different: a clustered design
in the rainfall experiment would imply the researchers should take a
repeated measurement \emph{on the same plot}, at the expense of
measuring grass growth in other plots. In sharp contrast to repetition
and replication, clusterization is highly nonintuitive and we will see
that its origins run considerably deeper.


Researchers of inverse problems widely agree that measurement
clusterization is undesirable
\cite{fedorovDesignSpatialExperiments1996, nyberg2012, fedorov1997,
  Ucinski05, neitzel2019sparse}, prompting the exploration of various
remedies to address this issue. One approach involves merging close
measurements \cite{fedorov1997}; however, this strategy merely
overlooks the phenomenon of measurement clusterization. An alternative
solution lies in \emph{clusterization-free design}s, where measurement
locations are deliberately chosen to be distant from one another. This
can be achieved by imposing distance constraints between measurements
or by introducing correlated errors that account for both observation
error and model misspecification \cite{Ucinski05}. For instance, in
the context of time-series analysis for pharmacokinetic experiments,
measurement clusterization can be mitigated by incorporating the
modeling of auto-correlation time within the noise terms
\cite{nyberg2012}.


In spatial problems involving choice of measurements within a domain
\(\Omega \subseteq \mathbb{R}^d, d=1,2,3\), many researchers
circumvent the problem of measurement clusterization by restricting
measurements to a coarse grid in \(\Omega\) \cite{koval2020,
  alexanderian2021, attia2022, alexanderian2014, alexanderian2016,
  alexanderian2018efficient, brunton2016}. This approach incurs a
significant computational cost as it requires solving a difficult
combinatorial optimization problem for measurement locations over a
discrete set. The combinatorial optimization problem is usually
relaxed by first assigning optimal measurement weights in
\(\mathbb{R}_+\) to the potential measurement locations. Some
researchers incorporate a sparsifying \(\ell_1\) penalty term into the
design criterion, which is subsequently thresholded to achieve the
desired binary design over the coarse grid
\cite{horesh2008borehole}. Others progressively relax the \(\ell_1\)
penalty to an \(\ell_0\) penalty via a continuation method
\cite{alexanderian2016, alexanderian2014}. Others cast the problem of
finding optimal measurement weights as a stochastic optimization
problem \cite{attia2022stochastic}. All of the aforementioned methods
may indeed find a binary optimal design restricted to a given coarse
grid. However, none addresses one fundamental issue: the restriction
of measurement locations to a coarse grid in \(\Omega\) fundamentally
changes the optimal design problem and thus results in a sub-optimal
design.

Avoiding measurement clusterization is a pragmatic approach:
intuitively, researchers recognize that measurement clusterization is
undesirable, even though the underlying reasons may not be fully
clear. Consequently, they strive to prevent it and devise various
methodologies to avoid it. Yet each and every one of these
methodologies achieves its objective by imposing restrictions on
measurement locations, thereby fundamentally altering the optimal
design problem. To the best of my knowledge, no previous study has
tried to address some seemingly simple yet fundamental questions:
%
Why does imposing correlations between observations alleviate
measurement clusterization?
%
Is measurement clusterization a generic phenomenon? 
%
And, most importantly: Why does measurement clusterization occur?
%
%Should we aim to avoid measurement clusterization?
%
%Is it possible to substitute an optimal clustered design with an
%equally optimal non-clustered design?
%Can an optimal clustered design be replaced with an equally optimal
%non-clustered design?


\subsection{Contribution}
The primary objective of this study is to provide a deep understanding
of measurement clusterization by addressing the aforementioned
questions. Our focus centers around investigating the Bayesian
D-optimality criterion. We conduct an analysis of Bayesian D-optimal
designs within the context of linear inverse problems over Hilbert
spaces and study two inverse problems: (a) In Sections
\ref{section:prelim} and \ref{section:D_and_grad} we propose a novel
generic model for an inverse problem where D-optimality maintains
analytical tractability and D-optimal designs are identified via
Lagrange multipliers. This analytical framework facilitates the
exploration of the questions posed at the end of the previous
paragraph. We also study (b) the inverse problem of the 1D heat
equation from Section \ref{subsec:toy} above. Investigating both
inverse problems allows us to answer the questions posed in the
previous section:

\begin{enumerate}
\item \label{q:generic} \textbf{Is measurement clusterization a
  generic phenomenon?}
  %\subsection{An answer for Question \ref{q:generic}: Genericity of measurement clusterization}
  %% Computer implementation of Lemma \ref{thm:char} for the inverse
  %% problem outlined in Section \ref{section:how} also generates \(\obs\)
  %% as a solution to the D-optimal design problem.  Furthermore,
  We give two complementing answers to this question. First, from a
  theoretical perspective, we show that clusterization mainly depends
  on how quickly the eigenvalues of the prior covariance in
  observation space decay. This decay depends mostly on how ill-posed
  the problem at hand is and does not depend much on the prior. See
  Section \ref{section:vanishing} --- particularly, Theorem
  \ref{thm:char} and the discussion following it. We also show results
  of numerical experiments, where simulations of our model give rise
  to D-optimal designs that exhibit clusterization with high
  probability in Section \ref{subsec:lemma_sims}. Thus, given the
  genericity of our model, we expect measurement clusterization to be
  a generic and ubiquitous phenomenon.

\item \label{q:mitigate} \textbf{Why does imposing correlations
  between observations alleviate measurement clusterization?} In
  Section \ref{section:non_vanishing}, we rigorously demonstrate the
  role of model error in mitigating clusterization, thereby
  corroborating earlier observations made by various
  researchers. Specifically, our proof shows that identical
  measurements result in no gain in design criterion when observation
  error amplitude tends to zero. Moreover, in Section
  \ref{subsec:corr_errors_sims}, we show that an error term
  corresponding to correlations between measurements mitigates
  clusterization in the inverse problem of the 1D heat equation.

\item \label{q:why} \textbf{Why does measurement clusterization
  occur?} In Section \ref{subsec:why} (specifically, Fig~\ref{fig:why}
  and the discussion following it) we give a compelling answer to this
  question by transporting insights we gain from our generic model to
  the inverse problem of the 1D heat equation. Our analysis reveals
  that a D-optimal design focuses on a select set of prior
  eigenvectors, specifically those with the largest eigenvalues in the
  prior covariance spectrum. In practical scenarios, the number of
  locations where (a) all relevant prior eigenvectors are
  significantly large, and (b) other eigenvectors are close to zero,
  is limited. Consequently, the clusterization of measurements arises
  as a natural consequence of the pigeonhole principle, as there are
  more measurements available than there are locations satisfying
  conditions (a) and (b).

\end{enumerate}

\subsubsection{Implications}
Our answer to Question \ref{q:generic} implies that encountering
clusterization should be expected in many different problems across
many different scientific fields. Researchers that encounter
clusterization should not be surprised or wary. In Our answer to
Question \ref{q:why}, we explain what our view of the cause of
clusterization is. It appears, the cause is generic: a D-optimal
design reduces uncertainty for a select set of prior covariance
eigenvectors --- those with the most prior uncertainty, i.e.~those the
practitioner cares about the most! We believe practitioners should not
try to avoid measurement clusterization. Rather, practitioners should
take repeated measurements (e.g.~in MRI and borehole tomography),
increase apparatus sensitivity (e.g.~in EIT), or take consecutive
measurements (e.g.~in the 1D heat equation). Overall, we show that
measurement clusterization is a natural and (almost) inevitable part
of Bayesian D-optimal designs (but see disclaimer below).

One interesting implication of the analysis presented here is that
clusterization can serve as an evidence to the number of relevant
eigenvectors. Since leading eigenvectors typically correspond to slow
variations in space and/or time, clusterization could be used to
estimate the number of relevant degrees of freedom, and even to reduce
the complexity of a computational model, e.g.~by dropping
discretization points.

It is important to note that we do not view clustered designs as
undesirable, nor do we believe it should be avoided at
all. Clusterization is a peculiar phenomenon and it is perfectly
reasonable for someone to argue against the D-optimality criterion
based on the fact that it results in clustered designs. We have seen,
however that there is a perfectly reasonable explanation for
clusterization. We have shown that clusterization is an inevitable
consequence of having a problem with some modes where uncertainty
decays faster than others.

Lastly, we believe that when clusterization arises, it should serve as
a warning sign to practitioners. In the inverse problem of the 1D heat
equation, clusterization occurs primarily because Laplacian
eigenvectors \emph{do not} decay in $\Omega$. Consequently, measuring
$u(x_1, T)$ at some point $x_1 \in \Omega$ provides information about
$u(x_2,T)$ for distant points $x_2 \in \Omega$. Intuitively, this
should not occur: for small $T$, the heat distribution at $x_1$ should
give very little knowledge on the heat distribution at $x_2$. This
behavior stems from a well-known property of the heat equation: it
allows information to spread \emph{instantly} across the computational
domain \cite{renardy2006PDE}. In reality, heat (and information)
propagate at finite speeds. Of course, the known physical barrier for
information spread is the speed of light, but we expect heat to spread
considerably slower: heating an Olympic pool at one end should have no
immediate effect on the temperature at the other end.

Our choice of prior is also a potential major contributor to
clusterization. Our choice of Gaussian prior similarly implies
information is shared between distant locations in $\Omega$. Thus, we
suggest refraining from choosing Gaussian priors with inverse
Laplacian covariance operators. Rather, non-Gaussian priors could be
employed instead \cite{hosseini2017, hosseini2019}.

We believe that the emergence of clusterization in this context is
thus non-physical, arising from the way the inverse problems we
consider are phrased. Clusterization therefore indicates that the
underlying mathematical / Bayesian model is overly permissive and
fails to capture crucial physical constraints of the problem. We
suggest that when clusterization occurs, practitioners should consider
alternative models where information is localized in space and travels
at finite speed in the medium. Such models may not only provide more
physically accurate and meaningful results but may also mitigate the
issue of clusterization.



\subsubsection{Other Contributions}
In Theorem \ref{thm:char}, we also show that D-optimal designs are
best understood in the space of \emph{observations}, see Section
\ref{section:vanishing} for a precise statement. This is in accordance
with previous work by \cite{koval2020}, who showed that A-optimal
designs are best constructed in the space of observations.

In the process of proving Theorem \ref{thm:char} we prove and
generalize several lemmas. Among those, is Lemma \ref{lemma:free},
which is (to the the best of my knowledge) novel: We decompose a
symmetric positive definite matrix \(M \in \mathbb{R}^{k \times k}\)
with \(\ttr M = m \in \mathbb{N}\) as \(M = AA^t\), where \(A\in
\mathbb{R}^{k \times m}\) has unit norm columns.


\subsection{Limitations}\label{subsec:limitations}
The main limitation of this study is that our generic model does not
correspond to any specific real-life problem. Specifically, in its
current form, our model does not allow point evaluations. Thus, while
our model is generic enough to be analytically tractable, some may
argue that our model is too far removed from any real application. To
these claims I would answer that scientists have a long history of
studying models that are bare-bones simplifications of real systems,
e.g.~the Ising model \cite{cipra1987}, the Lorenz system \cite{brin},
the Lotka-Volterra equations \cite{logan2006}, the Carnot engine
\cite{kardar2007} and many others.
