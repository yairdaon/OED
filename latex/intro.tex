\section{Introduction}\label{section:OED intro}
Experimental design is an important part of many scientific
investigations. When considering an inverse problem, one can often
specify sensor locations (e.g.\ in electricat impedance tomography
\cite{horesh2010impedance}), certain wavelengths (e.g.\ in MRI
\cite{horesh2008mri}), or location of sensors for waves reflecting
from the ground (e.g.\ searching for oil \cite{horesh2008borehole} or
using a radar). Whatever the allowed set of measurements is, one
should select the optimal observations to take, in order to increase
accuracy, reduce costs, or both.

A set of measurements is called a \emph{design}, and an \emph{optimal
design} is found by optimizing some \emph{design
criterion}. Surprisingly, optimal designs sometimes result in
measuremets that are very similar. For example consider measurements
taken by sensors that evaluate some function at $\x \in \Omega
\subseteq \R^d, d=1,2,3$. An optimal design may place sensors in very
close proximity to each other (Figure \ref{fig:clusterization
  illustration}). Following \cite{Ucinski05}, we refer to this
phenomenon as \emph{sensor clusterization}.

There is a consensus among researchers that sensor clusterization is
undesirable. Some authors acknowledge the problem of sensor
clusterization and suggest remedies \cite{fedorov1996, hooker2009,
  fedorov2012, Ucinski05, neitzel2019sparse}. Others bypass the
problem by allowing sensor locations to be chosen from a coarse finite
grid \cite{alexanderian2014sparsification,
  AlexanderianPetraStadlerEtAl16, alexanderian2018efficient,
  koval2020, attia2020}, thus eliminating the possibility of sensor
clusterization. Sadly, there is no free lunch, and choosing sensor
locations from a coarse finite grid requires solving a hard
combinatorial optimization problem. The current study is the first, to
my knowledge, that aims to gain a deep understanding of sensor
clusterization.


\subsection{D-Optimal Designs}
In this study we focus on the Bayesian \emph{D-optimality} criterion
\cite{Chaloner1995}: a D-optimal design maximizes the expected
Kullback-Leibler divergence \cite{CoverThomas91} between posterior and
prior. For a linear model in finite dimensions, with Gaussian prior
and Gaussian noise, a D-optimal design minimizes the determinant of
the posterior covariance matrix\footnote{The frequentist version of
the D-optimality criterion aims to minimize the determinant of the
Fisher information matrix \cite[page 16]{Ucinski05}. For a linear
model with Gaussian noise, the Bayesian criterion is just
a regularized version of the frequentist criterion
\cite{Chaloner1995}.}.

Bayesian D-optimal designs for inverse problems on function spaces
were first studied by \cite{AlexanderianGloorGhattas14,
  AlexanderianPetraStadlerEtAl16,
  AlexanderianPetraStadlerEtAl14}. Similarly to the finite dimensional
case, a D-optimal design maximizes the KL divergence between posterior
and prior measures. For a linear inverse problem with Gaussian prior
and Gaussian noise, a D-optimal design minimizes an expression
analogous to the determinant of the posterior covariance operator
\cite{AlexanderianGloorGhattas14} (also \cref{thm:d optimality}
below).

Considerable effort focused on computational aspects of finding
optimal designs in infinite-dimensional Bayesian inverse problems.
Attia \cite{attia2020} focused on QoI, Koval considered model
uncertainty and focused on reducing forward and adjoint solves, using
randomized linear algebra.\cite{koval2020}.


\subsection{Remedies to Sensor Clusterization}
Several remedies to sensor clusterization were suggeted through the
years, and we will now overview some. In \emph{clusterization-free
design} locations are forced to be far from each other
\cite{Ucinski05}. This is achieved in a frequentist and
finite-dimensional setting, by either imposing distance constraints
between observations, or by introducing correlated errors which
account for both observation error and model error
\cite{Ucinski05}. Merging close measurements is also possible
\cite{fedorov2012}. Sensor clusterization is mitigated in time-series
analysis for pharmacokinetic experiments by modeling auto-correlation
time in the noise \cite{hooker2009}, and this approach is similar in
spirit to imposing error correlations as in \cite{Ucinski05}.


Many authors bypass the sensor clusterization problem by restricting
measurements to a coarse finite grid \cite{koval2020,
  AlexanderianPetraStadlerEtAl16, alexanderian2021, attia2020}, but
this practice has two flaws: First, using a coarse grid requires
solving a hard combinatorial optimization problem. Second, restricting
sensors to a such a small subset of the computatoinal domain will
necessarily give rise to sub-optimal designs. Several remedies were
offered for the first problem: Stadler suggested ell0
sparsification. Others suggested other stuff. None were offered for
the second.


%% Today, including model errors are a bona-fide????  standard in the
%% inverse problems community: \cite{attia2020}, \cite{koval2020},
%% \cite{Alexanderian2021} all empower their models.  Both studies
%% present numerical techniques for finding optimal designs when model
%% error is present. Both are restricted to linear inverse problems
%% (although in the latter the authors use their method on a nonlinear
%% problem by taking a Laplace approximation for the posterior). Both
%% find an optimal design by first solving a continuous problem for
%% sensor weights. Said solution is then sparsified to give a binary
%% design. Both studies are successful in the task of Bayesian
%% inversion. Neither \cite{attia2020}, nor \cite{koval2020} mention any
%% effect model errors can have on sensor clusterization.


\subsection{Contribution}
Any of the above mentioned approaches might serve as a remedy and push
sensors away from each other. Yet, none offers any insight as to why
clusterization occurs. Also, as better models are employed, model
error is decreased and the clusterization phenomenon will eventually
reappear. While these approaches are practical and help practitioners
avoid sensor clusterization, they do not provide insight as to why
sensors are clustering.

The current study aims to fill the gap of understanding sensor
clusterization in infinte dimensional Bayesian linear inverse
problems. We propose and study a relaxed and analytically tractable
model for D-optimal designs. Under this model, D-optimal designs are
solutions of a constrained optimization problem, formulated using
Lagrange multipliers. The Lagrange multipliers formulation allows us
to rigorously show how model error mitigates clusterization (Section
\ref{section:non vanishing}), and why designs that exhibit
clusterization are indeed optimal when no model error is considered.

As a byproduct, the Lagrange multipliers problem for no model error
results in a characterization of D-optimal designs. The key idea is
that a D-optimal design reduces uncertainties where they are highest
first. This is the conclusion of part (5) of Theorem \ref{thm:char},
which we state below and prove in Section \ref{section:vanishing}.

\begin{restatable}[D-optimal designs with vanishing model error]{theorem}{char}\label{thm:char}
  %% \begin{theorem}[D-optimal designs with vanishing model error]\label{thm:char}
  %% Let Notation \ref{not:setup}. Further, let

  Let:
  \begin{itemize}
  \item $\hilp, \hilo$ Hilbert spaces.
  \item $\fwd:\hilp \to \hilo$ a linear compact operator.
  \item $\pr \sim \mathcal{N}(0, \prcov)$ prior Gaussian measure on $\hilp$,
    where $\prcov:\hilp \to \hilp$ is the prior covariance operator.
  \item $\obs: \hilo \to \mathbb{R}^m$ observation operator, where $m
    \in \mathbb{N}$ is the number of observations taken. 
  \item $\sigma^2 \in \mathbb{R}_{+}$ observation noise variance,
    $\data = \obs \fwd \param + \eps$, where $\eps \in \mathbb{R}^m$
    is iid $\mathcal{N}(0, \sigma^2)$ noise.
  \item $\post$ the posterior measure, with covariance $\postcov$.
  \item A D-optimality utility function
    \cite{AlexanderianGloorGhattas14}:
    \begin{align*}
      \begin{split}
        \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det ( I + \sigma^{-2} \prcov^{1/2} \fwd ^*
        \obs^* \obs \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  \item $\{\lambda_i\}_{i=1}^\infty$ eigenvalues of $\fwd\prcov\fwd^*$
    in decreasing order of magnitude.
  %% \item $\{\ev_i\}_{i=1}^\infty$ their corresponding eigenvectors.
  \item $\{\eta_i\}_{i=1}^\infty$ eigenvalues of $\obs^*\obs$.
  \item $\obs$ a D-optimal design operator
    \begin{equation*}
      \obs = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs).
    \end{equation*}
  \end{itemize}

  Then:
  \begin{enumerate}
  \item  $\tr{\obs^*\obs} = m$.
  \item $\obs^*\obs$ and $\fwd\prcov\fwd^*$ are simultaneously
    diagonalizable.
  \item $k := \rank \obs^*\obs \leq m$ and
    \begin{equation*}
      \tar(\obs) = \frac12 \sum_{i=1}^{k} \log (1 + \sigma^{-2}\lambda_i\eta_i). %= \frac12 \sum_{i=1}^{m} \log (1 + \sigma^{-2}\lambda_i\eta_i).
    \end{equation*}
  %% \item 
  %%   \begin{equation*}
  %%     k = \argmax \left \{ k:\lambda_k^{-1} < \sigma^{-2}\frac{m}{k} + \frac{1}{k} \sum_{j=1}^{k}
  %%     \lambda_j^{-1} \right \}.
  %%   \end{equation*}
  \item
    \begin{equation*}
        \eta_i = \begin{cases}
          \frac{m}{k} - \sigma^2 \lambda_i^{-1} + \sigma^2 \frac{1}{k} \sum_{j=1}^k \lambda_j^{-1} & 1 \leq i \leq k \\
          0 & i > k 
        \end{cases}.
    \end{equation*}
  \item The covariance of the pushforwad $\fwd_{*} \post$ is $\left (
    (\fwd \prcov \fwd^*)^{-1} + \sigma^{-2} \obs^*\obs \right )^{-1}$
    and has eigenvalues
    \begin{equation*}
      \theta_i =
      \begin{cases}
        \left(\frac{\sum_{j=1}^k \lambda_j^{-1} + \sigma^{-2}m}{k} \right )^{-1} & i \leq k \\
        \lambda_i &  i > k 
      \end{cases}
    \end{equation*}
  \end{enumerate}
%\end{theorem}
\end{restatable}

One byproduct of the proof of Theorem \ref{thm:char} is a novel lemma
in linear algebra: We show how to decompose a symmetric positive
definite matrix $M$ as $M = AA^t$, where $A$ has unit norm columns
(Lemma \ref{lemma:lax}). We also generalize two lemmas from linear
algebra to infinite-dimensional settings: In Lemma \ref{lemma:MDL} we
prove a matrix determinant lemma in Hilbert spaces, and in Lemma
\ref{lemma:lax} we generalize a lemma \cite{Lax07} for calculating
$\frac{\der}{\der t} \log \det (I + X(t))$, where $X(t)$ is an
operator valued function.

\subsection{Limitations}\label{subsec:limitations}
The study presented here is not free from drawbacks. First, our
relaxed model does not correspond to any real-life problem. It is
generic enough to be analytically tractable, but one may argue it is
to far removed from any real application. To these claims I would
argue that Mathematicians have a long history of studying models that
are bare-bones simplifications of real systems, e.g. the Ising model
\cite{cipra1987} the Lorenz system \cite{brin}, and the SIR model
\cite{keeling2008}, to name a few. The second drawback of our model is
that we do not show rigorously that clusterization necessarily occurs,
we only show that it does not pose any obstruction to D-optimality.


\subsection{An Example of Clusterization}\label{subsec:example}
\begin{figure}
  \begin{tikzpicture}[thick, scale=1.3, every node/.style={scale=0.99}]
    \begin{axis}
      [
      title={Posterior Pointwise Standard Deviations and D-Optimal Sensor Locations},  
      xmin = 0,
      xmax = 3.14,
      xlabel = {$x$},
      ylabel = posterior std,
      ymin   = 0,
      %compat = 1.3,
      % ymax   = 130,
      % ytick = \empty,
      legend cell align=left,
      % legend style={at={(0.45,0.2)}}
      legend pos= outer north east 
      ]
      % \draw[black!30!white, thin] (50,0) -- (50,130);
      % 
      %% \addplot [thin, black, mark=none] table{stdv-heat-sens1-var1.txt};
      %% \addlegendentry{1 sensors};
      
      %% \addplot [thin, blue, mark=none] table{stdv-heat-sens2-var1.txt};
      %% \addlegendentry{2 sensors};
      
      %% \addplot [thin, red, mark=none] table{stdv-heat-sens3-var1.txt};
      %% \addlegendentry{3 sensors};
      
      \addplot [thin, green, mark=none] table{stdv-heat-sens4-var1.txt};
      \addlegendentry{4 sensors};
      
      \addplot [thin, purple, mark=none] table{stdv-heat-sens5-var1.txt};
      \addlegendentry{5 sensors};
      
      \addplot [thin, cyan, mark=none] table{stdv-heat-sens6-var1.txt};
      \addlegendentry{6 sensors};

  
      %% \addplot [black,  only marks, mark=x, mark size=1.5] 
      %% table{locs-heat-sens1-var1.txt}; 
      %% \addplot [blue,   only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens2-var1.txt}; 
      %% \addplot [red,    only marks, mark=x, mark size=1.5]
      %% table{locs-heat-sens3-var1.txt};
      \addplot [green,  only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens4-var1.txt}; 
      \addplot [purple, only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens5-var1.txt}; 
      \addplot [cyan,   only marks, mark=*, mark size=1.5] 
      table{locs-heat-sens6-var1.txt}; 
  
      
    \end{axis}
  \end{tikzpicture}
  \caption{Sensor clusterization for inferring the initial condition
    of the 1D heat equation. Posterior pointwise standard deviations
    (lines) are plotted over the domain $[0, \pi]$, for varying
    numbers of sensors. Sensor locations (circles) were chosen to
    optimize the D-optimality criterion. Sensor clusterization occurs
    for six sensors. Only four observation locations are used --- two
    pairs of sensors are so close together they are
    indistinguishable.}
  \label{fig:clusterization illustration}
\end{figure}

For the purpose of illustration, consider a toy inverse problem ---
inferring the initial condition of the 1D heat equation in $[0,\pi]$
with a homogeneous Dirichlet boundary condition. Sensor clusterization
is clearly evident, see Figure \ref{fig:clusterization}.

The 1D heat equation with homogeneous Dirichlet boundary condition is:
\begin{subequations}\label{eq:heat equation}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,\pi] \times [0,\infty),\\
      u &= 0 &&\qquad \text{on } \{0, \pi\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,\pi] \times \{0\}.
  \end{alignat}
\end{subequations}


The goal is to infer the initial condition $u_0$. Model the initial
condition as $u_0 \sim \normal(0,\prcov)$, for $\prcov =
(-\Delta)^{-1}$ with a homogeneous Dirichlet boundary condition. Fix
some final time $T$ and denote by $\fwd$ the forward operator, so that
$u(\cdot, T) = \fwd u_0$. Let $\obs$ the observation operator, so that
$u(\x_j,T) = (\obs u)_j, j=1,\dots,m$. Assume iid centered Gaussian
observation error, so observations are given by $u(\x_j,T) +
\eps(\x_j)$ with $\eps(\x_j) \sim \normal(0, \sigma^2)$ iid, $\sigma^2
> 0$. The posterior covariance is known and depends only on $\prcov,
\fwd, \obs$ and $\sigma^2$ \cite{Stuart10} (see Section
\ref{section:prelim} and \eqref{eq:postcov} below). Finally, choose
$\x_j,j=1,\dots,m$ to optimize the D-optimality criterion
\cite{AlexanderianGloorGhattas14}.

Sensor clusterization is illustrated in Figure \ref{fig:clusterization
  illustration}. The posterior covariance does not depend on data, so
the plot has no reference to actual data observed. The posterior
covariance does depend on location of the observation taken, which
were chosen to optimize the D-optimality criterion. Sensor
clusterization is clearly seen for six sensors; it looks like only
four observations were taken. Two pairs of sensors are clustered so
close together that they are indistinguishable from each other.


%% A D-optimal design
%% is found via a generalization of the D-optimality criterion to
%% infinite dimension(also, Section
%% \ref{subsec:D optimal design} below).
