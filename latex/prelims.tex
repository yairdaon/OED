\section{Preliminaries and Notation}\label{section:prelim}
Here we present the basics of Bayesian inverse problems over Hilbert
spaces. Since we are ultimately interested in inferring a function
over some domain, we will keep in mind that these Hilbert spaces
should really be thought of as function spaces, utilizing a setup
similar to \cite{knapik2011}. A deeper treatment of the foundations
for inverse problems over function spaces can be found in
\cite{Stuart10}.


\subsection{Forward Problems}\label{subsec:abstract_OED}
In this section we give definitions and notations for forward
problems, which are an essential part of the inverse problems we
discuss later. Consider a "parameter space" \(\hilp\) and an
"observation space" \(\hilo\) --- both separable Hilbert spaces (the
subscripts p and o are for "parameter" and "observation",
respectively). The parameter space includes the quantity we seek to
infer; in the inverse problem of the heat equation, the parameter
space $\hilp$ is where the initial condition lives. The observation
space $\hilo$, on the other hand, is the space from which we take
measurements; in the example of the heat equation, $u(\cdot, T) \in
\hilo$.

The connection between parameter and observation spaces is made by the
\emph{forward operator} \(\fwd: \hilp \to \hilo\). We assume the
forward operator \(\fwd\) is linear. In the inverse problem of the 1D
heat equation, the forward operator is determined by the time
evolution of the 1D heat equation \eqref{eq:heat1} and
\eqref{eq:heat2}, so $u(\cdot, T) = \fwd u_0$.

Measurements are taken via a linear \emph{measurement operator}
\(\obs\), which is the concatenation of linear functionals
$\meas_1,\dots,\meas_m$ we call \emph{measurements}:
\begin{equation*}%\label{eq:O}
  \obs u = (\meas_1(u), \dots, \meas_m(u) )^t \in \R^m,\ u \in \hilo.
\end{equation*}
%% \footnote{It is common for the measurement and forward
%% operators to be merged \(\tmp := \obs \fwd\)
%% \cite{AlexanderianGloorGhattas14}, but the analysis carried out in the
%% following sections requires that \(\fwd\) and \(\obs\) are explicitly
%% separated, as in \cite{attia2022stochastic,
%%   cvetkovic2023choosing}.}
Thus, \(\obs \in ( \hilo^* )^m\), where \(m\) is the number of
measurements taken\footnote{The alert reader will likely ask how do we
reconcile point measurements $\delta_x$ as suggested by the
formulation of the 1D heat equation with working in Hilbert spaces. We
don't. We follow standard practice in the literature and restrict our
analysis to Hilbert spaces. We can satisfy ourselves with the fact
that point evaluations could be approximated in a standard Hilbert
space like $L^2(\Omega)$.}. %% Footnote

Data is acquired via noisy observations, and we consider two types of
error terms: Spatially correlated model error \(\eps' \sim
\normal(0,\modcov)\) with \(\modcov\) a covariance operator; and
observation error denoted \(\eps \sim \normal(0, \sigma^2 I_m)\), with
\(I_m \in \mathbb{R}^{m \times m}\) the identity. Both error terms and
the prior (see Section \ref{subsec:bayesian_inverse_problems} below)
are assumed independent of each other. Thus, data is acquired via
\begin{align}\label{eq:inverse_problem}
  \data := \obs (\fwd \param + \eps') + \eps = \obs \fwd \param + \obs \eps' + \eps.
\end{align}

It is easy to verify that \(\obs \eps' + \eps \in \R^m\) is a centered
Gaussian random vector with covariance matrix

\begin{align}\label{eq:Sigma}
  \begin{split}
    \Sigma(\obs) :&= \mathbb{E}[ (\obs \eps' + \eps) (\obs \eps' +
      \eps)^t ]
    % 
    % 
    = \obs \modcov \obs^* + \sigma^2I_m , 
  \end{split}
\end{align}
where
\begin{align}\label{eq:modcov_explained}
  \begin{split}
    [\obs \modcov \obs^*]_{ij} & = e_i^t \obs \modcov \obs^* e_j 
    %
    %
    %
    = \meas_i (\modcov \meas_j).% \text{ (by \eqref{eq:obs*})}.
  \end{split}
\end{align}
Taking \(\modcov = 0\) is a common practice
\cite{tarantola2005,kaipio2005,Vogel02} and then \(\Sigma =
\sigma^2I_m\) is a scalar matrix which does not depend on \(\obs\).

\subsection{Bayesian Linear Inverse Problems}\label{subsec:bayesian_inverse_problems}
In the previous section, we saw how a parameter $u\in \hilp$ is
transported to the observation space via the forward operator $\fwd u
\in \hilo$, how observations are generated from a parameter via $\obs
\fwd u$ and how observations and noise give rise to data $\data$. It
is time to formulate the process of inferring the parameter as a
Bayesian inverse problem. We have already defined the Gaussian
likelihood in the previous section, and now we will define the prior.

We take a Gaussian prior \(\param \sim \pr = \normal(\prmean
,\prcov)\) with some appropriate covariance operator \(\prcov\) on
\(\hilp\) \cite{Stuart10}. For example, for the inverse problem of the
1D heat equation we chose $\prcov = (-\Delta)^{-1}$, as described in
Section \ref{subsec:toy}. Note that \(\fwd \prcov \fwd^*\) is the
prior covariance in \(\hilo\) \cite{Stuart10}, and as such is assumed
invertible --- an assumption which we will use later (if \(\fwd\) has
a nontrivial kernel we utilize Occam's Razor and ignore said kernel
altogether).

Since $\fwd$ is linear and $\pr$ is Gaussian --- the posterior
\(\post\) is Gaussian as well. We do not utilize the posterior mean in
this study, but the posterior covariance operator $\postcov$ is given
by the known formula \cite{Stuart10}:
\begin{align}\label{eq:postcov}
  \postcov = (\prcov^{-1} + \fwd^* \obs^* \Sigma^{-1} \obs \fwd
  )^{-1}.
\end{align}

\subsection{Bayesian D-Optimal Designs}\label{subsec:D_optimal_design} 
A Bayesian D-optimal design maximizes the expected KL divergence
between posterior \(\post\) and prior measures \(\pr\). For arbitrary
posterior and prior measures, the KL divergence is defined,
analogously to eq.~\eqref{eq:basic_KL}, via the Radon-Nikodym
derivative:
\begin{equation*}
  D_{KL}(\post||\pr) = \int \log \frac{\der \post}{\der \pr}(\param) \der \post(\param).
\end{equation*}

The study of D-optimal designs for Bayesian linear inverse problems in
infinite dimensions was pioneered by \cite{AlexanderianGloorGhattas14,
  alexanderian2018efficient}. The main result we will make use of is
summarized (in our notation) below:

\begin{theorem}[Alexanderian, Gloor, Ghattas \cite{AlexanderianGloorGhattas14}]\label{thm:d_optimality}
  Let \(\pr = \normal(\prmean,\prcov)\) be a Gaussian prior on \(\hilp\)
  and let \(\post = \normal(\postmean,\postcov)\) the posterior measure
  on \(\hilp\) for the Bayesian linear inverse problem \(\data = \obs
  \fwd\param + \obs \eps' + \eps\) discussed above. Then
  \begin{align}\label{eq:objective}
    \begin{split}
      \tar( \obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
      % 
      % 
      % 
      &= \frac12 \log \det 
      ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}).
    \end{split}
  \end{align}
\end{theorem}

In \cite{AlexanderianGloorGhattas14, alexanderian2018efficient},
results are stated for \(\Sigma=I\) (implied by \(\modcov =
0,\sigma^2=1\)), but these results also hold for more general
covariance matrices \cite[p. 681]{AlexanderianGloorGhattas14}.

%% It is important to note that since \(\obs\) is finite-rank,
%% \(\prcov^{1/2} \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}\) is
%% trace-class.
\begin{definition}\label{def:d_optimality}
  We say \(\opt\) is \emph{D-optimal} if \(\opt =
  \argmax_{\obs} \tar(\obs)\), where entries of \(\obs \in (\hilo^*)^m\)
  are constrained to some allowed set of measurements in \(\hilo^*\).
\end{definition}

Intuition for Theorem \ref{thm:d_optimality} can be gained by
recalling from Section \ref{subsec:D} that for a Bayesian linear model
in finite dimensions, with Gaussian prior and Gaussian noise, a
D-optimal design minimizes the determinant of the posterior covariance
matrix. Theorem
\ref{thm:d_optimality} and Definition \ref{def:d_optimality} carry a
similar intuition:
\begin{align*}
  \begin{split}
    \tar(\obs) &= \frac12 \log \det ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}) \text{ by \eqref{eq:objective}}\\
    &= \frac12 \log \det \Big( \prcov ( \prcov^{-1} + \fwd ^* \obs^* \Sigma^{-1} \obs \fwd) \Big )\\
    &= \frac12 \log \det \prcov \postcov^{-1} \text{ by \eqref{eq:postcov}}.
    %%  &= \frac12 \log \det \prcov -\frac12 \log \det \postcov.
  \end{split}
\end{align*}
We think of \(\prcov\) as constant, in the sense that $\prcov$ does
not depend on data $\data$. Thus, a D-optimal design minimizes a
quantity analogous to the posterior covariance determinant, similarly
to the finite-dimensional case.


%% \subsection{Notation Summary}\label{subsec:notation}
%%   Our notation is summarized below. We let:
%%   \begin{itemize}
%%   \item \(\hilp, \hilo\) Hilbert spaces.
%%   \item \(\fwd:\hilp \to \hilo\) a linear compact operator.
%%   \item \(\pr \sim \mathcal{N}(0, \prcov)\) prior Gaussian measure on
%%     \(\hilp$, with prior covariance operator \(\prcov:\hilp \to \hilp$.
%%   \item \(\obs: \hilo \to \mathbb{R}^m\) measurement operator, where \(m
%%     \in \mathbb{N}\) is the number of measurements taken.
%%   \item \(\sigma^2 \in \mathbb{R}_{+}\) observation noise variance.
%%   \item \(\modcov\) model error covariance operator.
%%     %% \(\data = \obs \fwd \param + \eps$, where \(\eps \in
%%     %% \mathbb{R}^m\) isiid \(\mathcal{N}(0, \sigma^2)\) noise.
%%   \item \(\Sigma(\obs) = \obs \modcov \obs^* + \sigma^2I$. 
%%   \item \(\post\) the posterior measure, with covariance \(\postcov$.
%%   \item A D-optimality design criterion
%%     \cite{AlexanderianGloorGhattas14}:
%%     \begin{align*}
%%       \begin{split}
%%         \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
%%         % 
%%         % 
%%         % 
%%         &= \frac12 \log \det ( I + \prcov^{1/2} \fwd ^* \obs^* \Sigma(\obs)^{-1} \obs
%%         \fwd \prcov^{1/2}).
%%       \end{split}
%%     \end{align*}
%%   %% \item \(\{\lambda_i\}_{i=1}^\infty\) eigenvalues of \(\fwd\prcov\fwd^*$
%%   %%   in decreasing order of magnitude.
%%   %% %% \item \(\{\ev_i\}_{i=1}^\infty\) their corresponding eigenvectors.
%%   %% \item \(\{\eta_i\}_{i=1}^\infty\) eigenvalues of \(\obs^*\obs$.
%%   \end{itemize}


%% \subsection{Sequential vs Simultaneous Optimization}\label{subsec:seq_vs_sim}
%% From defintion \ref{def:d_optimality} we wish to characterize solution(s) of the
%% following optimization problem for \(\tar$. %%: (\hilo^*)^m \to \R$:
%% \begin{align}\label{eq:optimization}
%%   \obs^{\star} := \argmax_{\obs} \tar( \obs ) 
%%   = \argmax_{\obs} \frac12 \log \det 
%%   (I + \prcov^{1/2} \fwd^*\obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}),
%% \end{align}
%% where \(\obs\) is constrained to some allowed set of observations. We
%% call this problem ``simultaneous optimization'', since all
%% observations are decided on simulatneously.

%% For computational reasons, one may prefer to find the best
%% observations in a sequential manner. Denote
%% \begin{equation}\label{eq:def_obs_k}
%%   \obs_k := (\meas_1,\dots, \meas_k)^t,  k\leq m.
%% \end{equation}
%% Sequential optimal design proceeds as follows. Find \(\meas_1\) by
%% maximizing \(\tar(\obs_1)$. Then, keeping \(\meas_1\) fixed --- find
%% \(\meas_2\) as the maximizer of \(\tar(\obs_2)$. Then, find \(\meas_3\) by
%% keeping \(\meas_1,\meas_2\) fixed and taking \(\meas_3\) as the maximizer
%% of \(\tar(\obs_3)$. Continue this way until \(\obs_m = \obs\) is
%% found, where \(m\) is the number of available observations. %% It is
%% %% important to notice that this scheme does not require actually
%% %% observing data --- in \eqref{eq:objective} data is averaged out.

%% The analysis in this paper is conducted for the general simultaneous
%% optimization case. The sequential optimization case is dealt with in
%% section \ref{subsec:clusterization_sequential}. It is important to
%% note, however that all conclusions we arrive at for the simultaneous
%% case easily specialize to the sequential case by considering the
%% posterior as the next sequential step's prior.

