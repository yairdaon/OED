\section{Preliminaries and Notation}\label{section:prelim}

\subsection{Overview}
In this section we will define notations that will be used throughout
the paper. Notations presented in the section are summarized in
Section \ref{subsec:notation}. The theoretical foundations for inverse
problems over function spaces can be found in \cite{Stuart10}.

\subsection{Bayesian Linear Inverse Problems}\label{subsec:abstract OED}
Let $\hilp$ and $\hilo$ be separable Hilbert spaces (the subscripts p
and o are for ``parameter'' and ``observation'', respectfully). Denote
$\hilo^* \simeq \hilo$ the Hilbert space of all linear functionals on
$\hilo$. Consider $\fwd: \hilp \to \hilo$, a linear operator (the
``forward operator''). We are interested in forward operators that are
strongly smoothing (have fast decaying modes --- the heat operator
from section \ref{subsec:example} is a prime example). Our goal is to
infer $\param \in \hilp$ --- some parameter of the dynamics --- given
noisy observations of $\fwd \param$. We take a Gaussian prior $\pr$
for $\param$. Hence, $\param \sim \pr = \normal(\prmean ,\prcov)$ with
some appropriate covariance operator $\prcov$ \cite{Stuart10}. It is
important to note that $\fwd \prcov \fwd^*$ is the prior covariance in
$\hilo$ \cite{Stuart10}. As such, it is assumed invertible --- an
assumption we rely on below. Throughout this paper, we denote $m$ the
number of observations we are allowed to take. Observations are taken
via the operator $\obs \in ( \hilo^* )^m$. In an analogy with linear
algebra, where row vectors are thought of as linear functionals, we
think of $\obs$ as having ``rows'' $\meas_j, j=1,\dots,m$:
\begin{equation}\label{eq:O}
  \obs = (\meas_1,\dots, \meas_m)^t, \meas_j \in \hilo^*, j = 1,\dots,m.
\end{equation}
This way, for $u \in \hilo$ we have $\obs u = (\meas_1(u), \dots,
\meas_m(u) )^t \in \R^m$.
%% but we may drop the parentheses and write $\meas_j u = \meas_j(u)$.
%% A few observations regarding $\obs$ are in order.
%% First, it is good to keep in mind that $( \hilo^* )^m$ is a Hilbert
%% space with norm $\| \obs \| = \sum_{j=1}^m \|\meas_j\|$.
For $u \in \hilo$ and $v\in \R^m$:
\begin{align*}
  \big (\obs^*v \big ) (u) &= \langle v, \obs u \rangle_{\R^m} = \sum_{j=1}^m  v_j \meas_j(u)
  = v^t \left ( \obs u \right ) = (v^t \obs) (u),
\end{align*}
and thus:
\begin{align}\label{eq:obs*}
  \obs^*v &= \sum_{j=1}^m v_j \meas_j = v^t \obs.
\end{align}

%% \begin{observation}
%%   It is best to think of $\meas_j,j=1,\dots,m$ as row vectors and of
%%   $\obs$ as a matrix with rows $\meas_j$. 
%% \end{observation}

Each observation is a linear functional, chosen from some subset of
$\hilo^{*}$. Data is acquired via noisy observations
\begin{align}\label{eq:inverse problem}
  \data := \obs (\fwd \param + \eps') + \eps = \obs \fwd \param + \obs \eps' + \eps,
\end{align}
with $\data \in \R^m$ and $\eps, \eps'$ defined next. We consider two
types of error terms. First, there is spatially correlated model error
$\eps' \sim \normal(0,\modcov)$, modeled as a centered Gaussian
measure on $\hilo$ with covariance operator $\modcov$. Then there is
observation error $\eps \sim \normal(0, \sigma^2 I_m)$, with $I_m$ the
$m \times m$ identity. Both error terms and the prior are assumed
independent of each other. In the literature, it is common for the
observation and forward operators to be merged $\tmp := \obs \fwd$
\cite{AlexanderianGloorGhattas14}. Since we need to separate the
observation operator $\obs$ from the forward operator $\fwd$ in order
to carry out the analysis in the following sections, we explicitly
separate $\fwd$ and $\obs$ in \eqref{eq:inverse problem}. We now turn
to understand the terms present in \eqref{eq:inverse problem}.


\subsubsection{Dynamics and Observation Operators}\label{subsec:dynamics}
Rows of $\obs$ are linear functionals and we refer to these as {\it
  observation vector}s. Note that we cannot choose every observation
vector we want. For example, we may be restricted only to pointwise
evaluations of $\fwd \param$ by the sensors at our disposal, in which
case measuring, e.g., the mean $\ell(u) = \int_{\Omega}u$ of a
function is not possible. Thus, it is best to think of $(\obs \fwd
\param)_j$ as approximations to pointwise evaluations. For example, if
$\hilo = L^2([-\pi, \pi])$, consider $\ell_1(x) := (2\pi
\eps)^{-\frac{1}{2}}\exp(-\frac{x^2}{2\eps^2})$ and $\ell_2(x)
:=\eps^{-1}\mathbf{1}_{[-\frac{\eps}{2}, \frac{\eps}{2}]}$. Both
$\ell_1$ and $\ell_2$ approximate a point measurement at $0$ for small
$\eps$ and, crucially, $\ell_1, \ell_2 \in \hilo^*$.
%% It is important to note that although $\|\ell\|_1 =
%% \|\ell_2\|_1=1$, unfortunately $\|\ell_1\|_2^2 = (4\pi
%% \eps)^{-\frac{1}{2}}$ and $\|\ell_2\|_2^2 = \eps^{-1}$.


\subsubsection{Error Terms}
It is easy to verify that $\bar{\eps} := \obs \eps' + \eps \in \R^m$
is a centered Gaussian random vector with covariance
matrix

\begin{align}\label{eq:Sigma}
  \begin{split}
    \Sigma(\obs) :&= \mathbb{E}[ (\obs \eps' + \eps) (\obs \eps' +
      \eps)^t ]
    % 
    % 
    = \obs \modcov \obs^* + \sigma^2I , 
  \end{split}
\end{align}
where
\begin{equation}\label{eq:modcov explained}
  [\obs \modcov \obs^*]_{ij} = e_i^t \obs \modcov \obs^* e_j = \meas_i (\modcov \meas_j),
\end{equation}
and $e_j \in \R^m$ is the $j$th standard basis vector. The first
equality in \eqref{eq:modcov explained} holds by definition and the
second by \eqref{eq:obs*}. The explicit dependence on $\obs$ will be
mostly dropped for notational convenience, so $\Sigma(\obs) =
\Sigma$. Thus, for a fixed $\obs$ (i.e.\ a fixed set of observations)
we can equivalently write $\data = \obs \fwd \param + \bar{\eps}$ with
$\bar{\eps} \sim \normal(0,\Sigma)$. Taking $\modcov = 0$ is common
practice \cite{Tarantola05,Kaipio2006,Vogel02}. This reduces to the
case where we take iid observations. Then $\Sigma = \sigma^2I$ is
simply a scalar matrix that does not depend on $\obs$. Note that
taking an error model with a non-scalar covariance as we do here
allows us to consider model error (modeled by $\modcov$) as well as
observation error (modeled by $\sigma^2$). For example, say we believe
our forward model does not capture some small scale phenomenon.  Then
we may express this belief by saying $\tru = \fwd + \err$, with $\fwd$
depending on $\param$ and $\err$ depending on $\sspar$, with $\param
\perp \sspar$. We do not know much about this effect but it is
reasonable to assume it changes continuously in our domain. We (may
choose to) model it as $\normal (0, \modcov)$ and take $\modcov$ to
reflect the spatial (or other) variability we imagine $\err \sspar$
has. Such small scale phenomenon can arise as a modeling issue, where
we might not model the system in its entirety. It can also arise from
a numerical source, where our discretization of the system is not fine
enough to capture all small scale phenomena.
%%  In section \ref{section:non vanishing}, we will see that assuming
%% some correlation in the error mitigates the clusterization
%% phenomenon, as reported in the literature \cite{Ucinski05}.

Finally, it is useful to record a known result regarding the posterior
covariance operator of our inverse problem:
\begin{align}\label{eq:postcov}
  \postcov = (\prcov^{-1} + \fwd^* \obs^* \Sigma^{-1} \obs \fwd
  )^{-1}.
\end{align}


\subsection{D-Optimal Designs in Infinite Dimensions}\label{subsec:D optimal design} 
A D-optimal design maximizes expected KL divergence between posterior
and prior measures, denoted $\post$ and $\pr$, respecively. It is
useful to recall the definition of KL divergence for an arbitrary
prior measure:
$$
KL(\post||\pr) = \int \log \frac{\der \post}{\der \pr}(\param) \der \post(\param).
$$

The meaning of D-optimal design in infinite-dimensional Hilbert spaces
was investigated in \cite{AlexanderianGloorGhattas14}. There, the
authors make assumptions that amount to $\Sigma=I$ (implied by
$\modcov = 0,\sigma^2=1$), but we choose not to take these
simplification here. This is because $\Sigma$ can determine ``how
much'' clusterization we see. As stated
\cite[pp. 681]{AlexanderianGloorGhattas14}, the results hold for more
general covariance matrices. The conclusion is that in
infinite-dimensions, a D-optimal design is well-defined as maximizing
the expected KL divergence between posterior and prior. The main
result is summarized, using our notation, in the following theorem:
\begin{theorem}[Slightly modified Theorem 1 from \cite{AlexanderianGloorGhattas14}]\label{thm:d optimality}
  Let $\pr = \normal(\prmean,\prcov)$ be a Gaussian prior on $\hilp$
  and let $\post = \normal(\postmean,\postcov)$ the posterior measure
  on $\hilp$ for the Bayesian linear inverse problem $\data = \obs
  \fwd\param + \obs \eps' + \eps$ discussed above. Then
  \begin{align}\label{eq:objective}
    \begin{split}
      \tar( \obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
      % 
      % 
      % 
      &= \frac12 \log \det 
      ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}).
    \end{split}
  \end{align}
\end{theorem}
\begin{definition}\label{def:d optimality}
  A design $\obs^{\star}$ is said to be D-optimal if $\obs^{\star} = \argmax_{\obs} \tar(\obs)$.
\end{definition}

\noindent The intuition behind Definition \ref{def:d optimality} is
straightforward. From \eqref{eq:postcov} and \eqref{eq:objective}:
\begin{align*}
  \begin{split}
    \det ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}) &= \det \Big( \prcov ( \prcov^{-1} + \fwd ^* \obs^* \Sigma^{-1} \obs \fwd) \Big )\\
    &= \det \prcov \det \postcov^{-1}.
  \end{split}
\end{align*}
Since $\prcov$ is constant, a D-optimal design minimizes the posterior
covariance determinant, analogously to the finite-dimensional case.


\subsection{Notation Summary}\label{subsec:notation}
  Our notation is summarized below. We let:
  \begin{itemize}
  \item $\hilp, \hilo$ Hilbert spaces.
  \item $\fwd:\hilp \to \hilo$ a linear compact operator.
  \item $\pr \sim \mathcal{N}(0, \prcov)$ prior Gaussian measure on
    $\hilp$, with prior covariance operator $\prcov:\hilp \to \hilp$.
  \item $\obs: \hilo \to \mathbb{R}^m$ observation operator, where $m
    \in \mathbb{N}$ is the number of observations taken. 
  \item $\sigma^2 \in \mathbb{R}_{+}$ observation noise variance.
  \item $\modcov$ model error covariance operator.
    %% $\data = \obs \fwd \param + \eps$, where $\eps \in
    %% \mathbb{R}^m$ isiid $\mathcal{N}(0, \sigma^2)$ noise.
  \item $\Sigma(\obs) = \obs \modcov \obs^* + \sigma^2I$. 
  \item $\post$ the posterior measure, with covariance $\postcov$.
  \item A D-optimality utility function
    \cite{AlexanderianGloorGhattas14}:
    \begin{align*}
      \begin{split}
        \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det ( I + \prcov^{1/2} \fwd ^* \obs^* \Sigma(\obs)^{-1} \obs
        \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  %% \item $\{\lambda_i\}_{i=1}^\infty$ eigenvalues of $\fwd\prcov\fwd^*$
  %%   in decreasing order of magnitude.
  %% %% \item $\{\ev_i\}_{i=1}^\infty$ their corresponding eigenvectors.
  %% \item $\{\eta_i\}_{i=1}^\infty$ eigenvalues of $\obs^*\obs$.
  \end{itemize}


%% \subsection{Sequential vs Simultaneous Optimization}\label{subsec:seq vs sim}
%% From defintion \ref{def:d optimality} we wish to characterize solution(s) of the
%% following optimization problem for $\tar$. %%: (\hilo^*)^m \to \R$:
%% \begin{align}\label{eq:optimization}
%%   \obs^{\star} := \argmax_{\obs} \tar( \obs ) 
%%   = \argmax_{\obs} \frac12 \log \det 
%%   (I + \prcov^{1/2} \fwd^*\obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}),
%% \end{align}
%% where $\obs$ is constrained to some allowed set of observations. We
%% call this problem ``simultaneous optimization'', since all
%% observations are decided on simulatneously.

%% For computational reasons, one may prefer to find the best
%% observations in a sequential manner. Denote
%% \begin{equation}\label{eq:def obs_k}
%%   \obs_k := (\meas_1,\dots, \meas_k)^t,  k\leq m.
%% \end{equation}
%% Sequential optimal design proceeds as follows. Find $\meas_1$ by
%% maximizing $\tar(\obs_1)$. Then, keeping $\meas_1$ fixed --- find
%% $\meas_2$ as the maximizer of $\tar(\obs_2)$. Then, find $\meas_3$ by
%% keeping $\meas_1,\meas_2$ fixed and taking $\meas_3$ as the maximizer
%% of $\tar(\obs_3)$. Continue this way until $\obs_m = \obs$ is
%% found, where $m$ is the number of available observations. %% It is
%% %% important to notice that this scheme does not require actually
%% %% observing data --- in \eqref{eq:objective} data is averaged out.

%% The analysis in this paper is conducted for the general simultaneous
%% optimization case. The sequential optimization case is dealt with in
%% section \ref{subsec:clusterization sequential}. It is important to
%% note, however that all conclusions we arrive at for the simultaneous
%% case easily specialize to the sequential case by considering the
%% posterior as the next sequential step's prior.

