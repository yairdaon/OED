\section{Preliminaries and Notation}\label{section:prelim}

%% \subsection{Overview}
In this section I will present notations that will be used throughout
this article. These notations are summarized in Section
\ref{subsec:notation}. The theoretical foundations for inverse
problems over function spaces can be found in \cite{Stuart10}.


\subsection{Bayesian Linear Inverse Problems}\label{subsec:abstract OED}
Let $\hilp$ and $\hilo$ be separable Hilbert spaces (the subscripts p
and o are for ``parameter'' and ``observation'', respectively). Denote
$\hilo^* \simeq \hilo$ the Hilbert space of all linear functionals on
$\hilo$. Let $\fwd: \hilp \to \hilo$ the \emph{forward operator}. The
forward operator $\fwd$ is assumed linear and strongly smoothing (has
fast decaying modes --- the heat operator of the example in Section
\ref{section:intro} is a prime example). Take a Gaussian prior $\param
\sim \pr = \normal(\prmean ,\prcov)$ with some appropriate covariance
operator $\prcov$ on $\hilp$ \cite{Stuart10}. Note that $\fwd \prcov
\fwd^*$ is the prior covariance in $\hilo$ \cite{Stuart10}. As such,
$\fwd \prcov \fwd^*$ is assumed invertible --- an assumption which
will be used below. Let $m$ the number of measurements
taken. Measurements are taken via the \emph{measurement
operator}\footnote{It is common for the measurement and forward
operators to be merged $\tmp := \obs \fwd$
\cite{AlexanderianGloorGhattas14}, but the analysis carried out in the
following sections requires $\fwd$ and $\obs$ are explicitly separated
as in \cite{attia2022stochastic, cvetkovic2023choosing}.} $\obs \in (
\hilo^* )^m$, whose entries $\meas_j, j=1,\dots,m$ are called
\emph{measurement vector}s:

\begin{equation*}%\label{eq:O}
  \obs u = (\meas_1(u), \dots, \meas_m(u) )^t \in \R^m,\ u \in \hilo.
\end{equation*}
%
%

%% For $u \in \hilo$ and $v\in \R^m$:
%% %
%% %
%% \begin{align*}
%%   \big (\obs^*v \big ) (u) &= \langle v, \obs u \rangle_{\R^m} = \sum_{j=1}^m  v_j \meas_j(u)
%%   = v^t \left ( \obs u \right ) = (v^t \obs) (u),
%% \end{align*}
%% and thus:
%% \begin{align}\label{eq:obs*}
%%   \obs^*v &= \sum_{j=1}^m v_j \meas_j = v^t \obs.
%% \end{align}

Data is acquired via noisy observations, and we consider two types of
error terms: Spatially correlated model error $\eps' \sim
\normal(0,\modcov)$ with $\modcov: \hilo \to \hilo$ a covariance
operator. Observation error is denoted $\eps \sim \normal(0, \sigma^2
I_m)$, with $I_m \in \mathbb{R}^{m \times m}$ the identity. Both error
terms and the prior are assumed independent of each other. Thus, data
is acquired via
\begin{align}\label{eq:inverse problem}
  \data := \obs (\fwd \param + \eps') + \eps = \obs \fwd \param + \obs \eps' + \eps.
\end{align}
%% \subsubsection{Error Terms}
It is easy to verify that $\obs \eps' + \eps \in \R^m$ is a centered
Gaussian random vector with covariance matrix

\begin{align}\label{eq:Sigma}
  \begin{split}
    \Sigma(\obs) :&= \mathbb{E}[ (\obs \eps' + \eps) (\obs \eps' +
      \eps)^t ]
    % 
    % 
    = \obs \modcov \obs^* + \sigma^2I_m , 
  \end{split}
\end{align}
where
\begin{align}\label{eq:modcov explained}
  \begin{split}
    [\obs \modcov \obs^*]_{ij} & = e_i^t \obs \modcov \obs^* e_j 
    %
    %
    %
    = \meas_i (\modcov \meas_j).% \text{ (by \eqref{eq:obs*})}.
  \end{split}
\end{align}
%% and $e_j \in \R^m$ is the $j$th standard basis vector.
%% The explicit dependence on $\obs$ will be mostly dropped for
%% convenience, so $\Sigma(\obs) = \Sigma$.  Thus, for a fixed $\obs$
%% (i.e.\ a fixed set of observations), $\data = \obs \fwd \param +
%% \bar{\eps}$ with $\bar{\eps} \sim \normal(0,\Sigma)$.
Taking $\modcov = 0$ is a common practice
\cite{Tarantola05,Kaipio2006,Vogel02} and then $\Sigma = \sigma^2I_m$
is a scalar matrix and does not depend on $\obs$.

%% Taking an error model with a non-scalar covariance allows us to
%% consider model error (modeled by $\modcov$) as well as observation
%% error (modeled by $\sigma^2$). For example, say we believe our
%% forward model does not capture some small scale phenomenon.  Then
%% we may express this belief by saying $\tru = \fwd + \err$, with
%% $\fwd$ depending on $\param$ and $\err$ depending on $\sspar$, with
%% $\param \perp \sspar$. We do not know much about this effect but it
%% is reasonable to assume it changes continuously in our domain. We
%% (may choose to) model it as $\normal (0, \modcov)$ and take
%% $\modcov$ to reflect the spatial (or other) variability we imagine
%% $\err \sspar$ has. Such small scale phenomenon can arise as a
%% modeling issue, where we might not model the system in its
%% entirety. It can also arise from a numerical source, where our
%% discretization of the system is not fine enough to capture all
%% small scale phenomena.  In section \ref{section:non vanishing}, we
%% will see that assuming some correlation in the error mitigates the
%% clusterization phenomenon, as reported in the literature
%% \cite{Ucinski05}.



%% \subsubsection{Dynamics and Observation
%% Operators}\label{subsec:dynamics} Rows of $\obs$ are linear
%% functionals and we refer to these as {\it observation
%% vector}s. Note that we cannot choose every observation vector we
%% want. For example, we may be restricted only to pointwise
%% evaluations of $\fwd \param$ by the sensors at our disposal, in
%% which case measuring, e.g., the mean $\ell(u) = \int_{\Omega}u$ of
%% a function is not possible. Thus, it is best to think of $(\obs
%% \fwd \param)_j$ as approximations to pointwise evaluations. For
%% example, if $\hilo = L^2([-\pi, \pi])$, consider $\ell_1(x) :=
%% (2\pi \eps)^{-\frac{1}{2}}\exp(-\frac{x^2}{2\eps^2})$ and
%% $\ell_2(x) :=\eps^{-1}\mathbf{1}_{[-\frac{\eps}{2},
%% \frac{\eps}{2}]}$. Both $\ell_1$ and $\ell_2$ approximate a point
%% measurement at $0$ for small $\eps$ and, crucially, $\ell_1, \ell_2
%% \in \hilo^*$.  %% It is important to note that although $\|\ell\|_1
%% = %% \|\ell_2\|_1=1$, unfortunately $\|\ell_1\|_2^2 = (4\pi %%
%% \eps)^{-\frac{1}{2}}$ and $\|\ell_2\|_2^2 = \eps^{-1}$.


The posterior measure $\post$ is Gaussian in this setting and its
covariance operator is:
\begin{align}\label{eq:postcov}
  \postcov = (\prcov^{-1} + \fwd^* \obs^* \Sigma^{-1} \obs \fwd
  )^{-1}.
\end{align}


\subsection{Bayesian D-Optimal Designs in Infinite Dimensions}\label{subsec:D optimal design} 
A Bayesian D-optimal design maximizes expected KL divergence between
posterior and prior measures, $\post$ and $\pr$. It is useful to first
recall the definition of KL divergence:% for an arbitrary prior
measure:
$$
D_{KL}(\post||\pr) = \int \log \frac{\der \post}{\der \pr}(\param) \der \post(\param).
$$

The study of D-optimal designs for Bayesian linear inverse problems in
infinite dimensions was pioneered by \cite{AlexanderianGloorGhattas14,
  alexanderian2018efficient}. The main result we will make use of is
summarized (in our notation) in Theorem \ref{thm:d optimality}\footnote{In
\cite{AlexanderianGloorGhattas14, alexanderian2018efficient}, results
are stated for $\Sigma=I$ (implied by $\modcov = 0,\sigma^2=1$), but
the authors also mention these results hold for more general
covariance matrices \cite[p. 681]{AlexanderianGloorGhattas14}.}:


%% , and just like in finite dimensions, a D-optimal design maximizes
%% the Kullback-Leibler divergence between posterior and prior
%% measures For a linear inverse problem with Gaussian prior and
%% Gaussian noise, a D-optimal design minimizes an expression
%% analogous to the determinant of the posterior covariance operator
%% \cite{AlexanderianGloorGhattas14} (also \cref{thm:d optimality}
%% below).  D-optimal designs in infinite-dimensional Hilbert spaces
%% were investigated in \cite{AlexanderianGloorGhattas14}.
\begin{theorem}[Slightly modified Theorem 1 from \cite{AlexanderianGloorGhattas14}]\label{thm:d optimality}
  Let $\pr = \normal(\prmean,\prcov)$ be a Gaussian prior on $\hilp$
  and let $\post = \normal(\postmean,\postcov)$ the posterior measure
  on $\hilp$ for the Bayesian linear inverse problem $\data = \obs
  \fwd\param + \obs \eps' + \eps$ discussed above. Then
  \begin{align}\label{eq:objective}
    \begin{split}
      \tar( \obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
      % 
      % 
      % 
      &= \frac12 \log \det 
      ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}).
    \end{split}
  \end{align}
\end{theorem}
\begin{definition}\label{def:d optimality}
  $\obs^{\star}$ is said to be \emph{D-optimal} if $\obs^{\star} =
  \argmax_{\obs} \tar(\obs)$, where entries of $\obs \in (\hilo^*)^m$ are constrained to
  some allowed set of measurement vectors in $\hilo^*$.
\end{definition}

For a Bayesian linear model in finite dimensions, with Gaussian prior
and Gaussian noise, a D-optimal design minimizes the determinant of
the posterior covariance matrix\footnote{The frequentist version of
the D-optimality criterion aims to minimize the determinant of the
Fisher information matrix \cite[page 16]{Ucinski05}. Thus, the
Bayesian criterion is just a regularized version of the frequentist
criterion \cite{Chaloner1995}.}. Theorem \ref{thm:d optimality} and
Definition \ref{def:d optimality} have a similar intuition:
\begin{align*}
  \begin{split}
    \tar(\obs) &= \frac12 \log \det ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}) \text{ by \eqref{eq:objective}}\\
    &= \frac12 \log \det \Big( \prcov ( \prcov^{-1} + \fwd ^* \obs^* \Sigma^{-1} \obs \fwd) \Big )\\
    &= \frac12 \log \det \prcov \postcov^{-1} \text{ by \eqref{eq:postcov}} \\
    &= \frac12 \log \det \prcov -\frac12 \log \det \postcov.
  \end{split}
\end{align*}
Since $\prcov$ is constant, a D-optimal design minimizes the posterior
covariance determinant, analogously to the finite-dimensional case.


\subsection{Notation Summary}\label{subsec:notation}
  Our notation is summarized below. We let:
  \begin{itemize}
  \item $\hilp, \hilo$ Hilbert spaces.
  \item $\fwd:\hilp \to \hilo$ a linear compact operator.
  \item $\pr \sim \mathcal{N}(0, \prcov)$ prior Gaussian measure on
    $\hilp$, with prior covariance operator $\prcov:\hilp \to \hilp$.
  \item $\obs: \hilo \to \mathbb{R}^m$ measurement operator, where $m
    \in \mathbb{N}$ is the number of measurements taken.
  \item $\sigma^2 \in \mathbb{R}_{+}$ observation noise variance.
  \item $\modcov$ model error covariance operator.
    %% $\data = \obs \fwd \param + \eps$, where $\eps \in
    %% \mathbb{R}^m$ isiid $\mathcal{N}(0, \sigma^2)$ noise.
  \item $\Sigma(\obs) = \obs \modcov \obs^* + \sigma^2I$. 
  \item $\post$ the posterior measure, with covariance $\postcov$.
  \item A D-optimality design criterion
    \cite{AlexanderianGloorGhattas14}:
    \begin{align*}
      \begin{split}
        \tar(\obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det ( I + \prcov^{1/2} \fwd ^* \obs^* \Sigma(\obs)^{-1} \obs
        \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  %% \item $\{\lambda_i\}_{i=1}^\infty$ eigenvalues of $\fwd\prcov\fwd^*$
  %%   in decreasing order of magnitude.
  %% %% \item $\{\ev_i\}_{i=1}^\infty$ their corresponding eigenvectors.
  %% \item $\{\eta_i\}_{i=1}^\infty$ eigenvalues of $\obs^*\obs$.
  \end{itemize}


%% \subsection{Sequential vs Simultaneous Optimization}\label{subsec:seq vs sim}
%% From defintion \ref{def:d optimality} we wish to characterize solution(s) of the
%% following optimization problem for $\tar$. %%: (\hilo^*)^m \to \R$:
%% \begin{align}\label{eq:optimization}
%%   \obs^{\star} := \argmax_{\obs} \tar( \obs ) 
%%   = \argmax_{\obs} \frac12 \log \det 
%%   (I + \prcov^{1/2} \fwd^*\obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}),
%% \end{align}
%% where $\obs$ is constrained to some allowed set of observations. We
%% call this problem ``simultaneous optimization'', since all
%% observations are decided on simulatneously.

%% For computational reasons, one may prefer to find the best
%% observations in a sequential manner. Denote
%% \begin{equation}\label{eq:def obs_k}
%%   \obs_k := (\meas_1,\dots, \meas_k)^t,  k\leq m.
%% \end{equation}
%% Sequential optimal design proceeds as follows. Find $\meas_1$ by
%% maximizing $\tar(\obs_1)$. Then, keeping $\meas_1$ fixed --- find
%% $\meas_2$ as the maximizer of $\tar(\obs_2)$. Then, find $\meas_3$ by
%% keeping $\meas_1,\meas_2$ fixed and taking $\meas_3$ as the maximizer
%% of $\tar(\obs_3)$. Continue this way until $\obs_m = \obs$ is
%% found, where $m$ is the number of available observations. %% It is
%% %% important to notice that this scheme does not require actually
%% %% observing data --- in \eqref{eq:objective} data is averaged out.

%% The analysis in this paper is conducted for the general simultaneous
%% optimization case. The sequential optimization case is dealt with in
%% section \ref{subsec:clusterization sequential}. It is important to
%% note, however that all conclusions we arrive at for the simultaneous
%% case easily specialize to the sequential case by considering the
%% posterior as the next sequential step's prior.

