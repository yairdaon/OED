\documentclass{beamer}
\input{definitions}

\usepackage{multimedia}

\setbeamertemplate{footline}[frame number]

\title{Sensor Clusterization in Bayesian D-optimal Designs}
\author{Yair Daon}
\institute{Azrieli Faculty of Medicine, Bar-Ilan University}
\date{}

\begin{document}

%% Props: ball for toss, graduated lab cylinders, glass of water, colored water (vitaminchik), pippette for water drops
%% jug for excess water

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
\frametitle{Forward problem}
% Example 1: Ball toss
% Example 2: Flow field (placeholder for image)
% Example 3: Ink in water

\[
\fwd: \hil \to \hil
\]

\begin{figure}
  \centering
  %\includegraphics[width=10cm,height=6cm]{forward_heat_equation.png}
\end{figure}
\end{frame}



\begin{frame}
\frametitle{1D Heat Equation}
\begin{subequations}
  \begin{align*}
    u_t &= \Delta u &&\qquad \text{in } [0,1] \times [0,\infty),\\
      u &= 0 &&\qquad \text{on } \{0, 1\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,1] \times \{0\}.
  \end{align*}
\end{subequations}

\begin{figure}
  \centering
  \includegraphics[width=10cm,height=6cm]{heat_rod.png} 
\end{figure}
\end{frame}


\begin{frame}
  \begin{figure}
%    \centering
    \href{run:forward_heat_equation.mp4}{\includegraphics[width=12cm,height=8cm]{forward_heat_equation.png}}    
    
    %% \includemovie[inline=false,text={\includegraphics[scale=0.25]{forward_heat_equation.png}}]{}{}{forward_heat_equation.mp4}
    %% \flashmovie[width=12cm,height=8cm,engine=flv-player,auto=1]{forward_heat_equation.mp4}
    %% \movie[width=10cm,height=6cm,poster,autostart,loop]{
    %%   %\includegraphics[width=10cm,height=6cm]{forward_heat_equation.png}
    %% }{forward_heat_equation.mp4}
  \end{figure}
\end{frame}


\begin{frame}
\frametitle{Inverse Problems}
% Explain the inverse problems for the ball toss, flow field examples
% Argue that there are many valid solutions
% Explain that such problems are ill-posed

\begin{minipage}{0.5\textwidth}
  \begin{itemize}
    \item<1-> Separable Hilbert space $\hil$.
    \item<2-> \(\fwd: \hil \to \hil\).
    \item<3-> \(\meas_j \in \hil^*, j=1,\dots,m \).
    \item<4-> \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t\).
    \item<5-> \(\data := \obs \fwd \param + \eps\).% \text{ where } \eps \sim \mathcal{N}(0,\sigma^2 I_m)\)
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth]{CT.png}\\
    \vspace{0.5cm}
    \includegraphics[width=0.99\textwidth]{borehole.png}
\end{minipage}
% Show "sensor" and discuss measurement placement

% Present the entire formalism of inverse problems from the paper, for zero model error
\end{frame}


\begin{frame}
  \frametitle{Ill Posedness}
  %% \begin{figure}
    \href{run:reverse_heat_equation.mp4}{\includegraphics[width=12cm,height=8cm]{reverse_heat_equation.png}}    
  %%   \movie[width=10cm,height=6cm,poster,autostart,loop]{
  %%     \includegraphics[width=10cm,height=6cm]{reverse_heat_equation.png}
  %% }{reverse_heat_equation.mp4}
  %% \end{figure}
\end{frame}


\begin{frame}
\frametitle{Bayesian Inverse Problems (Stuart 2010)}
\begin{itemize}
\item Separable Hilbert space $\hil$.
\item \(\fwd: \hil \to \hil\). 
\item \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t \in \R^m\).
\item \(\data := \obs \fwd \param + \eps \text{ where } \eps \sim \mathcal{N}(0,I_m)\)
\item \(\pr \sim \mathcal{N}(\prmean, \prcov)\).
\item \(\post \sim \mathcal{N}(\postmean, \postcov)\).
\end{itemize}

% Introduce prior and posterior measurea 
\end{frame}

\begin{frame}
\frametitle{Gaussian measures on Separable Hilbert spaces}
\begin{figure}
   \includegraphics[width=10cm,height=6cm]{MultivariateNormal.png}
\end{figure}

\begin{align*}
  \postcov^{-1} = \prcov^{-1} + \fwd^* \obs^*\obs \fwd
\end{align*}

% Define Gaussian measure
% Show a 2D example with iso-density lines and projections
% Introduce notation for mean $\mathbf{m}$ and covariance $\Gamma$
\end{frame}

% Slide 8: Recap and notation summary
\begin{frame}
\frametitle{Recap and notation summary}

\begin{itemize}
\item Separable Hilbert space \(\hil\).
\item Forward operator \(\fwd: \hil \to \hil\).
%\item Observation operator \(\obs: \hil \to \mathbb{R}^m\).
\item Observation operator \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t \in \R^m,\ u \in \hil\).
\item Data \(\data := \obs \fwd \param + \eps,\ \eps \sim \mathcal{N}(0,I_m)\).
\item Prior \(\pr\) with covariance \(\prcov\).
\item Posterior \(\post\) with covariance \(\postcov^{-1} = \prcov^{-1} + \fwd^* \obs^*\obs \fwd\)
\end{itemize}
%% 1D heat equation
\end{frame}

\begin{frame}
 
\end{frame}





\begin{frame}
 
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{figure}
        \includegraphics[width=\linewidth]{borehole.png}
      \end{figure}
      \begin{figure}
        \includegraphics[width=\linewidth]{heat_rod.png}
      \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}
        \includegraphics[width=\linewidth]{CT.png}
      \end{figure}
      How do we choose $\obs$?
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}
  D-optimal \(\obs\) maximizes
  \begin{equation*}
    \onslide<3->{\mathbb{E}_{\data} \Big [}D_{\text{KL}} (\post || \pr ) \onslide<3->{\Big ]}
    \onslide<2->{=} \onslide<3->{\mathbb{E}_{\data} \Big [}\onslide<2->{\int \log \frac{\der \post}{\der \pr}(\param) \der \post(\param)}  \onslide<3->{\Big ]}
  \end{equation*}



  %% \begin{equation*}
  %%   \onslide<1->{\phantom{\mathbb{E_{\data}}}}D_{\text{KL}} (\post || \pr ) = \onslide<1->{\phantom{\mathbb{E_{\data}}}} \int \log \frac{\der \post}{\der \pr}(\param) \der \post(\param)
  %%   \onslide<2->{\mathbb{E_{\data}}}
  %% \end{equation*}
  %% \vspace{-2.5em} % Adjust this value if necessary to align the second line correctly
  %% \begin{equation*}
  %%   \onslide<2->{\mathbb{E_{\data}}}
  %% \end{eion*}

  
\end{frame}

% Slide 10: D-optimality criterion
\begin{frame}
  \frametitle{D-optimality criterion}
  \begin{theorem}[Alexanderian, Gloor, Ghattas (2014)]\label{thm:d optimality}
    %% Let \(\pr = \normal(\prmean,\prcov)\) a Gaussian prior on \(\hil\)
    %% and let \(\post = \normal(\postmean,\postcov)\) the posterior.
    %% measure on \(\hil\) for the Bayesian linear inverse problem
    %% \(\data = \obs \fwd\param + \eps\).
    %% Then
    \begin{align*}
      \begin{split}
        \tar( \obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det 
        ( I +  \prcov^{1/2}  \fwd ^* \obs^* \obs \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  \end{theorem}

  \pause
  \begin{definition}
  \(\opt\) is \emph{D-optimal} if \(\opt = \argmax_{\obs}
  \tar(\obs)\).
  \end{definition}


% Show the D-optimality criterion from Alexanderian et al.
% Assume linear forward operator and linear observation operator
\end{frame}

\begin{frame}
  \frametitle{Intuition}
  \begin{align*}
  \begin{split}
    \tar(\obs) &= \frac12 \log \det ( I + \prcov^{1/2}  \fwd ^* \obs^* \obs \fwd \prcov^{1/2})\\ 
    &= \frac12 \log \det \Big( \prcov ( \prcov^{-1} +  \fwd ^* \obs^* \obs \fwd) \Big )\\
    &= \frac12 \log \det \prcov \postcov^{-1}.
    %%  &= \frac12 \log \det \prcov -\frac12 \log \det \postcov.
  \end{split}
  \end{align*}

  \pause D-optimal designs minimize $\det \postcov$.
\end{frame}


%% \begin{frame}
%%   Recall \(\tar( \obs) = \frac12 \log \det ( I + 
%%   \prcov^{1/2} \fwd ^* \obs^* \obs \fwd \prcov^{1/2})\).

%%   \begin{definition}
%%   \(\obs^{\star}\) is \emph{D-optimal} if \(\obs^{\star} =
%%   \argmax_{\obs} \tar(\obs)\).
%%   \end{definition}
  
%%   %% , where entries of \(\obs \in (\hil^*)^m\)
%%   %% are constrained to some allowed set of measurements in \(\hil^*\). 

%%   %% \(\obs\) must be constrained somehow! E.g.~norm has to be bounded.
%% \end{frame}


\begin{frame}
\frametitle{Problem: Sensor clusterization}
\begin{figure}
  \centering
  \includegraphics[height=0.5\textwidth]{dst_modelError0.png}
\end{figure}

% Discuss sensor clusterization in the 1D heat equation example
% Show figure from the paper (placeholder for figure)
\end{frame}

\begin{frame}
  \frametitle{Avoiding clusterization}

  \begin{columns}
    \begin{column}{0.6\textwidth}
      \begin{itemize}
      \item<1> Merging close measurements (Fedorov 1997).
      \item<2> Distance constraints, correlated errors (Ucinski 2005).
      \item<3> A finite set of locations (Alexanderian et al. 2020).
      \end{itemize}
    \end{column}
    \begin{column}{0.55\textwidth}
      \only<1>{\includegraphics[width=\textwidth]{merge.png}}
      \only<2>{\includegraphics[width=\textwidth]{constraints.png}}
      \only<3>{\includegraphics[width=\textwidth]{stadler_OED.png}}
    \end{column}
  \end{columns}  
\end{frame}



\begin{frame}
\frametitle{Questions}
\begin{itemize}
\item Why does imposing correlations between observations alleviate
measurement clusterization?
%
\item Is measurement clusterization a generic phenomenon?
%
\item And, most importantly: Why does measurement clusterization occur?
% How do we tackle this problem?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A generic formulation for D-optimality}

\begin{itemize}
\item Separable Hilbert space \(\hil\).
\item Forward operator \(\fwd: \hil \to \hil\).
\item Observations \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t \in \R^m,\).
\item Prior and posterior covariance \( \prcov, \postcov\).
\item Data \(\data := \obs \fwd \param + \eps\).
\item \(\tar( \obs) = \frac12 \log \det ( I + 
  \prcov^{1/2} \fwd ^* \obs^* \obs \fwd \prcov^{1/2})\).
\item \(\obs^{\star}\) is \emph{D-optimal} if \(\obs^{\star} =
  \argmax_{\obs} \tar(\obs)\).
\end{itemize}

\(\obs\) must be constrained somehow: \(\|\meas_j\| \leq 1\).

%% \begin{align}\label{eq:objective}
%%   \begin{split}
%%     \tar( \obs) &= \frac12 \log \det 
%%     ( I + \prcov^{1/2}  \fwd ^* \obs^* \obs \fwd \prcov^{1/2}).
%%   \end{split}
%% \end{align}

% Discuss the main obstacle: the measurement operator
% Impose unit-norm constraints
% Explain that sensors have a unit inf-norm (show example)
% Show relevant equation from the paper
\end{frame}

% Slide 15: Constrained optimization problem
\begin{frame}
\frametitle{Constrained optimization problem}
% Restate the optimization criterion from Alexandrian et al.'s theorem
% Restate the unit norm constraints
% Pose the problem as a constrained optimization problem
% Ask the audience how to solve constrained optimization problems (Lagrange multipliers)
\begin{align*}
  \begin{split}
  \opt &= \argmax\tar(\obs)\\
  \text{subject to }& \|\meas_j\| = 1 \text{ for } j=1,\dots,m.
  \end{split}
\end{align*}

Where
\begin{align*}
  \begin{split}
    \tar(\obs) %:&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
    % 
    % 
    % 
    &= \frac12 \log \det ( I + \prcov^{1/2} \fwd ^* \opt^* \opt \fwd
    \prcov^{1/2}).
  \end{split}
\end{align*}
\end{frame}

\begin{frame}
%% \frametitle{Necessary conditions for optimality}
% Present the nonlinear eigenvalue problem for the vanishing model error case
% Explain why it is called a "nonlinear" eigenvalue problem
  
  \begin{theorem}[D.]
    Let:
    \begin{align*}
      \begin{split}
        \opt &= \argmax\tar(\obs)\\
        \text{subject to }& \|\meas_j\| = 1 \text{ for } j=1,\dots,m.
      \end{split}
  \end{align*}
    
    Then:
    \[
    \fwd \postcov \fwd^* \opt^* = \opt^* \Xi
    \]
  
    %% \begin{equation*}
    %%    \fwd \postcov \fwd^* \obs^* = \obs^* \Xi, 
    %% \end{equation*}
    where \(\Xi \in \mathbb{R}^{m \times m}\) is diagonal.
  \end{theorem}

\end{frame}


\begin{frame}
%% \frametitle{Implication of first-order conditions}
 
  \begin{theorem}[D.]
    \begin{gather*}
      \fwd\prcov\fwd^* \text{ and } \opt^* \opt \text{ are simultaneously diagonalizable}.\\
      \Uparrow \\
      \onslide<3->{\left( (\fwd\prcov\fwd^*)^{-1} + \opt^* \opt \right)^{-1} \opt^* = \opt^* \Xi \\
        \Uparrow}  \\
      \onslide<2->{\fwd  \left(\prcov^{-1} + \fwd^* \opt^*\opt \fwd \right)^{-1} \fwd^* \opt^* = \opt^* \Xi  \\
        \Uparrow}  \\
      \fwd \postcov \fwd^* \opt^* = \opt^* \Xi  \\
    \end{gather*}
  \end{theorem}
  \uncover<4->{Proof idea: Consider invariant space of each eigenvalue, use spectral decomposition.}
\end{frame}


%% \begin{frame}
%%   %\frametitle{Simultaneous diagonizability}
%%   \begin{lemma}
%%     %% Let \(\hil\) separable ilbert space,
%%     Let \(C:\hil \to \hil\) with orthogonal eigenbasis.
%%     %% and \(\func_1,\dots,\func_m \in \hil\). Denote \(\func^*\) the element
%%     %% \(\func\) acting as a linear functional. If
%%   \begin{equation*}
%%     (C + \sum_{j=1}^m \func_j\func_j^*) \func_l = \xi_l \func_l,\ l = 1, \dots, m.
%%   \end{equation*}
%%   $\Rightarrow$ \(C\) and \(\sum_{j=1}^m \func_j \func_j^*\) are simultaneously
%%   diagonalizable.
%%   \end{lemma}
%% Proof idea: Consider invariant space of each eigenvalue, use spectral decomposition.
%% \end{frame}


\begin{frame}
  \begin{theorem}[D.]
    Let:
    \begin{itemize}
      \item \(\opt\) a D-optimal design.
      \item \(\{\eta_i\}_{i=1}^{\infty}\) eigenvalues of \(\opt^*\opt\).
      \item \(\{\lambda_i\}_{i=1}^{\infty}\) corresponding eigenvalues of \(\fwd \prcov \fwd^*\).
    \end{itemize}
    Then \(\eta_i > 0\) for eigenvectors corresponding to largest \(\lambda_i\).
  \end{theorem}
  
  
  \uncover<2->{How many? \(\rank \opt^*\opt\).}

  \uncover<3->{Why?
    \begin{align*}
      \det(I + \prcov^{1/2}  \fwd ^* \opt^* \opt \fwd \prcov^{1/2}) \sim \prod (1+\lambda_i\eta_i)
    \end{align*}
  }
  %% INTRODUCE CYLINDERS!!!!
\end{frame}


\pgfplotstableread{
  Label     prior  optimal  sub-optimal  worst
  1         0.2    1.8           1.7      1.7
  2         0.8    1.2           0.8      0.0
  3         2.2    0             0.5      0.5
  4         3.5    0             0.0      0.0
}\optimalvsnot

\begin{frame}
  \begin{figure}
    \centering
    \begin{tikzpicture}[scale=0.6]
      \begin{axis}[
          ybar stacked,
          ymin=0,
          ymax=4,
          xtick=data,
          legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
          reverse legend=false, % set to false to get correct display, but I'd like to have this true
          xticklabels from table={\optimalvsnot}{Label},
          xticklabel style={text width=2cm,align=center},
          legend plot pos=right,
          ylabel={\LARGE precision --- prior and posterior},
          xlabel={\LARGE eigenvector},
        ]
        \addplot [fill=blue!60]  table [y=prior,   meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addplot [pattern=north east lines, pattern color=green!80]  table [y=optimal, meta=Label, x expr=\coordindex] {\optimalvsnot};     
        \addlegendentry[scale=1.4]{$\lambda_i^{-1}$}
        \addlegendentry[scale=1.4]{optimal $\eta_i$s}
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[scale=0.6]
      \begin{axis}[
          ybar stacked,
          ymin=0,
          ymax=4,
          xtick=data,
          legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
          reverse legend=false, % set to false to get correct display, but I'd like to have this true
          xticklabels from table={\optimalvsnot}{Label},
          xticklabel style={text width=2cm,align=center},
          legend plot pos=right,
          ylabel={\LARGE precision --- prior and posterior},
          xlabel={\LARGE eigenvector} ,
        ]   
        \addplot [fill=blue!60]  table [y=prior,       meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addplot [pattern=north east lines, pattern color=green!80]  table [y=worst, meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addlegendentry[scale=1.4]{$\lambda_i^{-1}$}
        \addlegendentry[scale=1.4]{sub-optimal $\eta_i$s}
      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}


%% \begin{frame}
%% %% \begin{proposition}
%%   Let \(\tar: \mathbb{R}^m \to \mathbb{R}\), \(\tar(\eta) =
%%   \frac{1}{2}\sum_{i=1}^m \log (1+\lambda_i \eta_i)\), with
%%   \(\lambda_i > 0\). Then the maximum of \(\tar\)
%%   subject to \(\eta_i \geq 0\) and \(\sum\eta_i = m\) is obtained at
%%   \begin{equation}
%%   \eta_i = \begin{cases}
%%     \frac{m}{k} -  \lambda_i^{-1} +  \frac{1}{k} \sum_{j\in A} \lambda_j^{-1} & i \in A \\
%%     0 & i \in A^c
%%   \end{cases}
%%   \end{equation}
%%   where \(A:= \{1\leq i \leq m: \eta_i > 0\}\) and \(A^c = \{1,\dots, m\}
%%   \backslash A\), and \(k = |A|\), the cardinality of \(A\).
%% %% \end{proposition}
%% \end{frame}


\begin{frame}
  \begin{theorem}[D.]
    The covariance of the pushforwad \(\fwd_{*} \post\)
    %is \(\left ( (\fwd \prcov \fwd^*)^{-1} +  \obs^*\obs
    %\right )^{-1}\) and its
    has eigenvalues 
    \begin{equation*}
      \theta_i =
      \begin{cases}
        \left(\frac{\sum_{j=1}^k \lambda_j^{-1} + m}{k} \right )^{-1} & i \leq k \\
        \lambda_i &  i > k 
      \end{cases}
    \end{equation*}
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{figure}
    \centering
    \begin{tikzpicture}[scale=0.6]
      \begin{axis}[
          ybar stacked,
          ymin=0,
          ymax=4,
          xtick=data,
          legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
          reverse legend=false, % set to false to get correct display, but I'd like to have this true
          xticklabels from table={\optimalvsnot}{Label},
          xticklabel style={text width=2cm,align=center},
          legend plot pos=right,
          ylabel={\LARGE precision --- prior and posterior},
          xlabel={\LARGE eigenvector},
        ]
        \addplot [fill=blue!60]  table [y=prior,   meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addplot [pattern=north east lines, pattern color=green!80]  table [y=optimal, meta=Label, x expr=\coordindex] {\optimalvsnot};     
        \addlegendentry[scale=1.4]{$\lambda_i^{-1}$}
        \addlegendentry[scale=1.4]{optimal $\eta_i$s}
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[scale=0.6]
      \begin{axis}[
          ybar stacked,
          ymin=0,
          ymax=4,
          xtick=data,
          legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
          reverse legend=false, % set to false to get correct display, but I'd like to have this true
          xticklabels from table={\optimalvsnot}{Label},
          xticklabel style={text width=2cm,align=center},
          legend plot pos=right,
          ylabel={\LARGE precision --- prior and posterior},
          xlabel={\LARGE eigenvector} ,
        ]   
        \addplot [fill=blue!60]  table [y=prior,       meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addplot [pattern=north east lines, pattern color=green!80]  table [y=sub-optimal, meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addlegendentry[scale=1.4]{$\lambda_i^{-1}$}
        \addlegendentry[scale=1.4]{sub-optimal $\eta_i$s}
      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Four Sensors in the 1D Heat Equation}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{eigenvectors_dst.png}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Summary}
  \begin{itemize}
  \item Study a generic inverse problem with unit ball constraints.
  \item D-optimality implies \(\fwd \postcov\fwd^* \obs^* = \obs^* \Xi\).
  \item Key: $\obs^*\obs$ and \(\fwd \prcov\fwd^* \obs^*\) are
    simultaneously diagonalizable.
  \item A D-optimal design concentrates on smallest prior covariance eigenvectors...
  \item ... and a D-optimal design reduces uncertainty uniformly on these.
  \item Saw manifestation in the 1D heat equation.
  \item Clusterization is just a result of avoiding higher eigenvectors
    and pigeonhole principle.
  \item Did not review: correlated errors, genericity of clusterization.
  \end{itemize}
\end{frame}
\end{document}
