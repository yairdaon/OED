\documentclass{beamer}
\input{definitions}
\usepackage{setspace}


\usefonttheme{professionalfonts} % using non standard fonts for beamer
\usefonttheme{serif} % default family is serif
%% \usepackage{fontspec}
%% \setmainfont{Liberation Serif}
\usepackage{multimedia}
\setbeamertemplate{footline}[frame number]
\usepgfplotslibrary{fillbetween}


\title{Clusterization in D-optimal designs: the case against linearization}
\author{Yair Daon}
\institute{Azrieli Faculty of Medicine, Bar-Ilan University}
\date{}

\begin{document}

%% Props: ball for toss, graduated lab cylinders, glass of water, colored water (vitaminchik), pippette for water drops
%% jug for excess water

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}
  \frametitle{Inverse Problems}
  % Explain the inverse problems for the ball toss, flow field examples
  % Argue that there are many valid solutions
  % Explain that such problems are ill-posed

  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/inverse_problem.png}
  \end{figure}

  {\hfill \tiny Neumayer, Suppan and Bretterklieber (2019)}
  %Paper: Statistical solution of inverse problems using a state
  %reduction. Image of electrical capacitance tomography (ECT).
\end{frame}


\begin{frame}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \begin{itemize}
      \item<3-> Separable Hilbert space $\hil$.
      \item<4-> Linear \(\fwd: \hil \to \hil\).
      \item<5-> \(\meas_j \in \hil^*, j=1,\dots,m \).
      %\item<6-> \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t\).
      \item<6-> \(\data_j := \meas_j \fwd \param + \eps\).% \text{ where } \eps \sim \mathcal{N}(0,\sigma^2 I_m)\)
      \end{itemize}
  
      \begin{figure}
        \centering
        \only<2->{\includegraphics[width=1.3\textwidth, height=0.76\textwidth]{figs/heat_rod.png}}
      \end{figure}
    \end{column}

    \begin{column}{0.5\textwidth}

      \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{figs/CT.png}
      \end{figure}

      \begin{figure}
        \centering
        \includegraphics[width=\linewidth, height=4cm]{figs/borehole.png}
      \end{figure}
      
    \end{column}
  \end{columns}
  {\hfill \tiny Wikimedia Commons, Conroy and Guy 2005, radiologykey.com }
\end{frame}


%% \begin{frame}
%% \frametitle{Forward Problems}
%% % Example 1: Ball toss
%% % Example 2: Flow field (placeholder for image)
%% % Example 3: Ink in water

%% \[
%% \fwd: \hil \to \hil
%% \]

%% \begin{figure}
%%   \centering
%%   %\includegraphics[width=10cm,height=6cm]{forward_heat_equation.png}
%% \end{figure}
%% \end{frame}




\begin{frame}
\frametitle{Heat Equation}
\begin{subequations}
  \begin{align*}
    u_t &= \Delta u &&\qquad \text{in } [0,l] \times [0,\infty),\\
    u &= 0 &&\qquad \text{on } \{0, l\} \times [0,\infty),\\
    u &= f &&\qquad \text{on }[0,l] \times \{0\}.
  \end{align*}
\end{subequations}

\begin{figure}
  \centering
  \includegraphics[width=8cm,height=5cm]{figs/heat_rod.png} 
\end{figure}

{\hfill \tiny Wikimedia Commons}
\end{frame}


\begin{frame}
  \begin{figure}
    \centering
    \href{run:figs/forward_heat_equation.mp4}{\includegraphics[width=12cm,height=8cm]{figs/forward_heat_equation.png}}    
    
    %% \includemovie[inline=false,text={\includegraphics[scale=0.25]{forward_heat_equation.png}}]{}{}{forward_heat_equation.mp4}
    %% \flashmovie[width=12cm,height=8cm,engine=flv-player,auto=1]{forward_heat_equation.mp4}
    %% \movie[width=10cm,height=6cm,poster,autostart,loop]{
    %%   %\includegraphics[width=10cm,height=6cm]{forward_heat_equation.png}
    %% }{forward_heat_equation.mp4}
  \end{figure}
\end{frame}


  
%% \begin{minipage}{0.5\textwidth}
%%   \begin{itemize}
%%     \item<1-> Separable Hilbert space $\hil$.
%%     \item<2-> \(\fwd: \hil \to \hil\).
%%     \item<3-> \(\meas_j \in \hil^*, j=1,\dots,m \).
%%     \item<4-> \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t\).
%%     \item<5-> \(\data := \obs \fwd \param + \eps\).% \text{ where } \eps \sim \mathcal{N}(0,\sigma^2 I_m)\)
%%   \end{itemize}
%% \end{minipage}%
%% \begin{minipage}[t]{0.5\textwidth}
%%     \centering
%%     \includegraphics[width=0.99\textwidth]{CT.png}\\
%%     \vspace{0.5cm}
%%     \includegraphics[width=0.99\textwidth]{borehole.png}
%% \end{minipage}

% Show "sensor" and discuss measurement placement

% Present the entire formalism of inverse problems from the paper, for zero model error
% \end{frame}


\begin{frame}
  \frametitle{Ill Posedness}
  \begin{figure}
    \centering
    \href{run:figs/reverse_heat_equation.mp4}{\includegraphics[width=12cm,height=8cm]{figs/reverse_heat_equation.png}}    
    %% \movie[width=10cm,height=6cm,poster,autostart,loop]{
    %% \includegraphics[width=10cm,height=6cm]{reverse_heat_equation.png}
    %% }{reverse_heat_equation.mp4}
  \end{figure}
\end{frame}

  
\begin{frame}
  \begin{itemize}
  \item<1-> Separable Hilbert space $\hil$.
  \item<2-> Linear \(\fwd: \hil \to \hil\).
  \item<3-> \(\meas_j \in \hil^*, j=1,\dots,m \).
  \item<4-> \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t\).
  \item<5-> \(\data := \obs \fwd \param + \eps\).% \text{ where } \eps \sim \mathcal{N}(0,\sigma^2 I_m)\)
  \item<6-> Goal: infer "parameter" $\param$ from data $\data$.
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Bayes' Theorem}
  \begin{center}
    \begin{tikzpicture}[scale=1.5]
      
      % Bayes' Theorem formula
      \node (theorem) at (0,0) {\huge $\Pr(\param|\data) \propto \Pr(\data|\param) \Pr(\param)$};
      
      % Likelihood arrow
      \draw[->, thick, blue] (2.5,1.5) -- (2,0.5) node[pos=0, above] {Prior};

      % Prior arrow
      \draw[->, thick, green] (0,1.5) -- (0.5,0.5) node[pos=0, above] {\only<1>{Likelihood}\only<2>{Inverse Problem}};

      % Posterior arrow
      \draw[->, thick, red] (-2.5,1.5) -- (-1.5,0.5) node[pos=0, above] {Posterior};

    \end{tikzpicture}
  \end{center}
\end{frame}



\begin{frame}
  \begin{center}
    \includegraphics[width=1\textwidth]{figs/bayes.png}
  \end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \includegraphics[width=\textwidth]{figs/bayesian_inverse_problems.jpg}
  \end{center}
  {\hfill \tiny Alexanderian (2021).}
\end{frame}

\begin{frame}
\frametitle{Bayesian Inverse Problems (Stuart 2010)}
\begin{itemize}
\item Separable Hilbert space $\hil$.
\item Linear \(\fwd: \hil \to \hil\). 
\item \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t \in \R^m\).
\item \(\data := \obs \fwd \param + \eps \text{ where } \eps \sim \mathcal{N}(0,I_m)\).
\item Prior $\pr$, posterior $\post$.
\end{itemize}

% Define Gaussian measure
% Show a 2D example with iso-density lines and projections
% Introduce notation for mean $\mathbf{m}$ and covariance $\Gamma$
\end{frame}

\begin{frame}
\frametitle{Gaussian measures via linear pushforward}
\begin{align*}
  \pr &\sim \mathcal{N}(\param, \prcov), \text{ $\prcov$ is trace-class}\\
  \onslide<2->{&\postcov^{-1} = \prcov^{-1} + \fwd^* \obs^*\obs \fwd} 
\end{align*}

\begin{figure}
   \includegraphics[width=10cm,height=6cm]{figs/gaussian.png}
\end{figure}

% Introduce prior and posterior measurea
%% Let v_1 = [1,1], v_2 = [1,-1], v_3 = [-1,-0.5]. Take random 2 by 2 positive definite matrix  and call it Gamma. Let m = [1,1.4]. 
%% Create a figure with two columns of height 6 and width 12. 

%% 1a. On the left half draw a heatmap of a gaussian N(m, Gamma) in greyscale.
%% 1b. Draw mu as a black dot and mark it \mathbf{m} (in black). 
%% 1c. Draw the vectors v_1, v_2, v_3 as red, green and blue arrows emanating from the origin.
%% 1d. Restrict this image to [-2,3] on both x and y axes.

%% 2. Let X ~ N(mu, Gamma). Partition the right column into three rows of equal height. 
%% 2a. In row 1 draw the density a gaussian N(v_1^t mu, v_1^t Gamma v_1) in red and at the top right write \mathcal{N}(v_1^t\mu, v_1^t \Gamma v_1).
%% 2b. In row 2 draw the density a gaussian N(v_2^t mu, v_2^t Gamma v_2) in green and at the top right write \mathcal{N}(v_2^t\mu, v_2^t \Gamma v_2).
%% 3c. In row 3 draw the density a gaussian N(v_3^t mu, v_3^t Gamma v_3) in blue and at the top right write \mathcal{N}(v_3^t\mu, v_3^t \Gamma v_3).
%% 2d. Make sure the scales on the x axes are the same and remove all ticks and labels from axes.

%% In this figure use fontsize 18


%% import numpy as np
%% import matplotlib.pyplot as plt
%% 
%% # Define parameters
%% v_1 = np.array([1, 1])
%% v_2 = np.array([1, -1])
%% v_3 = np.array([-1, -0.5])
%% m = np.array([1, 1.4])
%% 
%% # Generate a random 2x2 positive definite matrix Gamma
%% np.random.seed(42)
%% A = np.random.rand(2, 2)
%% Gamma = np.dot(A, A.T)  # Make it positive definite
%% 
%% # Define the grid for the heatmap
%% x = np.linspace(-2, 3, 500)
%% y = np.linspace(-2, 3, 500)
%% X, Y = np.meshgrid(x, y)
%% 
%% # Compute the Gaussian density
%% pos = np.dstack((X, Y))
%% inv_Gamma = np.linalg.inv(Gamma)
%% Z = np.exp(-0.5 * ((pos - m) @ inv_Gamma * (pos - m)).sum(axis=2)) / (2 * np.pi * np.sqrt(np.linalg.det(Gamma)))
%% 
%% # Compute parameters for Gaussian densities
%% gaussians = []
%% for v in [v_1, v_2, v_3]:
%%     mean = np.dot(v, m)
%%     variance = np.dot(v, np.dot(Gamma, v))
%%     gaussians.append((mean, variance))
%% 
%% # Determine the shared x-axis range for the densities
%% x_min = min(mean - 3 * np.sqrt(variance) for mean, variance in gaussians)
%% x_max = max(mean + 3 * np.sqrt(variance) for mean, variance in gaussians)
%% x_shared = np.linspace(x_min, x_max, 500)
%% 
%% # Create the figure
%% fig = plt.figure(figsize=(12, 6))
%% fontsize = 18
%% 
%% # Plot the heatmap in the left column
%% ax_left = fig.add_subplot(1, 2, 1)
%% ax_left.imshow(Z, extent=(-2, 3, -2, 3), origin='lower', cmap='gray')
%% ax_left.set_xlim([-2, 3])
%% ax_left.set_ylim([-2, 3])
%% 
%% # Draw m as a black dot and label it
%% ax_left.plot(m[0], m[1], 'o', color='black', markersize=8)
%% ax_left.text(m[0] + 0.2, m[1], r"$\mathbf{m}$", color="black", fontsize=fontsize)
%% 
%% # Draw the vectors as arrows
%% arrow_colors = ['red', 'green', 'blue']
%% for v, color in zip([v_1, v_2, v_3], arrow_colors):
%%     ax_left.arrow(0, 0, v[0], v[1], color=color, head_width=0.2, head_length=0.3, linewidth=2)
%% 
%% # Remove all formatting from the left column axes
%% ax_left.axis("off")
%% 
%% # Plot the densities in the right column with respective colors, labels, and shared x-axis
%% for i, (mean, variance) in enumerate(gaussians):
%%     ax_right = fig.add_subplot(3, 2, 2 + i * 2)
%%     density = (1 / np.sqrt(2 * np.pi * variance)) * np.exp(-0.5 * ((x_shared - mean) ** 2) / variance)
%%     ax_right.plot(x_shared, density, color=arrow_colors[i], linewidth=2)
%%     
%%     # Add label at the top right
%%     label = rf"$\mathcal{{N}}(v_{i+1}^T\mathbf{{m}}, v_{i+1}^T \Gamma v_{i+1})$"
%%     ax_right.text(0.95, 0.9, label, color=arrow_colors[i], fontsize=fontsize, transform=ax_right.transAxes, ha='right')
%%     
%%     # Remove all formatting from the right column axes
%%     ax_right.axis("off")
%% 
%% # Adjust layout
%% plt.tight_layout()
%% plt.show()




\end{frame}


% Slide 8: Recap and notation summary
\begin{frame}
\frametitle{Recap}

\begin{itemize}
\item Separable Hilbert space \(\hil\).
\item Linear forward operator \(\fwd: \hil \to \hil\).
%\item Observation operator \(\obs: \hil \to \mathbb{R}^m\).
\item Observation operator \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t \in \R^m,\ u \in \hil\).
\item Data \(\data := \obs \fwd \param + \eps,\ \eps \sim \mathcal{N}(0,I_m)\).
\item Prior \(\pr\) with covariance \(\prcov\).
\item Posterior \(\post\) with precision \(\postcov^{-1} = \prcov^{-1} + \fwd^* \obs^*\obs \fwd\).
\end{itemize}
%% heat equation
\end{frame}



\begin{frame}
  \begin{center}
    \includegraphics[width=\textwidth]{figs/bayesian_inverse_problems.jpg}
  \end{center}
  {\hfill \tiny Alexanderian (2021).}
\end{frame}



\begin{frame}
  \frametitle{How to choose $\obs$?}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \begin{itemize}
      \item \(\fwd: \hil \to \hil\).
      \item \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t\).
      \item \(\data := \obs \fwd \param + \eps\).
      \item Prior $\pr$, posterior $\post$ ...
      \end{itemize}
  
      \begin{figure}
        \centering
        \includegraphics[width=1.2\textwidth, height=0.66\textwidth]{figs/heat_rod.png}
      \end{figure}
    \end{column}

    \begin{column}{0.5\textwidth}

      \begin{figure}
        \centering
        \includegraphics[width=\linewidth, height=3.5cm]{figs/CT.png}
      \end{figure}

      \begin{figure}
        \centering
        \includegraphics[width=\linewidth, height=3.5cm]{figs/borehole.png}
      \end{figure}
      
    \end{column}
  \end{columns}
  %{\hfill \tiny Wikimedia Commons, Conroy and Guy 2005, radiologykey.com }
\end{frame}

\begin{frame}
  \only<1>{
    \begin{center}
      \includegraphics[width=1\textwidth]{figs/bayes.png}
    \end{center}
  }
  \only<2>{
    \begin{center}
      \includegraphics[width=1\textwidth]{figs/kl.png}
    \end{center}
  }
\end{frame}


\begin{frame}
  We have a \emph{design criterion}!  \only<4->{Maximize over $\obs$}
  \begin{equation*}
    \onslide<5->{\mathbb{E}_{\data} \Big [}\onslide<2->{D_{\text{KL}} (\post || \pr )} \onslide<5->{\Big ]}
    \onslide<3->{=} \onslide<5->{\mathbb{E}_{\data} \Big [}\onslide<3->{\int \log \frac{\der \post}{\der \pr}(\param) \der \post(\param)}  \onslide<5->{\Big ]}
  \end{equation*}
  %% \begin{equation*}
  %%   \onslide<1->{\phantom{\mathbb{E_{\data}}}}D_{\text{KL}} (\post || \pr ) = \onslide<1->{\phantom{\mathbb{E_{\data}}}} \int \log \frac{\der \post}{\der \pr}(\param) \der \post(\param)
  %%   \onslide<2->{\mathbb{E_{\data}}}
  %% \end{equation*}
  %% \vspace{-2.5em} % Adjust this value if necessary to align the second line correctly
  %% \begin{equation*}
  %%   \onslide<2->{\mathbb{E_{\data}}}
  %% \end{eion*}
\end{frame}



% Slide 10: D-optimality criterion
\begin{frame}
  % \frametitle{D-optimality criterion}
  \begin{theorem}[Alexanderian, Gloor, Ghattas (2014)]\label{thm:d_optimality}
    %% Let \(\pr = \normal(\prmean,\prcov)\) a Gaussian prior on \(\hil\)
    %% and let \(\post = \normal(\postmean,\postcov)\) the posterior.
    %% measure on \(\hil\) for the Bayesian linear inverse problem
    %% \(\data = \obs \fwd\param + \eps\).
    %% Then
    For linear $\fwd$ and Gaussian prior and likelihood:
    \begin{align*}
      \begin{split}
        \tar( \obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det 
        ( I +  \prcov^{1/2}  \fwd ^* \obs^* \obs \fwd \prcov^{1/2}).
      \end{split}
    \end{align*}
  \end{theorem}

  \pause
  \begin{definition}
    \(\opt\) is \emph{D-optimal} if \(\opt = \argmax_{\obs}
    \tar(\obs)\).
  \end{definition}


% Show the D-optimality criterion from Alexanderian et al.
% Assume linear forward operator and linear observation operator
\end{frame}

\begin{frame}
  \frametitle{Intuition}
  \begin{align*}
  \begin{split}
    \tar(\obs) &= \frac12 \log \det ( I + \prcov^{1/2}  \fwd ^* \obs^* \obs \fwd \prcov^{1/2})\\ 
    \onslide<2->{&= \frac12 \log \det \Big( \prcov ( \prcov^{-1} +  \fwd ^* \obs^* \obs \fwd) \Big )\\}
    \onslide<3->{&= \frac12 \log \det \prcov \postcov^{-1}.}
    %%  &= \frac12 \log \det \prcov -\frac12 \log \det \postcov.
  \end{split}
  \end{align*}

  \onslide<4->{D-optimal designs minimize $\det \postcov$.}
\end{frame}

\begin{frame}
  \frametitle{D-Optimality}
  \(\opt\) is \emph{D-optimal} if \(\opt = \argmax_{\obs}
  \tar(\obs)\), \pause \newline \newline where 
  \begin{equation*}
    \tar \stackrel{\sim}{=} \log \det \prcov \postcov^{-1}.
  \end{equation*}
\end{frame}

%% \begin{frame}
%%   Recall \(\tar( \obs) = \frac12 \log \det ( I + 
%%   \prcov^{1/2} \fwd ^* \obs^* \obs \fwd \prcov^{1/2})\).

%%   \begin{definition}
%%   \(\opt\) is \emph{D-optimal} if \(\opt =
%%   \argmax_{\obs} \tar(\obs)\).
%%   \end{definition}
  
%%   %% , where entries of \(\obs \in (\hil^*)^m\)
%%   %% are constrained to some allowed set of measurements in \(\hil^*\). 

%%   %% \(\obs\) must be constrained somehow! E.g.~norm has to be bounded.
%% \end{frame}


\begin{frame}
\frametitle{Problem: Clusterization}
\begin{figure}
  \centering
  \includegraphics[height=0.5\textwidth]{figs/dst_modelError0.png}
\end{figure}

% Discuss sensor clusterization in the heat equation example
% Show figure from the paper (placeholder for figure)
\end{frame}

\begin{frame}
  \frametitle{Avoiding clusterization}

  \begin{columns}
    \begin{column}{0.6\textwidth}
      \begin{itemize}
      \item<1> Merging close measurements, {\tiny Fedorov 1997}.
      \item<2> Distance constraints / correlated errors, {\tiny Ucinski 2005}.
      \item<3> Finitely many locations, {\tiny Alexanderian et al. 2020}.
      \end{itemize}
    \end{column}
    \begin{column}{0.55\textwidth}
      \only<1>{\includegraphics[width=\textwidth]{figs/merge.png}}
      \only<2>{\includegraphics[width=\textwidth]{figs/constraints.png}}
      \only<3>{\includegraphics[width=\textwidth]{figs/stadler_OED.png}}
    \end{column}
  \end{columns}  
\end{frame}



\begin{frame}
\frametitle{Questions}
\begin{itemize}
\item<2-> Why does imposing correlations between observations alleviate
measurement clusterization?
%
\item<3-> Is measurement clusterization a generic phenomenon?
%
\item<4-> And, most importantly: \onslide<5->{Why does measurement clusterization occur?}
% How do we tackle this problem?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A generic inverse problem}

\begin{itemize}
\item Separable Hilbert space \(\hil\).
\item Linear forward operator \(\fwd: \hil \to \hil\).
\item Observations \(\obs u = (\meas_1^*u, \dots, \meas_m^*u )^t \in \R^m\).
\item Prior and posterior covariance \( \prcov, \postcov\).
\item Data \(\data := \obs \fwd \param + \eps\).
\item \(\tar( \obs) = \frac12 \log \det ( I + 
  \prcov^{1/2} \fwd ^* \obs^* \obs \fwd \prcov^{1/2})\).
\item \(\opt\) is \emph{D-optimal} if \(\opt = \argmax_{\obs}
  \tar(\obs)\).
\end{itemize}

\pause \(\obs\) must be constrained somehow:\pause \(\|\meas_j\| \leq 1\).

%% \begin{align}\label{eq:objective}
%%   \begin{split}
%%     \tar( \obs) &= \frac12 \log \det 
%%     ( I + \prcov^{1/2}  \fwd ^* \obs^* \obs \fwd \prcov^{1/2}).
%%   \end{split}
%% \end{align}

% Discuss the main obstacle: the measurement operator
% Impose unit-norm constraints
% Explain that sensors have a unit inf-norm (show example)
% Show relevant equation from the paper
\end{frame}

% Slide 15: Constrained optimization problem
\begin{frame}
%\frametitle{Constrained optimization problem}
% Restate the optimization criterion from Alexandrian et al.'s theorem
% Restate the unit norm constraints
% Pose the problem as a constrained optimization problem
% Ask the audience how to solve constrained optimization problems (Lagrange multipliers)
\begin{align*}
  \begin{split}
  \opt &= \argmax\tar(\obs)\\
  \text{subject to }& \|\meas_j\| = 1 \text{ for } j=1,\dots,m.
  \end{split}
\end{align*}

Where
\begin{align*}
  \begin{split}
    \tar(\obs) %:&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
    % 
    % 
    % 
    &= \frac12 \log \det
    %
    \only<1>{
      ( I + \prcov^{1/2} \fwd ^* \obs^* \obs \fwd \prcov^{1/2})
    }
    %
    \only<2>{
    \prcov \postcov^{-1}
    }. 
  \end{split}
\end{align*}
\end{frame}


\begin{frame}[plain]
  \centering
      {\Huge \bfseries \setstretch{2} Results}
\end{frame}




\begin{frame}
%% \frametitle{Necessary conditions for optimality}
% Present the nonlinear eigenvalue problem for the vanishing model error case
% Explain why it is called a "nonlinear" eigenvalue problem
  
  \begin{theorem}[D.]
    Let:
    \begin{align*}
      \begin{split}
        \opt &= \argmax\tar(\obs)\\
        \text{subject to }& \|\meas_j\| = 1 \text{ for } j=1,\dots,m.
      \end{split}
  \end{align*}
    
    Then:
    \[
    \fwd \postcov \fwd^* \opt^* = \opt^* \Xi
    \]
  
    %% \begin{equation*}
    %%    \fwd \postcov \fwd^* \obs^* = \obs^* \Xi, 
    %% \end{equation*}
    where \(\Xi \in \mathbb{R}^{m \times m}\) is diagonal.
  \end{theorem}
  \pause A nonlinear eigenvalue problem. \pause Proof: Lagrange multipliers.
\end{frame}


\begin{frame}
%% \frametitle{Implication of first-order conditions}
 
  \begin{theorem}[D.]
    \begin{gather*}
      \fwd\prcov\fwd^* \text{ and } \opt^* \opt \text{ are simultaneously diagonalizable}.\\
      \Uparrow \\
      \onslide<4->{\left( (\fwd\prcov\fwd^*)^{-1} + \opt^* \opt \right)^{-1} \opt^* = \opt^* \Xi \\
        \Uparrow}  \\
      \onslide<3->{\fwd  \left(\prcov^{-1} + \fwd^* \opt^*\opt \fwd \right)^{-1} \fwd^* \opt^* = \opt^* \Xi  \\
        \Uparrow}  \\
      \onslide<2->{\fwd \postcov \fwd^* \opt^* = \opt^* \Xi  \\
      \Uparrow} \\
      \text{D-optimality}
    \end{gather*}
  \end{theorem}
  \uncover<5->{Last step: invariant space for each eigenvalue, use spectral decomposition.}
\end{frame}


%% \begin{frame}
%%   %\frametitle{Simultaneous diagonizability}
%%   \begin{lemma}
%%     %% Let \(\hil\) separable ilbert space,
%%     Let \(C:\hil \to \hil\) with orthogonal eigenbasis.
%%     %% and \(\func_1,\dots,\func_m \in \hil\). Denote \(\func^*\) the element
%%     %% \(\func\) acting as a linear functional. If
%%   \begin{equation*}
%%     (C + \sum_{j=1}^m \func_j\func_j^*) \func_l = \xi_l \func_l,\ l = 1, \dots, m.
%%   \end{equation*}
%%   $\Rightarrow$ \(C\) and \(\sum_{j=1}^m \func_j \func_j^*\) are simultaneously
%%   diagonalizable.
%%   \end{lemma}
%% Proof idea: Consider invariant space of each eigenvalue, use spectral decomposition.
%% \end{frame}


\begin{frame}
  \begin{theorem}[D.]
    Let:
    \begin{itemize}
      \item \(\opt\) a D-optimal design.
      \item \(\{\eta_i\}_{i=1}^{\infty}\) eigenvalues of \(\opt^*\opt\).
      \item \(\{\lambda_i\}_{i=1}^{\infty}\) corresponding eigenvalues of \(\fwd \prcov \fwd^*\).
    \end{itemize}
    \uncover<2->{Then \(\eta_i > 0\) for eigenvectors corresponding to largest \(\lambda_i\).}
  \end{theorem}
  
  %% \begin{itemize}
  %% \item <3-> How many? \(\rank \opt^*\opt\).
  %% \item <4->
  \uncover<3->{
    Why? Sylvester $\det I+ AB = \det I +BA$:
    \begin{align*}
      \det(I + \prcov^{1/2}  \fwd ^* \opt^* \opt \fwd \prcov^{1/2}) = \prod (1+\lambda_i\eta_i)
    \end{align*}
  }
  %% \end{itemize}
  %% INTRODUCE CYLINDERS!!!!
\end{frame}


\pgfplotstableread{
  Label     prior  optimal  sub-optimal  worst
  1         0.2    1.8           1.7      2.3
  2         0.8    1.2           0.8      0.0
  3         2.2    0             0.5      0.7
  4         3.5    0             0.0      0.0
}\optimalvsnot

\begin{frame}
  \begin{figure}
    \centering
    \begin{tikzpicture}[scale=0.61]
      \begin{axis}[
          ybar stacked,
          ymin=0,
          ymax=4,
          xtick=data,
          legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
          reverse legend=false, % set to false to get correct display, but I'd like to have this true
          xticklabels from table={\optimalvsnot}{Label},
          xticklabel style={text width=2cm,align=center},
          legend plot pos=right,
          ylabel={\LARGE precision --- prior and posterior},
          xlabel={\LARGE eigenvector},
        ]
        \addplot [fill=blue!60]  table [y=prior,   meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addplot [pattern=north east lines, pattern color=green!80]  table [y=optimal, meta=Label, x expr=\coordindex] {\optimalvsnot};     
        \addlegendentry[scale=1.4]{$\lambda_i^{-1}$}
        \addlegendentry[scale=1.4]{optimal $\eta_i$s}
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[scale=0.61]
      \begin{axis}[
          ybar stacked,
          ymin=0,
          ymax=4,
          xtick=data,
          legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
          reverse legend=false, % set to false to get correct display, but I'd like to have this true
          xticklabels from table={\optimalvsnot}{Label},
          xticklabel style={text width=2cm,align=center},
          legend plot pos=right,
          ylabel={\LARGE precision --- prior and posterior},
          xlabel={\LARGE eigenvector} ,
        ]   
        \addplot [fill=blue!60]  table [y=prior,       meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addplot [pattern=north east lines, pattern color=green!80]  table [y=worst, meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addlegendentry[scale=1.4]{$\lambda_i^{-1}$}
        \addlegendentry[scale=1.4]{sub-optimal $\eta_i$s}
      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}


%% \begin{frame}
%% %% \begin{proposition}
%%   Let \(\tar: \mathbb{R}^m \to \mathbb{R}\), \(\tar(\eta) =
%%   \frac{1}{2}\sum_{i=1}^m \log (1+\lambda_i \eta_i)\), with
%%   \(\lambda_i > 0\). Then the maximum of \(\tar\)
%%   subject to \(\eta_i \geq 0\) and \(\sum\eta_i = m\) is obtained at
%%   \begin{equation}
%%   \eta_i = \begin{cases}
%%     \frac{m}{k} -  \lambda_i^{-1} +  \frac{1}{k} \sum_{j\in A} \lambda_j^{-1} & i \in A \\
%%     0 & i \in A^c
%%   \end{cases}
%%   \end{equation}
%%   where \(A:= \{1\leq i \leq m: \eta_i > 0\}\) and \(A^c = \{1,\dots, m\}
%%   \backslash A\), and \(k = |A|\), the cardinality of \(A\).
%% %% \end{proposition}
%% \end{frame}


\begin{frame}
  \begin{theorem}[D.]
    \( \fwd \postcov \fwd^*\)
    %of the pushforward \(\fwd_{*} \post\) is and its
    has eigenvalues 
    \begin{align*}
      &\left(\frac{\sum_{j=1}^k \lambda_j^{-1} + m}{k} \right )^{-1} &&\quad i \leq k:= \rank \opt^*\opt \\
      &\lambda_i  &&\quad i > k.
    \end{align*}
  \end{theorem}
  \pause Proof: KKT and a lemma.
\end{frame}

\begin{frame}
  \begin{lemma}[D.]
    Let $M \in \R^{k \times k}$ symmetric positive definite with $\ttr
    M = m$, $m \geq k$. We can find $\func_j \in \R^k,j=1,\dots,m$
    with $\|\func_j\|=1$ and $A = (\func_1,\dots,\func_m)$ such that
    $AA^t = M$.
  \end{lemma}
  \pause Proof: $M = UDU^t$. Find $V$ orthogonal such that $V^tDV$ has
  unit diagonal via Givens rotations. Take $A := U\sqrt{D} V^t$.
\end{frame}


\begin{frame}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/la.png}
  \end{figure}
  % https://x.com/masonporter/status/1732149577007149348
\end{frame}



\begin{frame}
  \begin{figure}
    \centering
    \begin{tikzpicture}[scale=0.6]
      \begin{axis}[
          ybar stacked,
          ymin=0,
          ymax=4,
          xtick=data,
          legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
          reverse legend=false, % set to false to get correct display, but I'd like to have this true
          xticklabels from table={\optimalvsnot}{Label},
          xticklabel style={text width=2cm,align=center},
          legend plot pos=right,
          ylabel={\LARGE precision --- prior and posterior},
          xlabel={\LARGE eigenvector},
        ]
        \addplot [fill=blue!60]  table [y=prior,   meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addplot [pattern=north east lines, pattern color=green!80]  table [y=optimal, meta=Label, x expr=\coordindex] {\optimalvsnot};     
        \addlegendentry[scale=1.4]{$\lambda_i^{-1}$}
        \addlegendentry[scale=1.4]{optimal $\eta_i$s}
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[scale=0.6]
      \begin{axis}[
          ybar stacked,
          ymin=0,
          ymax=4,
          xtick=data,
          legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
          reverse legend=false, % set to false to get correct display, but I'd like to have this true
          xticklabels from table={\optimalvsnot}{Label},
          xticklabel style={text width=2cm,align=center},
          legend plot pos=right,
          ylabel={\LARGE precision --- prior and posterior},
          xlabel={\LARGE eigenvector} ,
        ]   
        \addplot [fill=blue!60]  table [y=prior,       meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addplot [pattern=north east lines, pattern color=green!80]  table [y=sub-optimal, meta=Label, x expr=\coordindex] {\optimalvsnot};
        \addlegendentry[scale=1.4]{$\lambda_i^{-1}$}
        \addlegendentry[scale=1.4]{sub-optimal $\eta_i$s}
      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Four Sensors for the Heat Equation}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/eigenvectors.png}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Simulating Generic Model}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/simulations.png}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Contributions}
  \begin{itemize}
  \item <1-> Generic model for D-optimality.
  \item <2-> D-optimality \( \Longrightarrow \) nonlinear eigenvalue problem.%\fwd \postcov\fwd^* \opt^* = \opt^* \Xi\).
  \item <3-> Key: $\opt^*\opt$ and \(\fwd \prcov\fwd^*\) are
    simultaneously diagonalizable.
  \item <4-> D-optimal design concentrates on smallest prior covariance eigenvectors...
  \item <5-> ... and reduces uncertainty uniformly on these.
  %\item <6-> Saw manifestation in the heat equation.
  \item <6-> Avoiding higher eigenvectors + pigeonhole principle
    $\Rightarrow$ clusterization.
  %\item <7-> We know how many d.o.f we can expect to resolve. 
  \item <7-> Clusterization is generic in \emph{linear} problems.
  \item <8-> Bottom line: avoid linearization. % when seeking optimal designs.
  \end{itemize}
\end{frame}
\end{document}


