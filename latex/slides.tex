\documentclass{beamer}
\input{definitions}

\begin{document}


% Slide 1: Title
\begin{frame}
\frametitle{Measurement Clusterization in Bayesian D-optimal Designs in Infinite Dimensions}
\end{frame}

% Slide 2: Forward problem
\begin{frame}
\frametitle{Forward problem}
% Example 1: Ball toss
% Example 2: Flow field (placeholder for image)
% Example 3: Ink in water
\[
\fwd: \hilp \to \hilo
\]
\end{frame}

% Slide 3: Inverse problem
\begin{frame}
\frametitle{Inverse problem}
% Explain the inverse problems for the ball toss, flow field examples
% Argue that there are many valid solutions
% Explain that such problems are ill-posed
\end{frame}

% Slide 4: Toy example
\begin{frame}
\frametitle{Toy example: 1D heat equation}
% Present the 1D heat equation with homogeneous Dirichlet boundary condition

\begin{subequations}\label{eq:heat equation}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,1] \times [0,\infty),\\
      u &= 0 &&\qquad \text{on } \{0, 1\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,1] \times \{0\}.
  \end{alignat}
\end{subequations}

\end{frame}

% Slide 5: Solving inverse problems
\begin{frame}
\frametitle{Measurements}
% Show "sensor" and discuss measurement placement
\begin{equation*}%\label{eq:O}
  \obs u = (\meas_1(u), \dots, \meas_m(u) )^t \in \R^m,\ u \in \hilo.
\end{equation*}

% Introduce D-Optimality heuristically
\end{frame}

% Slide 6: Prior and posterior measures
\begin{frame}
\frametitle{Prior and posterior}
% Introduce prior measure $\mu_{\text{pr}}$ and posterior measure $\mu_{\text{post}}$
% Define KL-divergence and provide an intuitive explanation
\end{frame}

% Slide 7: Gaussian measures on Hilbert spaces
\begin{frame}
\frametitle{Gaussian measures on Hilbert spaces}
% Define Gaussian measure
% Show a 2D example with iso-density lines and projections
% Introduce notation for mean $\mathbf{m}$ and covariance $\Gamma$
\end{frame}

% Slide 8: Recap and notation summary
\begin{frame}
\frametitle{Recap and notation summary}
% Summarize notation:
\begin{itemize}
\item Forward operator \(\fwd: \hilp \to \hilo\)
\item Parameter space $\hilp$.
\item Observation space $\hilo$.
\item Observation operator $\obs: \hilo \to \mathbb{R}^m$
\item Prior measure $\pr \sim \mathcal{N}(0, \prcov)$.
\item Posterior measure $\post \sim \mathcal{N}(0, \postcov)$.
\end{itemize}
%% 1D heat equation
\end{frame}

% Slide 9: Formalism of inverse problems
\begin{frame}
  \frametitle{Formalism of inverse problems}
  \begin{align}\label{eq:inverse problem}
    \data := \obs \fwd \param + \eps
  \end{align}
  
  % Present the entire formalism of inverse problems from the paper, for zero model error
\end{frame}

% Slide 10: D-optimality criterion
\begin{frame}
  \frametitle{D-optimality criterion}
  \begin{theorem}[Alexanderian, Gloor, Ghattas \cite{AlexanderianGloorGhattas14}]\label{thm:d optimality}
    Let \(\pr = \normal(\prmean,\prcov)\) be a Gaussian prior on \(\hilp\)
    and let \(\post = \normal(\postmean,\postcov)\) the posterior measure
    on \(\hilp\) for the Bayesian linear inverse problem \(\data = \obs
    \fwd\param +  \eps\). Then
    \begin{align}\label{eq:objective}
      \begin{split}
        \tar( \obs) :&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
        % 
        % 
        % 
        &= \frac12 \log \det 
        ( I + \sigma^{-2} \prcov^{1/2}  \fwd ^* \obs^* \obs \fwd \prcov^{1/2}).
      \end{split}
    \end{align}
  \end{theorem}

% Show the D-optimality criterion from Alexanderian et al.
% Assume linear forward operator and linear observation operator
% Provide intuition about the log det of quotient of posterior and prior covariance/precision operators
\end{frame}

\begin{frame}
  \begin{definition}\label{def:d optimality}
  We say \(\obs^{\star}\) is \emph{D-optimal} if \(\obs^{\star} =
  \argmax_{\obs} \tar(\obs)\), where entries of \(\obs \in (\hilo^*)^m\)
  are constrained to some allowed set of measurements in \(\hilo^*\).
\end{definition}

  \end{frame}
% Slide 11: Sensor clusterization
\begin{frame}
\frametitle{Problem: Sensor clusterization}
\begin{figure}
    \centering
    \includegraphics[height=0.5\textwidth]{example.pdf}
  %%   \caption{Measurement clusterization for optimal designs when
  %%     inverting for the initial condition of the 1D heat equation (see
  %%     supplementary material for details). Measurement locations were
  %%     chosen according to the Bayesian D-optimality criterion of
  %%     Theorem \ref{thm:d optimality}. Measurement locations are
  %%     plotted over the computational domain \(\Omega = [0, 1]\)
  %%     (x-axis), for varying numbers of measurements (y-axis). The
  %%     colored numbers are measurement indices, plotted for visual
  %%     clarity. Measurement clusterization already occurs for three
  %%     measurements: the second measurement (red) is overlaid with the
  %%     third (green). For five measurements, first (blue) and second
  %%     (red) measurements are clustered, as well as the fourth (black)
  %%     and the fifth (magenta).}
  %% \label{fig:clusterization illustration}
\end{figure}

% Discuss sensor clusterization in the 1D heat equation example
% Show figure from the paper (placeholder for figure)
\end{frame}

% Slide 12: Avoiding clusterization
\begin{frame}
\frametitle{Avoiding clusterization}
\begin{itemize}
\item Model correlation
\item Choose from a finite set of sensor locations
\end{itemize}
\end{frame}

% Slide 13: Main questions
\begin{frame}
\frametitle{Main questions}
% Present the main questions from the introduction of the paper
\begin{itemize}
\item Why does imposing correlations between observations alleviate
measurement clusterization?
%
\item Is measurement clusterization a generic phenomenon?
%
\item And, most importantly: Why does measurement clusterization occur?
% How do we tackle this problem?
\end{itemize}
\end{frame}

% Slide 14: Proposed approach
\begin{frame}
\frametitle{A "generic" inverse problem}
Recall definitions

\begin{align}\label{eq:inverse problem}
    \data := \obs \fwd \param + \eps
\end{align}

%% \begin{align}\label{eq:objective}
%%   \begin{split}
%%     \tar( \obs) &= \frac12 \log \det 
%%     ( I + \prcov^{1/2}  \fwd ^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2}).
%%   \end{split}
%% \end{align}

% Discuss the main obstacle: the measurement operator
% Impose unit-norm constraints
% Explain that sensors have a unit inf-norm (show example)
% Show relevant equation from the paper
\end{frame}

% Slide 15: Constrained optimization problem
\begin{frame}
\frametitle{Constrained optimization problem}
% Restate the optimization criterion from Alexandrian et al.'s theorem
% Restate the unit norm constraints
% Pose the problem as a constrained optimization problem
% Ask the audience how to solve constrained optimization problems (Lagrange multipliers)
Let The D-optimality design criterion
\cite{AlexanderianGloorGhattas14}:
\begin{align*}
  \begin{split}
    \tar(\obs) %:&= \mathbb{E}_{\data}\left [ D_{\text{KL}} (\post || \pr ) \right ] \\
    % 
    % 
    % 
    &= \frac12 \log \det ( I + \sigma^{-2} \prcov^{1/2} \fwd ^*
    \obs^* \obs \fwd \prcov^{1/2}), 
  \end{split}
\end{align*}
The constrained optimization problem of D-Optimal designs entails finding
\begin{equation*}
  \obs = \argmax_{\|\meas_j\| = 1, j=1,\dots,m}\tar(\obs),
\end{equation*}
\end{frame}

% Slide 16: Necessary conditions for optimality
\begin{frame}
\frametitle{Necessary conditions for optimality}
% Present the nonlinear eigenvalue problem for the vanishing model error case
% Explain why it is called a "nonlinear" eigenvalue problem

\begin{theorem}[Necessary conditions for D-Optimality]\label{thm:constrained}
  Let:
  \begin{equation*}
    \obs^{\star} &= \tar(\obs).
    \text{subject to: } \|\meas_j\| &= 1\ \text{ for } j=1,\dots,m
  \end{equation*}
  
  Then:
  \begin{equation*}
    \fwd \postcov \fwd^* \obs^*  \Sigma^{-1}  = \obs^* \Xi, 
  \end{equation*}
  where $\Xi \in \mathbb{R}^{m \times m}$ is diagonal.
\end{theorem}

\end{frame}

% Slide 17: Lemma 17
\begin{frame}
\frametitle{Lemma 17}
% State Lemma 17 (placeholder for the actual lemma)
\end{frame}

% Slide 18: Main theorem
\begin{frame}
\frametitle{Main theorem}
% State the main theorem
\end{frame}

% Slide 19: Main theorem explanation
\begin{frame}
\frametitle{Main theorem explanation}
% Explain the main theorem using a graph and three graduated lab cylinders
\end{frame}

% Add the remaining slides here
% ...

\end{document}
