\documentclass{article}
\input{definitions}
\usepackage{amsthm}
\usepackage{authblk}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,filecolor=blue,backref=page]{hyperref}
%% \newtheorem{lemma}{Lemma}
%% \newtheorem{proposition}{Proposition}
%% \externaldocument[][nocite]{ms}

\title{Supplementary Material: Measurement Clusterization in D-optimal
  Designs for Bayesian Linear Inverse Problems over Hilbert Spaces}

\author{Yair Daon\thanks{yair.daon@gmail.com}}
\affil{Azrieli Faculty of Medicine, Bar Ilan University, Safed,
  Israel.}

\begin{document}
\maketitle



\section{Inverse Problem of the 1D Heat Equation}\label{subsec:1d_heat}
Below, we give details of the inverse problem of the 1D heat
equation. Our goal is to infer the initial condition of the
one-dimensional heat equation with a homogeneous Dirichlet boundary
condition (eq.~\ref{eq:heat_equation} in the manuscript), i.e.~ an
absorbing boundary. We model the prior for the initial condition as
$u_0 \sim \normal(0,\prcov)$, for $\prcov = (-\Delta)^{-1}$ with a
homogeneous Dirichlet boundary condition.

%% Recall that by Weyl's law, eigenvalues
%% $\lambda_j$ of the Laplace operator increase as $\lambda_j \sim
%% j^{d/2}$, where $d = \dim \Omega$ is the dimensionality of the
%% domain. In accordance with Weyl's law, \cite[Theorem 3.1]{Stuart10}
%% suggests taking priors as "Laplacian-like" operators raised to a power
%% smaller than $-\frac{d}{2}$, to maintain regularity of posterior
%% realizations.

We let the final time $T = 0.03$ and denote $\fwd$ the forward
operator, so that $u(\cdot, T) = \fwd u_0$. We let $\obs$ a
measurement operator, so that $u(\x_j,T) = (\obs u)_j, j=1,\dots,m$,
where $m$ is the number of allowed measurements. We assume iid
centered Gaussian observation error, so observations are given by
$u(\x_j,T) + \eps(\x_j)$ with $\eps(\x_j) \sim \normal(0, \sigma^2)$
iid, $\sigma = 0.05$. Measurement locations $\x_j,j=1,\dots,m$ are
chosen according to the D-optimality criterion
\cite{AlexanderianGloorGhattas14}.

We also implement a similar inverse problem with different boundary
condition, i.e.~a homogeneous Neumann boundary condition. Such
boundary condition is specified by enforcing the derivative normal to
the boundary to equal zero:
\begin{subequations}\label{eq:neumann}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,1] \times [0,\infty),\\
      \frac{\partial u}{\partial \mathbf{n}} &= 0 &&\qquad \text{on } \{0, 1\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,1] \times \{0\},
  \end{alignat}
\end{subequations}
where $\mathbf{n}$ is the vector perpendicular to the boundary. A
homogeneous Neumann boundary condition implements a reflective
boundary, i.e.~heat that interacts with the boundary is immediately
reflected back into $\Omega$.


\subsection{Numerical Implementation}
Our solution is based on moving the entire problem to the frequency
domain, where the Laplacian $\Delta$ is a diagonal operator. For the
homogeneous Dirichlet boundary version, we move to the frequencey
domain via the Discrete Sine Transform, while for the homogeneous
Neumann boundary condition version we do this via the Discrete Cosine
Transform. Importantly, both transforms respect their corresponding
boundary condtions.

The implementation of the D-optimality criterion for this inverse
problem with homogeneous Neumann and Dirichlet boundary conditions can
be found in directory \texttt{src/} and script
\texttt{clusterization.py} in the accompanying
\href{https://github.com/yairdaon/OED}{repository}. Unfortunately, I
was not able to install PyOED \cite{attia2023pyoed}, nor was I able to
utilize hIPPYlib \cite{VillaPetraGhattas16, VillaPetraGhattas18,
  VillaPetraGhattas21}, hence the inverse problem is implemented from
scratch.










\section{Proofs from Main Text}

\begin{proposition}[Proposition 4 from main text]%\label{prop:tar_grad}
  The gradient of the D-optimality objective $\tar$ is
  \begin{equation*}
    %% \delta \tar(\obs) V = \tr{V ( I - \modcov \obs^* \Sigma^{-1}\obs )
    %%   \fwd \postcov \fwd^* \obs^* \Sigma^{-1}}.
    \nabla \tar(\obs) = ( I - \modcov \obs^* \Sigma^{-1}\obs ) \fwd
    \postcov \fwd^* \obs^* \Sigma^{-1}
  \end{equation*}
\end{proposition}

\begin{proof}  
  From the definition of $\Sigma(\obs)$ \eqref{eq:Sigma}: 

  \begin{align}\label{eq:der_sig}
    \begin{split}
      \frac{\der}{\der \tau} \Big |_{\tau=0} \Sigma( \obs + \tau V )
      &= \frac{\der}{\der \tau} \Big |_{\tau=0} 
      (\obs + \tau V ) \modcov (\obs + \tau V )^*  + \sigma^2I\\
      % 
      % 
      % 
      &= V \modcov \obs^* + \obs \modcov V^*.
    \end{split}
  \end{align}

  Then, using \eqref{eq:der_sig}: 
  \begin{align*}
    0 &= \frac{\der}{\der \tau} \Big |_{\tau=0} I \\
    % 
    % 
    % 
    &= \frac{\der}{\der \tau} \Big |_{\tau=0}
    \left (\Sigma(\obs+\tau V)^{-1} \Sigma(\obs+\tau V) \right ) \\
    % 
    % 
    % 
    &= \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0} \Sigma+
    \Sigma^{-1} \frac{\der \Sigma(\obs+\tau V)}{\der \tau} \Big |_{\tau=0}\\  
    %
    %
    %
    &= \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0} \Sigma+
    \Sigma^{-1} (V\modcov \obs^* + \obs \modcov V^*). 
    %\text{, by \eqref{eq:der_sig}. }
  \end{align*}

  Thus:
  \begin{align}\label{eq:der_sig_inv}
    \frac{\der \Sigma(\obs+\tau V)^{-1}}{\der \tau} \Big |_{\tau=0}  
      &= -\Sigma^{-1} (V \modcov \obs^* + \obs \modcov V^*) \Sigma^{-1}.
    \end{align}
  % 

  Let
  \begin{equation*}
    T(\obs) = \obs^* \Sigma^{-1}(\obs)\obs.
  \end{equation*}
  
  By Leibniz (product) rule and \eqref{eq:der_sig_inv}:
  % 
  \begin{align}\label{eq:T}
    \begin{split}
    \delta T(\obs) V 
    &= \frac{\der T(\obs + \tau V)}{\der \tau} \Big |_{\tau=0} \\
    %
    %
    %
    &= V^* \Sigma^{-1} \obs 
    - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
    &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs
    + \obs^* \Sigma^{-1} V.
    \end{split}
  \end{align}

  We now record a lemma which generalizes a known result in linear
  algebra \cite[Chapter 9, Theorem 4]{Lax07}. Its proof is delegated
  to the supplementary material.
  \begin{lemma}\label{lemma:lax}
    Let $Y(t)$ be a differentiable operator-valued function. Assume 
    $I+Y(t)$ is invertible, $Y(t)$ self-adjoint and trace-class. Then
    \begin{equation*}
      \frac{\der \log \det (I+Y(t))}{\der t} = \tr{(I+Y(t))^{-1} \dot{Y}(t)}.
    \end{equation*}
  \end{lemma}
  
  We employ Lemma \ref{lemma:lax} and calculate the first variation of
  $\tar$:

  \begin{align*}
    %\begin{split}
      \delta \tar(\obs) V 
      :&= \frac{\der}{\der\tau} \Big |_{\tau=0} \tar(\obs + \tau V) \text{ (Definition \ref{def:var})}\\
      % 
      % 
      % 
      &= \frac12 \frac{\der}{\der \tau} \Big |_{\tau=0} \log \det 
      (I + \prcov^{1/2} \fwd^* T(\obs+\tau V)\fwd \prcov^{1/2} ) \text{ (Theorem \ref{thm:d_optimality})} \\
      % 
      % 
      % 
      &= \frac12 \tr{( I + \prcov^{1/2} \fwd^* \obs^* \Sigma^{-1}
        \obs\fwd \prcov^{1/2} )^{-1}
        \frac{\der}{\der \tau} \Big |_{\tau=0}
        \prcov^{1/2} \fwd^* T(\obs+\tau V) \fwd \prcov^{1/2}}\ \text{ (Lemma \ref{lemma:lax})} \\
      % 
      % 
      % 
      &= \frac12 \ttr\Big \{ \postcov \fwd^* (V^* \Sigma^{-1} \obs 
      - \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs \\
      &\ \ \ - \obs^* \Sigma^{-1} \obs \modcov V^* \Sigma^{-1}\obs 
      + \obs^* \Sigma^{-1} V ) \fwd \Big \}  \text{ (by \eqref{eq:T})} \\
      %
      %
      %
      &= \tr{\postcov \fwd^* ( \obs^* \Sigma^{-1} V -
      \obs^*\Sigma^{-1} V\modcov \obs^* \Sigma^{-1}\obs ) \fwd} \\
      %
      %
      % 
      &= \tr{\postcov \fwd^* \obs^* \Sigma^{-1} V 
      ( I - \modcov \obs^* \Sigma^{-1}\obs ) \fwd} \\
      % 
      %
      %
      &= \tr{V ( I - \modcov \obs^* \Sigma^{-1}\obs )
      \fwd \postcov \fwd^* \obs^* \Sigma^{-1}}.
    %\end{split}
  \end{align*} 
  Recalling Definition \ref{def:var} concludes the proof.
\end{proof}















\begin{proposition}[Proposition 6 from main text]%\label{prop:constraints_grad}
  Let
  \begin{align*}
    \phi_j(\obs) :=\frac12 \| \obs^* e_j\|_{\hilp}^2 - \frac12 = 0,\ j=1,\dots,m.
  \end{align*}
  Then
  \begin{equation*}
    %% \delta \phi_j(\obs)V = \tr{V \obs^* e_je_j^t}.
    \nabla \phi_j(\obs) = \obs^* e_je_j^t.
  \end{equation*}
\end{proposition}

\begin{proof}
  \begin{align*}
    \delta \phi_j(\obs)V  
    &= \frac12\lim_{\tau \to 0}\tau^{-1}
    ( \|(\obs + \tau V)^*e_j \|_{\hilp}^2 - \|\obs ^*e_j \|_{\hilp}^2  ) \\
    %
    %
    %
    &= \frac12\lim_{\tau \to 0}\tau^{-1}
    ( \langle (\obs + \tau V)^*e_j, (\obs + \tau V)^*e_j \rangle_{\hilp} - 
    \langle \obs^*e_j, \obs^*e_j \rangle_{\hilp} ) \\
    % 
    % 
    %
    &= \frac12\lim_{\tau \to 0}\tau^{-1}
    (2\tau \langle \obs^*e_j,V^*e_j \rangle_{\hilp} 
    +\tau^2 \langle V^*e_j, V^*e_j \rangle_{\hilp} ) \\
    %
    %
    % 
    &= \langle \obs^*e_j,V^*e_j \rangle_{\hilp} \\
    %
    %
    % 
    &= \langle V \obs^*e_j,e_j \rangle_{\R^m} \\
    %
    %
    %
    &= e_j^t V \obs^* e_j \\
    % 
    %
    %
    &= \tr{V \obs^* e_je_j^t}.
  \end{align*}
\end{proof}
















\begin{proposition}[Proposition 9 from main text]
  Let $\obs = (\meas_1,\dots,\meas_m)^t$ and $\obsm :=
  (\meas_1,\dots,\meas_{m-1})^t$. Then
  \begin{equation}\label{eq:conclusion}
    \tar( \obs ) - \tar (\obsm ) =
    \frac12 \log \left ( 1 + \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m \rangle
    }{
      \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
    }       
    \right ).
  \end{equation}
\end{proposition}

\begin{proof}
  Let
  \begin{align*}
    \Sigma( \obs ) &= 
    \begin{bmatrix}
      \Sigma (\obsm )           & \obsm \modcov \meas_m \\
      \meas_m \modcov \obsm^*   & \sigma^2 + \meas_m \modcov \meas_m
    \end{bmatrix}
    : =
    \begin{bmatrix}
      \Sigmam   & w \\
      w^t       & c
    \end{bmatrix}\\
    %
    %
  \end{align*}

  The Schur complement implies:
  \begin{align}\label{eq:schur}
    \begin{split}
          \Sigma^{-1} &=
          \begin{bmatrix}
            \Sigmam^{-1} + \Sigmam^{-1} w ( c - w^t \Sigmam^{-1} w)^{-1} w^t \Sigmam^{-1} & - \Sigmam^{-1} w ( c - w^t \Sigmam^{-1} w)^{-1} \\
            -( c - w^t \Sigmam^{-1} w)^{-1} w^t \Sigmam^{-1}                            &  ( c - w^t \Sigmam^{-1} w)^{-1}
          \end{bmatrix} \\
          &=
          \begin{bmatrix}
            \Sigmam^{-1} & 0 \\
            0           & 0 
          \end{bmatrix}
          + (c -w^t \Sigmam^{-1} w )^{-1}
          \begin{bmatrix}
            \Sigmam^{-1} w \\
            -1
          \end{bmatrix}
          \begin{bmatrix}
            w^t \Sigmam^{-1} & -1 
          \end{bmatrix},
    \end{split}
  \end{align}
  %
  and denote:
  %
  \begin{align}\label{eq:M_def}
    \M (\obs ):&= \prcov^{\frac12}\fwd^* \obs^* \Sigma^{-1} \obs \fwd
    \prcov^{\frac12}.
  \end{align}
  
  From \eqref{eq:schur} and \eqref{eq:M_def}:
  \begin{align*}
    \M(\obs) &= \prcov^{1/2} \fwd^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2} \\
    %
    %
    %
    &= \prcov^{1/2} \fwd^* \obs^* \left \{
    \begin{bmatrix}
      \Sigmam^{-1} & 0 \\
      0           & 0 
    \end{bmatrix}
    + (c -w^t \Sigmam^{-1} w )^{-1}
    \begin{bmatrix}
      \Sigmam^{-1} w \\
      -1
    \end{bmatrix}
    \begin{bmatrix}
      w^t \Sigmam^{-1} & -1 
    \end{bmatrix} 
    \right \} \obs \fwd \prcov^{1/2} \\
    %
    %
    %
    &= \M (\obsm) + (c -w^t \Sigmam^{-1} w )^{-1}
    \prcov^{1/2} \fwd^* \obs^*
    \begin{bmatrix}
      \Sigmam^{-1} w \\
      -1
    \end{bmatrix}
    \begin{bmatrix}
      w^t \Sigmam^{-1} & -1 
    \end{bmatrix} 
    \obs \fwd \prcov^{1/2}
  \end{align*}
  %
  Now, denote:
  %
  \begin{align}\label{eq:u}
    \begin{split}
      u :&= (c -w^t \Sigmam^{-1} w )^{-1/2}
      \prcov^{1/2} \fwd^* \obs^* 
      \begin{bmatrix}
        \Sigmam^{-1} w \\
        -1 
      \end{bmatrix} \\
      %
      %
      %
      & = (c -w^t \Sigmam^{-1} w )^{-1/2} ( \prcov^{1/2}\fwd^* \obsm^* \Sigmam^{-1} \obsm  \modcov \meas_m - \prcov^{1/2} \fwd^* \meas_m )\\
      %
      %
      %
      u^* :&=  (c -w^t \Sigmam^{-1} w )^{-1/2} (\meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \fwd \prcov^{1/2} - \meas_m \fwd \prcov^{1/2} ),
    \end{split}
  \end{align}
  %
  so that
  %
  \begin{equation}\label{eq:M_plus_I}
    I + \M( \obs ) = I + \M (\obsm ) + uu^*.
  \end{equation}
  %
  Note that
  \begin{equation}\label{eq:M_postcov}
    \prcov^{1/2} \left (I + \M( \obsm ) \right )^{-1} \prcov^{1/2} = \postcovm.
  \end{equation}
  From a generalization of the matrix determinant lemma to Hilbert
  spaces ($\det(A + uv^*) = (1 + \langle A^{-1} u,u \rangle) \det
  A$, statement and proof in the supplementary material):
  %
  \begin{align}\label{eq:diffs}
    \begin{split}
      \tar( \obs ) - \tar( \obsm )
      %
      %
      %
      &= \frac12 \log \left (\det \big ( I + \M ( \obs ) \big ) / \det \big ( I + \M (\obsm) \big ) \right )\\
      %
      %
      %
      &= \frac12  \log \left (\det \left ( I + \M(\obsm) + uu^* \right ) / \det \big ( I + \M (\obsm) \big )\right ) \\
      %
      %
      %
      &= \frac12 \log \left ( 1 + \left \langle \left ( I+\M(\obsm) \right )^{-1} u, u  \right \rangle \right ).
    \end{split}
  \end{align}
  From \eqref{eq:u} and \eqref{eq:M_postcov}:
  \begin{align}\label{eq:final}
    \begin{split}
      &\left \langle \left (I+\M (\obsm)\right )^{-1}u, u \right \rangle\\
      &= \frac{
        \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m,
        (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m \rangle
      }{
        c- w^t \Sigmam^{-1} w
      }\\
      %
      %
      %
      &= 
      \frac{
      \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m,
      (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m \rangle
      }{
        \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
      }
    \end{split}
  \end{align}
  and the conclusion follows by substituting \eqref{eq:final} into
  \eqref{eq:diffs}.
\end{proof}








\begin{lemma}[Lemma 11 from main text]%\label{lemma:sim_diag}
  Let $\hil$ separable Hilbert space, $C:\hil \to \hil$ self-adjoint
  and $\func_1,\dots,\func_m \in \hil$. Denote $\func^*$ the element
  $\func$ acting as a linear functional. If
  \begin{equation*}
   (C + \sum_{j=1}^m \func_j\func_j^*) \func_l = \xi_l \func_l,\ l = 1,\dots,m
  \end{equation*}
  then $C$ and $\sum_{j=1}^m \func_j \func_j^*$ are simultaneously
  diagonalizable.
\end{lemma}
\begin{proof}
  First, enumerate the eigenvalues of $C + \sum_{j=1}^m
  \func_j\func_j^*$ as $\xi_1,\dots,\xi_\ell$. Denote the indices of
  the eigenvectors corresponding to $\xi_i$
  \begin{equation*}
    S_i := \{ 1 \leq k \leq m | (C + \sum_{j=1}^m \func_j\func_j^* )\func_k = \xi_i \func_k \}.
  \end{equation*}
  Define further
  \begin{equation*}
    A_i := \sum_{k \in S_i} \func_k \func_k^*,
  \end{equation*}
  which is self-adjoint. Two observations are in order. First,
  $\sum_{j=1}^m \func_j\func_j^* = \sum_{i=1}^\ell A_i$. Second, $A_i
  \func_k = 0$ if $k\not \in S_i$, since eigenvectors of different
  eigenvalue are orthogonal. For $k \in S_i$
  \begin{equation}\label{eq:on_vi}
    \xi_i \func_k = (C + \sum_{j=1}^m \func_j \func_j^* ) \func_k = (C + A_i) \func_k.
  \end{equation}
  Let $V_i := span \{\func_k \}_{k\in S_i}$. Observe that $V_i$ is
  invariant under $A_i$, by definition, and under $C$, by \eqref{eq:on
____vi}. A second application of \eqref{eq:on_vi} shows that $A_i =
  \xi_iI - C$ on $V_i$. This immediately implies $A_i$ and $C$ are
  simultaneously diagonalizable on $V_i$. This holds for every $1 \leq
  i \leq \ell$ and we conclude that $C$ and $A$ are simultaneously
  diagonalizable.
\end{proof}






















\begin{proposition}[proposition 10]%\label{prop:twice_woodbury}
  Assume $\fwd \prcov \fwd^*$ is invertible. Then
  \begin{align*}
    \begin{split}
      \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* \obs^* \obs \fwd )^{-1} \fwd^* 
      %
      %
      = \left ( (\fwd\prcov\fwd^*)^{-1} + \sigma^{-2}  \obs^* \obs \right )^{-1},
    \end{split}
  \end{align*}  
\end{proposition}


\begin{proof}
  Recall Woodbury's matrix identity:
  \begin{equation}\label{eq:WMI}
    (A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}. 
  \end{equation}

  Denote:
  \begin{align}\label{eq:notation}
    \begin{split}
      A :&= \prcov^{-1} \\
      U :&= \fwd^* \\
      V :&= \fwd \\
      C :&= \sigma^{-2} (\obs^*\obs+\nu I),\ \nu > 0
    \end{split}
  \end{align}

  Then, \eqref{eq:WMI} with the notation \eqref{eq:notation} implies:
  \begin{align}\label{eq:first}
    \begin{split}
    &\fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\nu I) \fwd )^{-1}\fwd^* \\
    &\ \ \ \ \ = \fwd ( \prcov - \prcov \fwd^* ( \sigma^2(\obs^*\obs + \nu I)^{-1} + \fwd \prcov \fwd^* )^{-1} \fwd \prcov ) \fwd^*. \\
      %
      %
      %% &= X - X(\sigma^2Y_{\nu}^{-1} + X)^{-1}X
  %%   \end{split}
    %% \end{align*}
    \end{split}
  \end{align}
  Now, denote:
  \begin{align*}
    X :&= \fwd\prcov \fwd^*, \\
    Y_{\nu} :&= \obs^*\obs + \nu I,
  \end{align*}
  and observe that $Y_{\nu}$ is invertible. Substitute $X, Y_{\nu}$
  into the RHS of \eqref{eq:first}:
   \begin{equation}\label{eq:second}
     \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\nu I) \fwd )^{-1}\fwd^* = X - X(\sigma^2Y_{\nu}^{-1} + X)^{-1}X
   \end{equation}

   Note that $X + \sigma^2 Y_{\nu}^{-1}$ is invertible, as the sum of
   two positive definite operators. Now, let
   \begin{align}\label{eq:notation2}
     \begin{split}
       A :&= X^{-1}, \\
       C :&= \sigma^2Y_{\nu}^{-1}, \\
       U :&= I, \\
       V :&= I.
     \end{split}
   \end{align}
   Apply \eqref{eq:WMI} with the notation \eqref{eq:notation2} to the
   RHS of \eqref{eq:second}:
  \begin{align*}
    \begin{split}
      \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\nu I) \fwd )^{-1}\fwd^* &= X - X(\sigma^2Y_{\nu}^{-1} + X)^{-1}X \\
      %
      %
      %
      &= (X^{-1} + \sigma^{-2}Y_{\nu})^{-1} \\
      %
      %
      %
      &= (\fwd \prcov \fwd^* + \sigma^{-2} (\obs^*\obs + \nu I))^{-1}.
    \end{split}
  \end{align*}

  We conclude that $\forall \nu > 0$
  \begin{align*}
    \begin{split}
      \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\nu I) \fwd )^{-1}\fwd^* 
     &= (\fwd \prcov \fwd^* + \sigma^{-2} (\obs^*\obs + \nu I))^{-1}.
    \end{split}
  \end{align*}
  Letting $\nu \to 0$ completes the proof.
\end{proof}












\begin{proposition}[Proposition 14 from main text]
  Let $\tar: \mathbb{R}^m \to \mathbb{R}$, $\tar(\eta) =
  \frac{1}{2}\sum_{i=1}^m \log (1+\sigma^{-2}\lambda_i \eta_i)$, with
  $\lambda_i > 0$ and $\sigma^{2} > 0$. Then the maximum of $\tar$
  subject to $\eta_i \geq 0$ and $\sum\eta_i = m$ is obtained at
  \begin{equation}
  \eta_i = \begin{cases}
    \frac{m}{k} - \sigma^2 \lambda_i^{-1} + \sigma^2 \frac{1}{k} \sum_{j\in A} \lambda_j^{-1} & i \in A \\
    0 & i \in A^c
  \end{cases}
  \end{equation}
  where $A:= \{1\leq i \leq m: \eta_i > 0\}$ and $A^c = \{1,\dots, m\}
  \backslash A$, and $k = |A|$, the cardinality of $A$.
\end{proposition}



\begin{proof}
  Let $\Phi(\eta) = \sum_{i=1}^k \eta_i - m$ and $\Omega_j(\eta) =
   -\eta_j$. Then
  \begin{align*}
    \begin{split}
      \frac{\partial \tar}{\partial \eta_i}  &=
       \frac12 \frac{\sigma^{-2}\lambda_i}{1 + \sigma^{-2} \lambda_i\eta_i} = \frac12 \frac{1}{\sigma^{2}\lambda_i^{-1} + \eta_i} \\
      %
      %
      %
      \frac{\partial\Phi}{\partial \eta_i} &= 1 \\
      %
      %
      %
      \frac{\partial \Omega_j}{\partial \eta_i} &= -\delta_{ij}      
    \end{split}
  \end{align*}

  From the KKT conditions, there are $\alpha, \beta_i$ such that for $i=1,\dots,m$:
  \begin{align}
    \begin{split}
      -\frac12 \frac{1}{\sigma^{2}\lambda_i^{-1} + \eta_i} + \alpha - \beta_i  &= 0 \\
      %
      %
      %
      \eta_i &\geq 0\\
      %
      %
      %
      \beta_i &\geq 0\\
      %
      %
      %
      \beta_i \eta_i &= 0\\
      %
      %
      %
      \sum_{i=1}^m \eta_i &= m 
    \end{split}
  \end{align}

  Then, for $i \in A$:
  \begin{align*}
    \begin{split}
      \beta_i &= 0\\
      %
      %
      %
      \sigma^{2}\lambda_i^{-1} + \eta_i  &= \frac{1}{2\alpha}%  \text{ for }  1 \leq i \leq k.\\
      %
      %
      %
      %% \sigma^{-2} \lambda_i &= 2\alpha - 2\beta_i, i \in B \\
    \end{split}
  \end{align*}
  
  Summing over $i \in A$, substituting $\sum_{i\in A} \eta_i =
  \sum_{i=1}^m \eta_i = m$, and dividing by $k:= |A|$:
  $$
  \frac{1}{2\alpha} = \frac{m}{k} + \frac{\sigma^2}{k} \sum_{j\in A} \lambda_j^{-1}.
  %\Rightarrow \alpha = \left ( \frac{2m}{k} + \frac{2\sigma^2}{k}
  %\sum_{i\in A} \lambda_i^{-1} \right )^{-1}.
  $$

  Consequently:
  \begin{align} \label{eq:etas}
    \begin{split}
      \eta_i &= \frac{1}{2\alpha} - \sigma^2 \lambda_i^{-1}\\
      %
      %
      %
      &=\frac{m}{k} + \frac{\sigma^2}{k} \sum_{j\in A} \lambda_j^{-1} - \sigma^2 \lambda_i^{-1}.%\\
      %
      %
      %
      %% &= \frac{m}{k} + \frac{\sigma^2}{k} \sum_{j \in A, j\neq i} \lambda_j^{-1} - \frac{k-1}{k}\sigma^2 \lambda_i^{-1}\\
      %% ,\ i \in A.
    \end{split}
  \end{align}
\end{proof}







\begin{lemma}[Unit norm decomposition]\label{lemma:free}
  Let $M \in \R^{k \times k}$ symmetric positive definite with $\ttr M
  = m$, $m \geq k$. We can find $\func_j \in \R^k,j=1,\dots,m$
  with $\|\func_j\|=1$ and $A = (\func_1,\dots,\func_m)$ such that
  $AA^t = M$.
\end{lemma}

\begin{proof}
  Let us diagonalize $M$, so that $M = U D U^t$ with $D =
  \diag(d_1,\dots,d_k)$ and $U \in \R^{k \times k }$ orthogonal. Let
  $S \in \R^{k \times m}$ with $S_{ii} = \sqrt{d_{i}}$ and zeros
  otherwise. Define $A:= U S V^t$, where $V \in \R^{m \times m}$ is
  orthogonal and will be further restricted later. Then $AA^t = U
  SV^tVS^t U^t = UDU^t$, so $AA^t$ has the required eigenvalues and
  eigenvectors by construction. If we can choose $V$ such that $A$
  also satisfies the unit norm constraints we are done. These
  constraints are, for $j=1,\dots,m$:
  \begin{equation}\label{eq:V_constraints}
   1 = [A^tA]_{jj} = [V S^tS V^t]_{jj},
  \end{equation}
  and we can expect to do this since we assumed $\ttr D = m$.

  Define $C = S^tS - I \in \R^{m \times m}$. Note that $\ttr C = 0$ and
  $C$ is diagonal with non-zero entries $d_i-1,i=1,\dots,k$. It suffices
  to find $V$ orthogonal such that $V C V^t$ has zero diagonal. We
  construct such $V$ by sequentially inserting zeros in the diagonal
  and not destroying zeros we already introduced, starting from the
  last diagonal entry and moving to the first. Since $c_{mm} \neq 0$ ,
  let $p < m$ such that $c_{pp}c_{mm} < 0$ (such $p$ exists because
  the trace is zero) and let $\theta \in (0,\pi)$. Define a Givens
  rotation $R^{(m)} \in \R^{m \times m}$ by
  \begin{equation*}
    r^{(m)}_{ab} :=
    \begin{cases}
      1 & a = b \neq p \text{ or } a = b \neq m \\
      \cos \theta & a = b = p  \\
     -\sin \theta & a = p, b = m\\
      \cos \theta & a = b = m \\
      \sin \theta & a = m, b = p \\ 
      0 & o.w
    \end{cases}
  \end{equation*}
  Note that conjugating a matrix by $R^{(m)}$ changes only its $m$ and
  $p$ rows and columns. We want to choose $\theta$ such that
  \begin{equation}\label{eq:mm}
    0 = [R^{(m)} C (R^{(m)})^t]_{mm} = \cos^2 \theta c_{mm} + 2\cos \theta \sin
    \theta c_{mp} + \sin^2\theta c_{pp},
  \end{equation}
  and it suffices to choose $\theta$ such that
  \begin{equation*}
    c_{mm} \cot^2 \theta + 2 c_{mp} \cot \theta + c_{pp} = 0.
  \end{equation*}
  This quadratic in $\cot\theta$ has a real solution, since
  $c_{pp}c_{mm} < 0$ by assumption and we can find $\theta \in
  (0,\pi)$ such that \eqref{eq:mm} is satisfied. We continue to find
  $R^{(m-1)}$ that leaves row and column $m$ unchanged and
  continue introducing zeros to the diagonal. The assumption $\ttr D =
  m \Rightarrow \ttr C = 0$ guarantees we can do that. Taking $V:=
  R^{(1)} R^{(2)} \dots R^{(m-1)}R^{(m)}$ completes the proof.
\end{proof}



\section{Lemmas}






\begin{lemma}[Lax, Chapter 9, Theorem 4\cite{Lax07}]\label{lemma:lax}
  Let $Y(t)$ be a differentiable operator-valued function. Assume 
  $I+Y(t)$ is invertible, $Y(t)$ self-adjoint and trace-class. Then
  \begin{equation*}
    \frac{\der \log \det (I+Y(t))}{\der t} = \tr{(I+Y(t))^{-1} \dot{Y}(t)}.
  \end{equation*}
\end{lemma}

\begin{proof}
  Consider a differentiable operator-valued function $X(t)$ such that
  $X(0) = 0$ and $X(t)$ is positive, self-adjoint and trace-class for
  every $t\in \R$. We denote the eigenvalues of this operator by
  $\lambda_k(X(t))$ and sometimes drop the dependence on $X(t)$, so
  $\lambda_k = \lambda_k(X(t))$.  Then $\det (I+X(t)) =
  \prod_{k=1}^{\infty} (1+\lambda_k) < \infty$ and this is finite
  \cite{AlexanderianGloorGhattas14}. The full derivative
  is \begin{align*} \frac{\der \det (I+X(t))}{\der t}
    % 
    % 
    % 
    &= \sum_{k=1}^{\infty} 
    \frac{\partial \det (I+X(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    % 
    %
    %
    &= \sum_{k=1}^{\infty} \frac{\partial \prod_{l=1}^{\infty}
      (1+\lambda_l(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    %
    %
    %
    &= \sum_{k=1}^{\infty} \prod_{l=1, l\neq k}^{\infty}
      (1+\lambda_l(s)) \frac{\partial (1+\lambda_k(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    %
    %
    %    
    &= \sum_{k=1}^{\infty} \frac{\prod_{l=1}^{\infty}
      (1+\lambda_l(s))}{(1+\lambda_k)}\Big |_{s=t}
    \dot{\lambda_k}(X(t)) \\
    % 
    % 
    % 
    &= \sum_{k=1}^{\infty} \frac{\det (I+X(t))}{1 +\lambda_k} \dot{\lambda_k}(X(t)).
  \end{align*}
  The assumption $X(0) = 0$ means $\lambda_k(X(0)) = 0,\ \forall k \geq 1$. Thus:
  \begin{align*}
    \frac{\der (I+\det X(t))}{\der t}\Big |_{t=0} 
    = \sum_{k=1}^{\infty} \dot{\lambda_k}(X(0)) 
    = \frac{\der }{\der t}\tr{X(0)}
    = \tr{\dot{X}(0)},
  \end{align*}
  where the second equality follows by monotone convergence. 
  Let $Y(t)$ a trace-class self-adjoint operator such that 
  $I+Y(t)$ is invertible.
  Define $X(t)$ via $I+X(t) = (I+Y(0))^{-1/2} (I+Y(t)) (I+Y(0))^{-1/2}$. 
  We show $X(t)$ satisfies the conditions above. It is trace-class:
  \begin{align*}
    \tr{X(t)} = \tr{(I+Y(0))^{-1} (I+Y(t)) - I}
    \leq \tr{I+Y(t) - I}< \infty,
  \end{align*}
  since $Y(t)$ is trace-class. It is also clear that
  $X(0) = 0$ and $X(t)$ is self-adjoint.
  $I+Y(t) = (I+Y(0))^{1/2}(I+X(t))(I+Y(0))^{1/2}$, so
  \begin{align*}
    \frac{\der \det (I+Y(t))}{\der t}|_{t=0} 
    &= \det (I+Y(0))\frac{\der \det (I+X(t))}{\der t}\Big |_{t=0} \\
    % 
    % 
    % 
    &= \det (I+Y(0)) \tr{\dot{X}(0)} \\
    % 
    % 
    % 
    &= \det (I+Y(0)) \tr{(I+Y(0))^{-1} \dot{Y}(0)}.
  \end{align*}
  Consequently, by the one-variable chain rule:
  \begin{align*}
    \frac{\der \log \det (I+Y(t))}{\der t}\Big |_{t=0} &=
    % 
    % 
    % 
    \frac{1}{\det (I+Y(0))}\frac{\der \det (I+Y(t))}{\der t}\Big |_{t=0} \\ 
    % 
    % 
    % 
    &= \tr{ (I+Y(t))^{-1} \dot{Y}(t)} \big |_{t=0}.
  \end{align*}
  There is nothing special about $t_0 = 0$ --- we could have chosen
  any other $t_0$ instead. Thus, the relation holds for all $t$.
\end{proof}

The following is a generalization of the Matrix Determinant Lemma to
Hilbert spaces.
\begin{lemma}%\label{lemma:MDL}
  Let $\hil$ a separable Hilbert space, $u,v\in \hil$ and $A: \hil \to
  \hil$ an invertible linear operator such that $\tr{A-I} <
  \infty$. Then $\det A$ and $\det A + uv^*$ are well defined and
  \begin{equation*}
    \det (A + uv^*) = (1 + \langle A^{-1} u, v \rangle ) \det A,
  \end{equation*}
  where $(A + uv^*)w := Aw + \langle v,w \rangle u$.
\end{lemma}
\begin{proof}
  In this proof we rely on common definitions and results
  \cite{simon1977}. First, consider $B := I + xy^*$ for some $x,y \in
  \hil$. We construct an eigenbasis for $B$ and use that to show $\det
  B = 1 + \langle x, y \rangle$. First let $x_1 := x$.  Now, if $x
  \parallel y$, take $\{x_n \}_{n=2}^{\infty}$ an orthogonal basis for
  $span\{x_1\} ^{\perp}$. If, on the other hand, $x \not \parallel y$,
  let
  \begin{equation*}
    x_2 := x - \frac{ \langle x, y\rangle}{\|y\|^2}y
  \end{equation*}
  and it is easy to verify that $x_2 \perp y$ and $span \{x,y\} = span
  \{x_1,x_2\}$. Take $\{x_n \}_{n=3}^{\infty}$ an orthogonal basis for
  $span\{x_1,x_2\} ^{\perp}$. In both cases,
  \begin{equation*}
    B x_n =
    \begin{cases}
      (1 + \langle x, y \rangle) x_n & n = 1 \\
      x_n                            & n \neq 1,
    \end{cases}
  \end{equation*}
  and so $\det B = 1 + \langle x, y \rangle$.
  
  It is easy to verify that $uv^*$ is trace-class and since $\tr{A-I}
  < \infty$, also $\tr{A + uv^* - I} < \infty$ (sum of two trace-class
  operators is trace-class). Thus $\det A$ and $\det (A+uv^*)$ are
  well defined. Let $x:=A^{-1}u$ and $y := v$:
  \begin{equation*}
    \det (A + uv^*) = \det A \ \det(I+A^{-1}uv^*) =
    (1 + \langle A^{-1}u, v \rangle) \det A .
  \end{equation*}
\end{proof}


%\bibliographystyle{siamplain}
\bibliographystyle{amsplain}
\bibliography{/Users/yairdaon/work/lib.bib}

\end{document}
