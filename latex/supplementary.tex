\documentclass{siamonline220329}
%\documentclass{amsart}
%%\documentclass{article}
\input{definitions}

\title{Supplementary Material for: Measurement Clusterization in
  D-optimal Designs for Bayesian Linear Inverse Problems over Hilbert
  Spaces}

\author{Yair Daon\thanks{Porter School of the Environment and Earth
    Sciences, Tel Aviv University, Tel Aviv, Israel
    (\email{yair.daon@gmail.com}).}}
%% \author{Yair Daon}

%% \thanks{Azrieli Faculty of Medicine, Bar Ilan University, Safed,
%%   Israel.  (\email{yair.daon@gmail.com}).}
\date{\today}
\begin{document}
\maketitle


\section{The inverse problem of the 1D heat equation}\label{section:example}
The mathematical foundations of inversion in function spaces can be
found in \cite{Stuart10}. Our goal is to infer the initial condition of
the one-dimensional heat equation with a homogeneous Dirichlet
boundary condition:
\begin{subequations}\label{eq:heat equation}
  \begin{alignat}{2}
    u_t &= \Delta u &&\qquad \text{in } [0,1] \times [0,\infty),\\
      u &= 0 &&\qquad \text{on } \{0, 1\} \times [0,\infty),\\
        u &= u_0 &&\qquad \text{on }[0,1] \times \{0\}.
  \end{alignat}
\end{subequations}

Model the prior for the initial condition as $u_0 \sim
\normal(0,\prcov)$, for $\prcov = (-\Delta)^{-1}$ with a homogeneous
Dirichlet boundary condition. Let the final time $T = 0.03$ and denote
$\fwd$ the forward operator, so that $u(\cdot, T) = \fwd u_0$. Let
$\obs$ a measurement operator, so that $u(\x_j,T) = (\obs u)_j,
j=1,\dots,m$, where $m$ is the number of allowed measurements. Assume
iid centered Gaussian observation error, so observations are given by
$u(\x_j,T) + \eps(\x_j)$ with $\eps(\x_j) \sim \normal(0, \sigma^2)$
iid, $\sigma = 0.05$. Measurement locations $\x_j,j=1,\dots,m$ are
chosen according to the D-optimality criterion of
\cite{AlexanderianGloorGhattas14}.

Implementation of this inverse problem can be found in the
repository \url{https://github.com/yairdaon/OED}, with an optimization
routine for finding D-optimal designs. Unfortunately, I was not able
to install PyOED \cite{attia2023pyoed}, nor was I able to utilize
hIPPYlib \cite{VillaPetraGhattas16, VillaPetraGhattas18,
VillaPetraGhattas21}, hence the inverse problem is implemented from
scratch.

\section{Generalizations of known lemmas}

The following lemma is generalized from \cite[Chapter 9, Theorem 4,
p. 127]{Lax07}
\begin{lemma}\label{lemma:lax}
  Let $Y(t)$ be a differentiable operator-valued function. Assume 
  $I+Y(t)$ is invertible, $Y(t)$ self-adjoint and trace-class. Then
  \begin{equation*}
    \frac{\der \log \det (I+Y(t))}{\der t} = \tr{(I+Y(t))^{-1} \dot{Y}(t)}.
  \end{equation*}
\end{lemma}

\begin{proof}
  Consider a differentiable operator-valued function $X(t)$ such that
  $X(0) = 0$ and $X(t)$ is positive, self-adjoint and trace-class for
  every $t\in \R$. We denote the eigenvalues of this operator by
  $\lambda_k(X(t))$ and sometimes drop the dependence on $X(t)$, so
  $\lambda_k = \lambda_k(X(t))$.  Then $\det (I+X(t))
  = \prod_{k=1}^{\infty} (1+\lambda_k) < \infty$ and this is finite by
  the arguments given in \cite{AlexanderianGloorGhattas14}. The full
  derivative is \begin{align*} \frac{\der \det (I+X(t))}{\der t}
    % 
    % 
    % 
    &= \sum_{k=1}^{\infty} 
    \frac{\partial \det (I+X(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    % 
    %
    %
    &= \sum_{k=1}^{\infty} \frac{\partial \prod_{l=1}^{\infty}
      (1+\lambda_l(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    %
    %
    %
    &= \sum_{k=1}^{\infty} \prod_{l=1, l\neq k}^{\infty}
      (1+\lambda_l(s)) \frac{\partial (1+\lambda_k(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    %
    %
    %    
    &= \sum_{k=1}^{\infty} \frac{\prod_{l=1}^{\infty}
      (1+\lambda_l(s))}{(1+\lambda_k)}\Big |_{s=t}
    \dot{\lambda_k}(X(t)) \\
    % 
    % 
    % 
    &= \sum_{k=1}^{\infty} \frac{\det (I+X(t))}{1 +\lambda_k} \dot{\lambda_k}(X(t)).
  \end{align*}
  The assumption $X(0) = 0$ means $\lambda_k(X(0)) = 0,\ \forall k \geq 1$. Thus:
  \begin{align*}
    \frac{\der (I+\det X(t))}{\der t}\Big |_{t=0} 
    = \sum_{k=1}^{\infty} \dot{\lambda_k}(X(0)) 
    = \frac{\der }{\der t}\tr{X(0)}
    = \tr{\dot{X}(0)},
  \end{align*}
  where the second equality follows by monotone convergence. 
  Let $Y(t)$ a trace-class self-adjoint operator such that 
  $I+Y(t)$ is invertible.
  Define $X(t)$ via $I+X(t) = (I+Y(0))^{-1/2} (I+Y(t)) (I+Y(0))^{-1/2}$. 
  We show $X(t)$ satisfies the conditions above. It is trace-class:
  \begin{align*}
    \tr{X(t)} = \tr{(I+Y(0))^{-1} (I+Y(t)) - I}
    \leq \tr{I+Y(t) - I}< \infty,
  \end{align*}
  since $Y(t)$ is trace-class. It is also clear that
  $X(0) = 0$ and $X(t)$ is self-adjoint.
  $I+Y(t) = (I+Y(0))^{1/2}(I+X(t))(I+Y(0))^{1/2}$, so
  \begin{align*}
    \frac{\der \det (I+Y(t))}{\der t}|_{t=0} 
    &= \det (I+Y(0))\frac{\der \det (I+X(t))}{\der t}\Big |_{t=0} \\
    % 
    % 
    % 
    &= \det (I+Y(0)) \tr{\dot{X}(0)} \\
    % 
    % 
    % 
    &= \det (I+Y(0)) \tr{(I+Y(0))^{-1} \dot{Y}(0)}.
  \end{align*}
  Consequently, by the one-variable chain rule:
  \begin{align*}
    \frac{\der \log \det (I+Y(t))}{\der t}\Big |_{t=0} &=
    % 
    % 
    % 
    \frac{1}{\det (I+Y(0))}\frac{\der \det (I+Y(t))}{\der t}\Big |_{t=0} \\ 
    % 
    % 
    % 
    &= \tr{ (I+Y(t))^{-1} \dot{Y}(t)} \big |_{t=0}.
  \end{align*}
  There is nothing special about $t_0 = 0$ --- we could have chosen
  any other $t_0$ instead. Thus, the relation holds for all $t$.
\end{proof}

The following is a generalization of the Matrix Determinant Lemma to
Hilbert spaces.
\begin{lemma}%\label{lemma:MDL}
  Let $\hil$ a separable Hilbert space, $u,v\in \hil$ and $A: \hil \to
  \hil$ an invertible linear operator such that $\tr{A-I} <
  \infty$. Then $\det A$ and $\det A + uv^*$ are well defined and
  \begin{equation*}
    \det (A + uv^*) = (1 + \langle A^{-1} u, v \rangle ) \det A,
  \end{equation*}
  where $(A + uv^*)w := Aw + \langle v,w \rangle u$.
\end{lemma}
\begin{proof}
  In this proof we rely on definitions and results from
  \cite{simon1977}. First, consider $B := I + xy^*$ for some $x,y \in
  \hil$. We construct an eigenbasis for $B$ and use that to show $\det
  B = 1 + \langle x, y \rangle$. First let $x_1 := x$.  Now, if $x
  \parallel y$, take $\{x_n \}_{n=2}^{\infty}$ an orthogonal basis for
  $span\{x_1\} ^{\perp}$. If, on the other hand, $x \not \parallel y$, let
  \begin{equation*}
    x_2 := x - \frac{ \langle x, y\rangle}{\|y\|^2}y
  \end{equation*}
  and it is easy to verify that $x_2 \perp y$ and $span \{x,y\} = span
  \{x_1,x_2\}$. Take $\{x_n \}_{n=3}^{\infty}$ an orthogonal basis for
  $span\{x_1,x_2\} ^{\perp}$. In both cases,
  \begin{equation*}
    B x_n =
    \begin{cases}
      (1 + \langle x, y \rangle) x_n & n = 1 \\
      x_n                            & n \neq 1,
    \end{cases}
  \end{equation*}
  and so $\det B = 1 + \langle x, y \rangle$.
  
  It is easy to verify that $uv^*$ is trace-class and since $\tr{A-I}
  < \infty$, also $\tr{A + uv^* - I} < \infty$ (sum of two trace-class
  operators is trace-class). Thus $\det A$ and $\det (A+uv^*)$ are
  well defined. Let $x:=A^{-1}u$ and $y := v$:
  \begin{equation*}
    \det (A + uv^*) = \det A \ \det(I+A^{-1}uv^*) =
    (1 + \langle A^{-1}u, v \rangle) \det A .
  \end{equation*}
\end{proof}


\bibliographystyle{siamplain}
%% %%\bibliographystyle{amsplain}
\bibliography{/home/yair/projects/library.bib}

\end{document}
