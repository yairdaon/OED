%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION Analysis of Optimal Designs --- Vanishing Model Error
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Characterization of D-Optimal Designs with Vanishing Model Error}\label{section:vanishing}
In this section we analyze D-optimal designs when no model error is
present. We first derive a nonlinear eigenvalue problem in $\obs$ for
D-optimal designs, and then we characterize D-optimal designs as ones
that make all eigenvalues equal in the nonlinear eigenvalue
problem. Results are summarized in Theorem \ref{thm:char}.


%% \subsection{The Nonlinear Eigenvalue Problem}\label{subsec:eigenproblem}
$\modcov = 0$ implies $\Sigma= \sigma^2I$. The necessary first-order
condition for D-optimality \eqref{eq:conditions} becomes
\begin{equation}\label{eq:eigenproblem}
  \sigma^{-2}\fwd \postcov \fwd^* \obs^* = \obs^* \Xi,
\end{equation}
with $\Xi$ diagonal. We think of \eqref{eq:eigenproblem} as an
eigenvalue problem for the self-adjoint operator $\sigma^{-2}\fwd
\postcov \fwd^*$, where rows of $\obs$, namely $\meas_j,j=1,\dots, m$,
are eigenvectors. However, that $\postcov$ depends on $\obs$ and thus,
we refer to \eqref{eq:eigenproblem} as a nonlinear eigenvalue problem.

The following two lemmas allow us to gain better understanding of the
nonlinear eigenvalue problem.

\begin{restatable*}[Two applications of Woodbury Matrix Identity]{lemma}{woodbury}\label{lemma:twice woodbury}
  Assume $\fwd \prcov \fwd^*$ is invertible. Then
  \begin{align*}
    \begin{split}
      \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* \obs^* \obs \fwd )^{-1} \fwd^* 
      %
      %
      = \left ( (\fwd\prcov\fwd^*)^{-1} + \sigma^{-2}  \obs^* \obs \right )^{-1},
    \end{split}
  \end{align*}  
\end{restatable*}


%% The term $(\fwd \prcov \fwd^*)^{-1}$ is the prior precision in
%% $\hilo$, and $\fwd\prcov\fwd^*$ is invertible, as mentioned in
%% Section \ref{section:prelim}. $(\fwd\prcov\fwd^*)^{-1} + \sigma^{-2}
%% \obs^* \obs$ is the posterior precision in $\hilo$.

%% \subsection{Characterizing D-Optimal Designs}\label{subsec:characterization}
%% %% Denote corresponding eigenvectors $\{\ev_i\}_{i=1}^{\infty}$.
%% Denote eigenvalues of $\fwd \prcov \fwd^*$ by
%% $\{\lambda_i\}_{i=1}^{\infty}$, and note they are positive. Lemma
%% \ref{lemma:sim diag} is crucial for the rest of the analysis:

\begin{restatable*}[Simultaneous diagonizability for the nonlinear eigenvalue problem]{lemma}{simdiag}\label{lemma:sim diag}
  Let $\hil$ separable Hilbert space, $C:\hil \to \hil$ self-adjoint
  and $\func_1,\dots,\func_m \in \hil$. Denote $\func^*$ the element
  $\func$ acting as a linear functional. If
  \begin{equation*}
   (C + \sum_{j=1}^m \func_j\func_j^*) \func_l = \xi_l \func_l,\ l = 1,\dots,m
  \end{equation*}
  then $C$ and $\sum_{j=1}^m \func_j \func_j^*$ are simultaneously
  diagonalizable.
\end{restatable*}

\begin{corollary}[Eigenvectors of a D-optimal design]\label{cor:same ev}
  Let $\obs$ satisfy the nonlinear eigenvalue problem
  \eqref{eq:eigenproblem}. Then $\obs^*\obs$ has the same eigenvectors
  as $\fwd \prcov \fwd^*$.
\end{corollary}
\begin{proof}
  \begin{align}\label{eq:mod conditions}
    \begin{split}
      \obs^* \Xi &= \sigma^{-2}\fwd \postcov \fwd^* \obs^*  \text{ (by \eqref{eq:eigenproblem})}\\
      %
      %
      %
      &= \sigma^{-2} \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* \obs^* \obs \fwd )^{-1} \fwd^* \obs^*  \text{ (by \eqref{eq:postcov})} \\
      %
      %
      %
      &= \sigma^{-2} \left ( (\fwd\prcov\fwd^*)^{-1} + \sigma^{-2}  \obs^* \obs \right )^{-1} \obs^* \text{ (by Lemma \ref{lemma:twice woodbury})}.
    \end{split}
  \end{align}

  Now take $\func_j^{*} = \meas_j$ and $C := (\fwd \prcov
  \fwd^*)^{-1}$ and use Lemma \ref{lemma:sim diag}.
\end{proof}

Corollary \ref{cor:same ev} implies we only need to consider $\obs$
such that at most $m$ of the eigenvectors of $\obs^*\obs$ have a
positive eigenvalue, because $k := \rank \obs^*\obs \leq m$. Since we
made no assumption regarding the ordering of $\{\lambda_i\}$, we can
denote the corresponding non-zero eigenvalues of $\obs^*\obs$ by
$\{\eta_i\}_{i=1}^{k}$ and let $\eta_i = 0$ for $i \geq k+1$.

%% \begin{definition}
%%   Let $\{\lambda_i\}_{i=1}^{\infty}$ eigenvalues of $\fwd \prcov
%%   \fwd^*$ sorted in descending order. Let $\eta \in
%%   \mathbb{R}_+^m$. The \emph{utility proxy} $\bar{\tar}$ is defined
%%   as:
%%   \begin{equation*}
%%     \bar{\tar}(\eta) := \frac12 \sum_{i=1}^m \log(1 + \sigma^{-2}\lambda_i\eta_i).
%%   \end{equation*}
%% \end{definition}

\begin{corollary}\label{cor:true target}
  Let $\obs$ with $m$ observations satisfy the nonlinear eigenvalue
  problem \eqref{eq:eigenproblem}. Let $\{\eta_i\}_{i=1}^{\infty}$
  eigenvalues of $\obs^*\obs$ and $\{\lambda_i\}_{i=1}^{\infty}$ the
  corresponding eigenvalues of $\fwd \prcov \fwd^*$. Let $k:=\rank
  \obs^*\obs$. Without loss of generality, let $\eta_i > 0$ for $i\leq
  k$ and $\eta_i = 0$ for $i > k$. Then:
  \begin{enumerate}
    \item $k \leq m$ and $\obs^*\obs$ has exactly $k$ positive
      eigenvalues.
    \item
      \begin{equation*}
        \tar(\obs) = \frac12 \sum_{i=1}^{k} \log (1 + \sigma^{-2}\lambda_i\eta_i) = \frac12 \sum_{i=1}^{m} \log (1 + \sigma^{-2}\lambda_i\eta_i).
      \end{equation*}
    \item Let $\obs$ a D-optimal design. Then $\eta_i > 0$ for
      eigenvectors corresponding to the $k$ largest $\lambda_i$.
  \end{enumerate}
\end{corollary}
\begin{proof}
  Part (1) is trivial. To see part (2) holds: 
  \begin{align}
    \begin{split}
      \tar(\obs) &= \frac12\log \det \left (I + \sigma^{-2} \prcov^{1/2} \fwd ^* \obs^*
      \obs \fwd \prcov^{1/2}\right )\\% \text{ (by definition)}\\
      %
      &= \frac12 \log \det \left (I + \sigma^{-2} \obs^* \obs \fwd
      \prcov\fwd^* \right ) \text{ (Sylvester's Determinant
      Theorem)}\\
      %
      %
      %
      &=\frac12 \log \prod_{i=1}^{\infty} ( 1 + \sigma^{-2} \lambda_i\eta_i ) \text{ (Corollary \ref{cor:same ev})} \\
      %
      %
      %
      %&=\frac12 \log \left ( \prod_{i=1}^{k} ( \lambda_i^{-1} + \sigma^{-2} \eta_i )\prod_{i=1}^{k} \lambda_i \right )
      %
      %
      %
      &=\frac12 \sum_{i=1}^{k} \log (1 + \sigma^{-2}\lambda_i\eta_i). 
      %
      %
      %% &=\frac12 \sum_{i=1}^{k} \log(\sigma^2\lambda_i^{-1} + \eta_i) - k\log \sigma + \frac12\sum_{i=1}^k \log \lambda_i.
    \end{split}
  \end{align}
  Part (3) holds since $\log$ is increasing and $\eta_i \geq 0$.
\end{proof}


\begin{proposition}\label{prop:kkt}
  Let $\tar: \mathbb{R}^m \to \mathbb{R}$, $\psi(\eta) =
  \frac{1}{2}\sum_{i=1}^m \log (1+\sigma^{-2}\lambda_i \eta_i)$, with
  $\lambda_i > 0$ and $\sigma^{2} > 0$. Then the maximum of $\tar$
  subject to $\eta_i \geq 0$ and $\sum\eta_i = m$ is obtained at
  \begin{equation}
  \eta_i = \begin{cases}
    \frac{m}{k} - \sigma^2 \lambda_i^{-1} + \sigma^2 \frac{1}{k} \sum_{j\in A} \lambda_j^{-1} & i \in A \\
    0 & i \in A^c
  \end{cases}
  \end{equation}
  where $A:= \{1\leq i \leq m: \eta_i > 0\}$ and $A^c = \{1,\dots, m\}
    \backslash A$, and $|A|$ is the cardinality of $A$.
\end{proposition}
\begin{proof}
%% \begin{proposition}\label{prop:optimizer}
%%   Let $\{\lambda_i\}_{i=1}^{\infty}$ the sorted eigenvectors of $\fwd
%%   \prcov\fwd^*$, $k\in \mathbb{N}^+$ and
%%   %% Let
%%   %%   \begin{equation}
%%   %%   k = \argmax \left \{ k:\lambda_k^{-1} \leq \sigma^{-2}\frac{m}{k} + \frac{1}{k} \sum_{j=1}^{k}
%%   %%   \lambda_j^{-1} \right \}.
%%   %% \end{equation}

%%   \begin{equation}
%%   \eta_i = \begin{cases}
%%     \frac{m}{k} - \sigma^2 \lambda_i^{-1} + \sigma^2 \frac{1}{k} \sum_{j=1}^k \lambda_j^{-1} & i \leq k \\
%%     0 & i > k 
%%   \end{cases}.
%%   \end{equation}

%%   Let $\obs$ an observation operator such that $\obs^*\obs$ has the
%%   same eigenvectors as $\fwd \prcov \fwd^*$. If $\eta_i > 0$ for
%%   $i\leq k$ and $\obs^*\obs$ has eigenvalues $\eta_i$, then $\obs$
%%   maximizes $\tar$ subject to $\sum_{i=1}^m \eta_i = m$ and to $\rank
%%   \obs = k$.
%% \end{proposition}
  %% \begin{proof}
  %% et $A := \{i: \eta_i > 0\}$. A choice of $A$ where $\eta_i =0,
  %% \eta_j >0$ for some $1 \leq i < j \leq m$ is not optimal (just
  %% switch $\eta_j$ and $\eta_i$ and recall $\{\lambda_i\}$ are arranged
  %% in descending order). Thus, since $\rank \obs = k$ we conclude that,
  %% $\eta_i=0, i>k$.

  Let $\Phi(\eta) = \sum_{i=1}^k \eta_i - m$ and $\Omega_j(\eta) =
   -\eta_j$. Then
  \begin{align*}
    \begin{split}
      \frac{\partial \tar}{\partial \eta_i}  &=
       \frac12 \frac{\sigma^{-2}\lambda_i}{1 + \sigma^{-2} \lambda_i\eta_i} = \frac12 \frac{1}{\sigma^{2}\lambda_i^{-1} + \eta_i} \\
      %
      %
      %
      \frac{\partial\Phi}{\partial \eta_i} &= 1 \\
      %
      %
      %
      \frac{\partial \Omega_j}{\partial \eta_i} &= -\delta_{ij}      
    \end{split}
  \end{align*}

  From the KKT conditions, there are $\alpha, \beta_i$ such that for $i=1,\dots,m$:
  \begin{align}
    \begin{split}
      -\frac12 \frac{1}{\sigma^{2}\lambda_i^{-1} + \eta_i} + \alpha - \beta_i  &= 0 \\
      %
      %
      %
      \eta_i &\geq 0\\
      %
      %
      %
      \beta_i &\geq 0\\
      %
      %
      %
      \beta_i \eta_i &= 0\\
      %
      %
      %
      \sum_{i=1}^m \eta_i &= m 
    \end{split}
  \end{align}

  Then, for $i \in A$:
  \begin{align*}
    \begin{split}
      \beta_i &= 0\\
      %
      %
      %
      \sigma^{2}\lambda_i^{-1} + \eta_i  &= \frac{1}{2\alpha}%  \text{ for }  1 \leq i \leq k.\\
      %
      %
      %
      %% \sigma^{-2} \lambda_i &= 2\alpha - 2\beta_i, i \in B \\
    \end{split}
  \end{align*}
  
  Summing over $i \in A$, substituting $\sum_{i\in A} \eta_i =
  \sum_{i=1}^m \eta_i = m$, and dividng by $k:= |A|$:
  $$
  \frac{1}{2\alpha} = \frac{m}{k} + \frac{\sigma^2}{k} \sum_{j\in A} \lambda_j^{-1}.
  %\Rightarrow \alpha = \left ( \frac{2m}{k} + \frac{2\sigma^2}{k}
  %\sum_{i\in A} \lambda_i^{-1} \right )^{-1}.
  $$

  Consequently:
  \begin{align} \label{eq:etas}
    \begin{split}
      \eta_i &= \frac{1}{2\alpha} - \sigma^2 \lambda_i^{-1}\\
      %
      %
      %
      &=\frac{m}{k} + \frac{\sigma^2}{k} \sum_{j\in A} \lambda_j^{-1} - \sigma^2 \lambda_i^{-1}.%\\
      %
      %
      %
      %% &= \frac{m}{k} + \frac{\sigma^2}{k} \sum_{j \in A, j\neq i} \lambda_j^{-1} - \frac{k-1}{k}\sigma^2 \lambda_i^{-1}\\
      %% ,\ i \in A.
    \end{split}
  \end{align}
\end{proof}

\begin{corollary}\label{cor:descending}
  In Proposition \ref{prop:kkt}, if $\lambda_i$ are arranged in
  descending order, then $A= \{1,\dots,k\}$ for some $1 \leq k \leq
  m$.
\end{corollary}

\begin{corollary}\label{cor:eigenvalues}
  If $\obs$ is optimal and $k := \rank \obs$, then:
  \begin{enumerate}
    \item The covariance of the pushforwad $\fwd_{*} \post$ is $((\fwd
      \prcov \fwd^*)^{-1} + \sigma^{-2} \obs^*\obs)^{-1}$.
    \item The eigenvalues of said operator are
      \begin{equation}
        \eta_i = \begin{cases}
          \frac{m}{k} + \sigma^2 \frac{1}{k} \sum_{j=1}^k \lambda_j^{-1} & i \leq k \\
          \lambda_i^{-1} & i > k 
        \end{cases}.
      \end{equation}
  \end{enumerate}
\end{corollary}

\begin{proof}
  Recall from \eqref{eq:postcov}, that the posterior covariance is
  $\postcov^{-1} = \prcov^{-1} +
  \sigma^{-2}\fwd^*\obs^*\obs\fwd$. Part (1) now follows from Lemma
  \ref{lemma:twice woodbury}. Part (2) follows from Proposition
  \ref{prop:kkt} and Corollary \ref{cor:descending}.
\end{proof}

We are almost ready to state and prove Theorem \ref{thm:char} that
characterizes D-optimal designs. We just need one final lemma, whose
proof is delegated to the appendix:
\begin{restatable*}[Unit norm decomposition]{lemma}{free}\label{lemma:free}
  Let $M \in \R^{k \times k}$ symmetric positive definite with $\ttr M
  = m$, $m \geq k$. We can find $\func_j \in \R^k,j=1,\dots,m$
  with $\|\func_j\|=1$ and $A = (\func_1,\dots,\func_m)$ such that
  $AA^t = M$.
\end{restatable*}


\main

\begin{proof}
  Part (1) was proved in Corollary \ref{cor:same ev}. Parts (2) and
  (3) were proved in Corollary \ref{cor:true target}. Part (4) is a
  consequence of Corollary \ref{cor:eigenvalues}, with the caveat that
  we did not show that finding $\obs$ so that $\obs^*\obs$ has the
  desired eigenvalues is feasible. To this end, we utilize Lemma
  \ref{lemma:free}: let $M = \diag(\eta_1, \dots, \eta_k)$, diagonal
  with respect to the first $k$ eigenvectors of $\fwd \prcov
  \fwd^*$. We take $\obs := A$ from the Lemma \ref{lemma:free}, and
  this concludes the proof.
\end{proof}

\begin{corollary}\label{cor:equal eigs}
  In a D-optimal design, the posterior covariance of the pushforward
  $\fwd_{*} \post$ is constant for eigenvectors $1 \leq i \leq k$
  where $k =\rank \obs$.
\end{corollary}

\pgfplotstableread{
  Label     prior  optimal  sub-optimal 
  1         0.2    1.8           1.7
  2         0.8    1.2           0.8
  3         2.2    0             0.5
  4         3.5    0             0.0
}\optimalvsnot

\begin{figure}
  \begin{tikzpicture}[scale=0.85]
    \begin{axis}[
        ybar stacked,
        ymin=0,
        ymax=4,
        xtick=data,
        legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
        reverse legend=false, % set to false to get correct display, but I'd like to have this true
        xticklabels from table={\optimalvsnot}{Label},
        xticklabel style={text width=2cm,align=center},
        legend plot pos=right,
        ylabel=precision --- prior and posterior,
        xlabel=eigenvector $i$,
      ]
    
      
      \addplot [fill=green!80]  table [y=prior,   meta=Label, x expr=\coordindex] {\optimalvsnot};
      \addplot [fill=blue!60]   table [y=optimal, meta=Label, x expr=\coordindex] {\optimalvsnot};
            
      \addlegendentry{$\sigma^2\lambda_i^{-1}$}
      \addlegendentry{optimal $\eta_i$s}
      
    \end{axis}
  \end{tikzpicture}
  \qquad
  \begin{tikzpicture}[scale=0.85]
    \begin{axis}[
        ybar stacked,
        ymin=0,
        ymax=4,
        xtick=data,
        legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
        reverse legend=false, % set to false to get correct display, but I'd like to have this true
        xticklabels from table={\optimalvsnot}{Label},
        xticklabel style={text width=2cm,align=center},
        legend plot pos=right,
        ylabel=precision --- prior and posterior,
        xlabel=eigenvector $i$,
      ]   
      
      \addplot [fill=green!80]  table [y=prior,       meta=Label, x expr=\coordindex] {\optimalvsnot};
      \addplot [fill=blue!60]   table [y=sub-optimal, meta=Label, x expr=\coordindex] {\optimalvsnot};

      \addlegendentry{$\sigma^2\lambda_i^{-1}$}
      \addlegendentry{sub-optimal $\eta_i$s}
      
    \end{axis}
  \end{tikzpicture}
  \caption{As an illustration of Corollary \ref{cor:equal eigs}, a
    D-optimal design is compared with a sub-optimal design. Both
    designs are allowed three observations. Thus, the blue area has
    accumulated height of $3$ in both plots. A D-optimal design (left)
    increases precision (reduces uncertainty) where it is lowest. A
    sub-optimal design (right) does not. In this example, an
    observation is taken partly in the direction of the third
    eigenvector. The precision for this eigenvector is bigger even
    than the posterior precision of the D-optimal design on the left.}
  \label{fig:optimal vs not}
\end{figure}


Here is a way to understand D-optimal designs with vanishing model
error: Imagine each eigenvector corresponds to a graduated lab
cylinder (Figure \ref{fig:cylinder}). Each cylinder $i$ is filled,
a-priori, with green liquid of $\sigma^2\lambda_i^{-1}$ volume
units. This liquid represnets the prior precision, weighted by
observation noise. The volume of the green liquid is increasing in
$i$. A D-optimal design with $m$ observations ignores cylinders
indexed $m+1$ and above. It has blue liquid of volume $m$ units at its
disposal. It distributes said liquid by repeatedly adding a drop to
whatever cylinder currently has the lowest level of liquid in it
(lowest precision). The result of such a procedure is illustrated in
Figure \ref{fig:optimal vs not}.

\begin{figure}%{r}{0.25\textwidth} 
    %\centering
    \includegraphics[width=5cm, height=5cm]{cylinders.jpg}
    \caption{Three graduated lab cylinders, corresponding to three
      eigenvectors. Prior eigenvalues not shown.}
    \label{fig:cylinder}
\end{figure}



\section{Clusterization poses no obstruction to D-optimality}\label{section:clusterization}
Corollary \ref{cor:same ev} tells us that we can realize any design we
want (encoded in $\{\eta_i\}$), given the trace constraint
$\tr{\obs^*\obs} = m$ is satisfied. From Theorem \ref{thm:char} we
know we only need to make the posterior flat where we take
observations (recall figure \ref{fig:optimal vs not}). We are left
with the freedom of assigning any part of $\eta_i$ to any observation
$\meas_j$. Two possible assignments for the same problem are
illustrated in figure \ref{fig:clusterization}. The first exhibits
clusterization, the second does not. It is important to note that
neither arises as a result of some numerical optimization scheme. Both
designs give the same design criterion $\tar$, because their
posteriors are identical. By Theorem \ref{thm:char}, both are
maximizers of the same D-optimal design problem. One exhibits
clusterization, and one does not. Of course, a real-life problem is
more restricted than the model we consider here. Such restriction is
the likely cause of clusterization obseved in, e.g., the toy model of
section \ref{subsec:example}. Exactly how this happens is not
clear. At this point it is the subject of speculation and should be
further studied in the future.

A computer implementation of Lemma \ref{lemma:free} (see code in
Supplementary material) generates the design presented in left panel
Figure \ref{fig:clusterization}, which exhibits clusterization, over
the clusterization-free design presented in the right panel of Figure
\ref{fig:clusterization}. Modifying the numerical values of the prior
does not result in a clusterization-free design. Thus, the preference
to clusterization of Lemma \ref{lemma:free} is robust and
clusterization is a generic property of the model presented in this
study.

\pgfplotstableread{
  Label    prior  $o_1o_2$  $o_3$   $o_4$   $o_5$    topper
  1         0.1      0.55       1       0       1      0.001
  2         0.2      1.45       0       1       0      0.001
  3         3.5      0          0       0       0      0.001
}\clusterization


\pgfplotstableread{
  Label    prior  $o_1o_2$  $o_3o_4$   $o_5$     topper
  1         0.1       0.95        0.6        1         0.001
  2         0.2       1.0         1.4        0.        0.001
  3         3.5        0          0          0         0.001
}\noclusterization

\begin{figure}
  \begin{tikzpicture}[scale=0.85]
    \begin{axis}[
        ybar stacked,
        ymin=0,
        ymax=4,
        xtick=data,
        legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
        reverse legend=false, % set to false to get correct display, but I'd like to have this true
        xticklabels from table={\clusterization}{Label},
        xticklabel style={text width=2cm,align=center},
        legend plot pos=right,
        ylabel=precision --- prior and posterior,
        xlabel=eigenvector $i$,
      ]
    
      
      \addplot [fill=green!80]  table [y=prior, meta=Label, x expr=\coordindex] {\clusterization};
      \addplot [fill=blue!60]   table [y=$o_1o_2$, meta=Label, x expr=\coordindex] {\clusterization};
      \addplot [fill=red!60]    table [y=$o_3$, meta=Label, x expr=\coordindex] {\clusterization};
      \addplot [fill=black!60]  table [y=$o_4$, meta=Label, x expr=\coordindex] {\clusterization};
      \addplot [fill=orange!60] table [y=$o_5$, meta=Label, x expr=\coordindex] {\clusterization};
      %% \addplot [fill=cyan!60]   table [y=$o_6$, meta=Label, x expr=\coordindex] {\clusterization};
      %% \addplot [fill=purple!60] table [y=$o_7$, meta=Label, x expr=\coordindex] {\clusterization};

      
      \addlegendentry{prior}
      \addlegendentry{$o_1o_2$}
      \addlegendentry{$o_3$}
      \addlegendentry{$o_4$}
      \addlegendentry{$o_5$}
      %% \addlegendentry{$o_6$}
      %% \addlegendentry{$o_7$}   
    \end{axis}
  \end{tikzpicture}
  \qquad
  \begin{tikzpicture}[scale=0.85]
    \begin{axis}[
        ybar stacked,
        ymin=0,
        ymax=4,
        xtick=data,
        legend style={cells={anchor=east}, legend pos=north west, legend columns=-1},
        reverse legend=false, % set to false to get correct display, but I'd like to have this true
        xticklabels from table={\noclusterization}{Label},
        xticklabel style={text width=2cm,align=center},
        legend plot pos=right,
        ylabel=precision --- prior and posterior,
        xlabel=eigenvector $i$,
      ]
    
      
      \addplot [fill=green!80]  table [y=prior, meta=Label, x expr=\coordindex] {\noclusterization};
      \addplot [fill=blue!60]   table [y=$o_1o_2$, meta=Label, x expr=\coordindex] {\noclusterization};
      \addplot [fill=red!60]    table [y=$o_3o_4$, meta=Label, x expr=\coordindex] {\noclusterization};
      \addplot [fill=black!60]  table [y=$o_5$, meta=Label, x expr=\coordindex] {\noclusterization};
      %% \addplot [fill=orange!60] table [y=$o_4$, meta=Label, x expr=\coordindex] {\noclusterization};
      %% \addplot [fill=cyan!60]   table [y=$o_5$, meta=Label, x expr=\coordindex] {\noclusterization};
      %% \addplot [fill=purple!60] table [y=$o_6$, meta=Label, x expr=\coordindex] {\noclusterization};

      
      \addlegendentry{prior}
      \addlegendentry{$o_1o_2$}
      \addlegendentry{$o_3o_4$}
      \addlegendentry{$o_5$}
      %% \addlegendentry{$o_4$}
      %% \addlegendentry{$o_5$}
      %% \addlegendentry{$o_6$}   
    \end{axis}
  \end{tikzpicture}
  \caption{Clusterization (left) and non-clusterization (right) in
    D-optimal designs. Posterior precision per eigenvector after
    taking observations $\meas_i$ is plotted. Both designs are
    identical for all practical matters --- their posteriors are
    equal. In the left panel we see how an optimal design is achieved
    with repeated observations. $\meas_3 = \meas_5$. This is not
    necessary though, as can be seen in the right panel. Both designs
    achieve the same levels of uncertainty. The left exhibits
    clusterization. The right does not.}
  \label{fig:clusterization}
\end{figure}


%% may arise in various scenarios. We first consider
%% sequential designs, and then simultaneous designs.


%% \subsection{Sequential Design}\label{subsec:clusterization sequential}
%% In a sequential optimal design problem, a decision is made on each
%% measurement with all previous measurements fixed. In each step $m=1$,
%% so $k=1$. In this case, Theorem \ref{thm:char} implies each
%% measurement is taken along the direction of the eigenvector of $\fwd
%% \prcov \fwd^*$ with lowest posterior precision. If $\lambda_i$
%% decrease fast enough (e.g. exponentially), then $\lambda_i^{-1}$
%% increase exponentially. Then, at some point, a measurement will be
%% taken along an already measured eigenvector. This situation is
%% measurement clusterization.

%% , $\meas_1 \parallel \ev_1$ and $\eta_1 = 1$. As hinted in
%% section \ref{subsec:seq vs sim}, the posterior becomes the prior for
%% the next step. The eigenvectors of the new posterior are the same as
%% for the previous one. A new observation will be taken in the direction
%% of eigenvector with smalles eigenvalue. This leads to:
%% \begin{align*}
%%   \begin{split}
%%     \meas_2 \parallel \ev_1  &\text{ if } \sigma^2 \lambda^{-1}_1 + 1 < \sigma^2\lambda^{-1}_2 \\
%%     \meas_2 \parallel \ev_2  &\text{ o.w. }
%%   \end{split}
%% \end{align*}
%% In the first case, we get clusterization immediately. In the second,
%% we consider $\meas_3$. Same reasoning leads to the conclusion that the
%% eigenvectors of the new prior precision are the same. The eigenvalues,
%% however, are $\lambda^{-1}_1 + \sigma^{-2}, \lambda^{-1}_2 +
%% \sigma^{-2}, \lambda_3^{-1}, \dots$. A new observation is taken in the
%% direction of eigenvector of smallest precision (note that
%% $\lambda^{-1}_1 + \sigma^{-2} < \lambda^{-1}_2 + \sigma^{-2}$):
%% \begin{align*}
%%   \begin{split}
%%     \meas_3 \parallel \ev_1 &\text{ if } \sigma^2 \lambda^{-1}_1 + 1 <
%%     \sigma^2\lambda^{-1}_3 \\
%%     \meas_3 \parallel  \ev_3  &\text{ o.w. }
%%   \end{split}
%% \end{align*}
%% Again, the first case results in clusterization. The sequential design
%% proceeds in this fashion. Every $\meas_i$ is taken in the direction
%% some eigenvector of the prior $\fwd \prcov\fwd^*$. This eigenvector
%% has the smallest eigenvalue of the previous step's posterior precision
%% operator. If we do not encounter clusterization after $m$ such steps,
%% this means that observation $\meas_k$ was taken in the direction
%% $\ev_k$ for $k=1,\dots,m$. Thus:
%% \begin{align}\label{eq:failure}
%%   \begin{split}
%%     \sigma^2\lambda_{k-1}^{-1} + 1 \geq \sigma^2\lambda_k^{-1}, \text{ for } 1\leq k \leq m
%%     \Longrightarrow \lambda_m^{-1} - \lambda_{m-1}^{-1} \leq \sigma^{-2}.
%%   \end{split}
%% \end{align}

%% But \eqref{eq:failure} implies $\{\lambda_i^{-1}\}_{i=1}^{\infty}$
%% grows at most linearly. In section \ref{subsec:abstract OED} we
%% assumed $\fwd$ is strongly smoothing, which means eigenvalues of
%% $(\fwd \prcov\fwd^*)^{-1}$ grow quickly. Thus, if we take $m$ large
%% enough, \eqref{eq:failure} will eventually fail and we will observe
%% sensor clusterization.


%% \subsection{Simultaneous Design}\label{subsec:clusterization simultaneous}
%% If the eigenvalues $\lambda_i$ decrease quickly enough, we will
%% observe clusterizaion.
