\section{Reviewer 1}
\RC This paper explains a common practical issue: the cause of
clusterization in Bayesian D-optimal designs in infinite dimensions
and why adding a correlated measurement process decluster the design
points. Its theoretical results are valuable to the field of inverse
problems/uncertainty quantification and the conclusions should be
useful to practitioners in this field.

\AR I appreciate the kind words.


\RC However, I feel changing the assumption from independent to correlated
measurements is quite artificial: people tweak a data generation
process assumption to solve an optimal design problem, and the author
just takes the assumption as it is.

\AR If I understand correctly --- I completely agree. I added text
emphasizing that I do not endorse using correlated errors:


\begin{quote}
  It is important to note that we do not view clustered designs as
  undesirable, nor do we believe it should be avoided at
  all. Clusterization is a peculiar phenomenon and it is perfectly
  reasonable for someone to argue against the D-optimality criterion
  based on the fact that it results in clustered designs. We have
  seen, however that there is a perfectly reasonable explanation for
  clusterization. We have shown that clusterization is an inevitable
  consequence of having a problem with some modes where uncertainty
  decays faster than others.
\end{quote}

\RC Since the author proves the clusterization issue under the independent
measurement assumption, I expect the author to go deeper and explain
why an ordinary data generation assumption + a common optimal design
leads to a “suboptimal” design?

\AR In my view, this is a result of the unlocalized structure of
Laplacian eigenvectors. I believe this is more of a problem with our
(linear) physical models and choice of (Gaussian) priors. See
discussion below reproduced from the revised manuscript (but note that
I do not endorse clustered designs)


\begin{quote}
  Lastly, we believe that when clusterization arises, it should serve
  as a warning sign to practitioners. In the inverse problem of the 1D
  heat equation, clusterization occurs primarily because Laplacian
  eigenvectors \emph{do not} decay in $\Omega$. Consequently,
  measuring $u(x_1, T)$ at some point $x_1 \in \Omega$ provides
  information about $u(x_2,T)$ for distant points $x_2 \in
  \Omega$. Intuitively, this should not occur: for small $T$, the heat
  distribution at $x_1$ should give very little knowledge on the heat
  distribution at $x_2$. This behavior stems from a well-known
  property of the heat equation: it allows information to spread
  \emph{instantly} across the computational domain
  \cite{renardy2006PDE}. In reality, heat (and information) propagate
  at finite speeds. Of course, the known physical barrier for
  information spread is the speed of light, but we expect heat to
  spread considerably slower: heating an Olympic pool at one end
  should have no immediate effect on the temperature at the other end.

  Our choice of prior is also a potential major contributor to
  clusterization. Our choice of Gaussian prior similarly implies
  information is shared between distant locations in $\Omega$. Thus, we
  suggest refraining from choosing Gaussian priors with inverse
  Laplacian covariance operators. Rather, non-Gaussian priors should be
  employed instead \cite{hosseini2017, hosseini2019}.
  
  We believe that the emergence of clusterization in this context is
  thus non-physical, arising from the way the inverse problems we
  consider are phrased. Clusterization therefore indicates that the
  underlying mathematical / Bayesian model is overly permissive and
  fails to capture crucial physical constraints of the problem. We
  suggest that when clusterization occurs, practitioners should consider
  alternative models where information is localized in space and travels
  at finite speed in the medium. Such models may not only provide more
  physically accurate and meaningful results but may also mitigate the
  issue of clusterization.
\end{quote}


\RC Not just by the mathematical proof, but from an information theory
point of view. For example, does the redundancy points mean additional
points does not reduce the variance of the estimation at all? If
that’s the case, how does correlated measurement assumption “help”
with it?

\AR I think this is the same misunderstanding I commented on above;
For any nonzero observation error $\sigma^2 > 0$, repeated
measurements will indeed result in an increase of the design
criterion.


