\appendix
\section{Generalization of known lemmas}

The following lemma is generalized from \cite[Chapter 9, Theorem 4, p. 127]{Lax07}
\lax
\begin{proof}
  Consider a differentiable operator-valued function $X(t)$ such that
  $X(0) = 0$ and $X(t)$ is positive, self-adjoint and trace-class for
  every $t\in \R$. We denote the eigenvalues of this operator by
  $\lambda_k(X(t))$ and sometimes drop the dependence on $X(t)$, so
  $\lambda_k = \lambda_k(X(t))$.  Then $\det (I+X(t))
  = \prod_{k=1}^{\infty} (1+\lambda_k) < \infty$ and this is finite by
  the arguments given in \cite{AlexanderianGloorGhattas14}. The full
  derivative is \begin{align*} \frac{\der \det (I+X(t))}{\der t}
    % 
    % 
    % 
    &= \sum_{k=1}^{\infty} 
    \frac{\partial \det (I+X(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    % 
    %
    %
    &= \sum_{k=1}^{\infty} \frac{\partial \prod_{l=1}^{\infty}
      (1+\lambda_l(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    %
    %
    %
    &= \sum_{k=1}^{\infty} \prod_{l=1, l\neq k}^{\infty}
      (1+\lambda_l(s)) \frac{\partial (1+\lambda_k(s))}{\partial (1+\lambda_k)}\Big |_{s=t}
    \frac{\der (1+\lambda_k)}{\der t} \\
    %
    %
    %    
    &= \sum_{k=1}^{\infty} \frac{\prod_{l=1}^{\infty}
      (1+\lambda_l(s))}{(1+\lambda_k)}\Big |_{s=t}
    \dot{\lambda_k}(X(t)) \\
    % 
    % 
    % 
    &= \sum_{k=1}^{\infty} \frac{\det (I+X(t))}{1 +\lambda_k} \dot{\lambda_k}(X(t)).
  \end{align*}
  The assumption $X(0) = 0$ means $\lambda_k(X(0)) = 0,\ \forall k \geq 1$. Thus:
  \begin{align*}
    \frac{\der (I+\det X(t))}{\der t}\Big |_{t=0} 
    = \sum_{k=1}^{\infty} \dot{\lambda_k}(X(0)) 
    = \frac{\der }{\der t}\tr{X(0)}
    = \tr{\dot{X}(0)},
  \end{align*}
  where the second equality follows by monotone convergence. 
  Let $Y(t)$ a trace-class self-adjoint operator such that 
  $I+Y(t)$ is invertible.
  Define $X(t)$ via $I+X(t) = (I+Y(0))^{-1/2} (I+Y(t)) (I+Y(0))^{-1/2}$. 
  We show $X(t)$ satisfies the conditions above. It is trace-class:
  \begin{align*}
    \tr{X(t)} = \tr{(I+Y(0))^{-1} (I+Y(t)) - I}
    \leq \tr{I+Y(t) - I}< \infty,
  \end{align*}
  since $Y(t)$ is trace-class. It is also clear that
  $X(0) = 0$ and $X(t)$ is self-adjoint.
  $I+Y(t) = (I+Y(0))^{1/2}(I+X(t))(I+Y(0))^{1/2}$, so
  \begin{align*}
    \frac{\der \det (I+Y(t))}{\der t}|_{t=0} 
    &= \det (I+Y(0))\frac{\der \det (I+X(t))}{\der t}\Big |_{t=0} \\
    % 
    % 
    % 
    &= \det (I+Y(0)) \tr{\dot{X}(0)} \\
    % 
    % 
    % 
    &= \det (I+Y(0)) \tr{(I+Y(0))^{-1} \dot{Y}(0)}.
  \end{align*}
  Consequently, by the one-variable chain rule:
  \begin{align*}
    \frac{\der \log \det (I+Y(t))}{\der t}\Big |_{t=0} &=
    % 
    % 
    % 
    \frac{1}{\det (I+Y(0))}\frac{\der \det (I+Y(t))}{\der t}\Big |_{t=0} \\ 
    % 
    % 
    % 
    &= \tr{ (I+Y(t))^{-1} \dot{Y}(t)} \big |_{t=0}.
  \end{align*}
  There is nothing special about $t_0 = 0$ --- we could have chosen
  any other $t_0$ instead. Thus, the relation holds for all $t$.
\end{proof}


\begin{lemma}[Matrix Determinant Lemma in Hilbert Spaces]\label{lemma:MDL}
  Let $\hil$ a separable Hilbert space, $u,v\in \hil$ and $A: \hil \to
  \hil$ an invertible linear operator such that $\tr{A-I} <
  \infty$. Then $\det A$ and $\det A + uv^*$ are well defined and
  \begin{equation*}
    \det (A + uv^*) = (1 + \langle A^{-1} u, v \rangle ) \det A,
  \end{equation*}
  where $(A + uv^*)w := Aw + \langle v,w \rangle u$.
\end{lemma}
\begin{proof}
  In this proof we rely on definitions and results from
  \cite{simon1977}. First, consider $B := I + xy^*$ for some $x,y \in
  \hil$. We construct an eigenbasis for $B$ and use that to show $\det
  B = 1 + \langle x, y \rangle$. First let $x_1 := x$.  Now, if $x
  \parallel y$, take $\{x_n \}_{n=2}^{\infty}$ an orthogonal basis for
  $span\{x_1\} ^{\perp}$. If, on the other hand, $x \not \parallel y$, let
  \begin{equation*}
    x_2 := x - \frac{ \langle x, y\rangle}{\|y\|^2}y
  \end{equation*}
  and it is easy to verify that $x_2 \perp y$ and $span \{x,y\} = span
  \{x_1,x_2\}$. Take $\{x_n \}_{n=3}^{\infty}$ an orthogonal basis for
  $span\{x_1,x_2\} ^{\perp}$. In both cases,
  \begin{equation*}
    B x_n =
    \begin{cases}
      (1 + \langle x, y \rangle) x_n & n = 1 \\
      x_n                            & n \neq 1,
    \end{cases}
  \end{equation*}
  and so $\det B = 1 + \langle x, y \rangle$.
  
  It is easy to verify that $uv^*$ is trace-class and since $\tr{A-I}
  < \infty$, also $\tr{A + uv^* - I} < \infty$ (sum of two trace-class
  operators is trace-class). Thus $\det A$ and $\det (A+uv^*)$ are
  well defined. Let $x:=A^{-1}u$ and $y := v$:
  \begin{equation*}
    \det (A + uv^*) = \det A \ \det(I+A^{-1}uv^*) =
    (1 + \langle A^{-1}u, v \rangle) \det A .
  \end{equation*}
\end{proof}


%% \free
%% \begin{proof}
%%   Let us diagonalize $M$, so that $M = U D U^t$ with $D =
%%   \diag(d_1,\dots,d_k)$ and $U \in \R^{k \times k }$ orthogonal. Let
%%   $S \in \R^{k \times m}$ with $S_{ii} = \sqrt{d_{i}}$ and zeros
%%   otherwise. Define $A:= U S V^t$, where $V \in \R^{m \times m}$ is
%%   orthogonal and will be further restricted later. Then $AA^t = U
%%   SV^tVS^t U^t = UDU^t$, so $AA^t$ has the required eigenvalues and
%%   eigenvectors by construction. If we can choose $V$ such that $A$
%%   also satisfies the unit norm constraints we are done. These
%%   constraints are, for $j=1,\dots,m$:
%%   \begin{equation}\label{eq:V constraints}
%%    1 = [A^tA]_{jj} = [V S^tS V^t]_{jj},
%%   \end{equation}
%%   and we can expect to do this since we assumed $\ttr D = m$.

%%   Define $C = S^tS - I \in \R^{m \times m}$. Note that $\ttr C = 0$ and
%%   $C$ is diagonal with non-zero entries $d_i-1,i=1,\dots,k$. It suffices
%%   to find $V$ orthogonal such that $V C V^t$ has zero diagonal. We
%%   construct such $V$ by sequentially inserting zeros in the diagonal
%%   and not destroying zeros we already introduced, starting from the
%%   last diagonal entry and moving to the first. Since $c_{mm} \neq 0$ ,
%%   let $p < m$ such that $c_{pp}c_{mm} < 0$ (such $p$ exists because
%%   the trace is zero) and let $\theta \in (0,\pi)$. Define a Givens
%%   rotation $R^{(m)} \in \R^{m \times m}$ by
%%   \begin{equation*}
%%     r^{(m)}_{ab} :=
%%     \begin{cases}
%%       1 & a = b \neq p \text{ or } a = b \neq m \\
%%       \cos \theta & a = b = p  \\
%%      -\sin \theta & a = p, b = m\\
%%       \cos \theta & a = b = m \\
%%       \sin \theta & a = m, b = p \\ 
%%       0 & o.w
%%     \end{cases}
%%   \end{equation*}
%%   Note that conjugating a matrix by $R^{(m)}$ changes only its $m$ and
%%   $p$ rows and columns. We want to choose $\theta$ such that
%%   \begin{equation}\label{eq:mm}
%%     0 = [R^{(m)} C (R^{(m)})^t]_{mm} = \cos^2 \theta c_{mm} + 2\cos \theta \sin
%%     \theta c_{mp} + \sin^2\theta c_{pp},
%%   \end{equation}
%%   and it suffices to choose $\theta$ such that
%%   \begin{equation*}
%%     c_{mm} \cot^2 \theta + 2 c_{mp} \cot \theta + c_{pp} = 0.
%%   \end{equation*}
%%   This quadratic in $\cot\theta$ has a real solution, since
%%   $c_{pp}c_{mm} < 0$ by assumption and we can find $\theta \in
%%   (0,\pi)$ such that \eqref{eq:mm} is satisfied. We continue to find
%%   $R^{(m-1)}$ that leaves row and column $m$ unchanged and
%%   continue introducing zeros to the diagonal. The assumption $\ttr D =
%%   m \Rightarrow \ttr C = 0$ guarantees we can do that. Taking $V:=
%%   R^{(1)} R^{(2)} \dots R^{(m-1)}R^{(m)}$ completes the proof.
%% \end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \woodbury
%% \begin{proof}
%%   The proof amounts to using Woodbury's matrix identity twice and a
%%   regularization trick. The standard proof for Woodbury's matrix
%%   identity works in infinite dimensions, as long as all terms are well
%%   defined. Unfortunately, $\obs^*\obs$ is not invertible, so we force
%%   it to be. Recall Woodbury's matrix identity:
%%   $$
%%   (A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}.
%%   $$

%%   Denote $A := \prcov^{-1}$, $U := \fwd^*$, $V := \fwd$, $C :=
%%   \sigma^{-2} (\obs^*\obs+\eps I)$ for some $\eps > 0$. Then:
%%   %% \begin{align*}
%%   %%   \begin{split}
%%   \begin{equation}\label{eq:first}
%%   \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\eps I) \fwd )^{-1}\fwd^* = \fwd ( \prcov - \prcov \fwd^* ( \sigma^2(\obs^*\obs + \eps I)^{-1} + \fwd \prcov \fwd^* )^{-1} \fwd \prcov ) \fwd^* \\
%%       %
%%       %
%%       %% &= X - X(\sigma^2Y_{\eps}^{-1} + X)^{-1}X
%%   %%   \end{split}
%%   %% \end{align*}
%%   \end{equation}
%%    Now denote $X := \fwd\prcov \fwd^*$ and $Y_{\eps} := \obs^*\obs +
%%    \eps I$, and $Y_{\eps}$ is invertible. Then \eqref{eq:first}
%%    becomes:
%%    \begin{equation}\label{eq:second}
%%      \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\eps I) \fwd )^{-1}\fwd^* = X - X(\sigma^2Y_{\eps}^{-1} + X)^{-1}X
%%    \end{equation}

%%    Note that $X + \sigma^2 Y_{\eps}^{-1}$ is invertible, as the sum of
%%    two positive definite operators. Now, taking $A := X^{-1}, C :=
%%    \sigma^2Y_{\eps}^{-1}, U := I$ and $V := I$ and using Wodbury's
%%    matrix identity in reverse order:
%%   \begin{align*}
%%     \begin{split}
%%       \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\eps I) \fwd )^{-1}\fwd^* &= X - X(\sigma^2Y_{\eps}^{-1} + X)^{-1}X \\
%%       %
%%       %
%%       %
%%       &= (X^{-1} + \sigma^{-2}Y_{\eps})^{-1} \\
%%       %
%%       %
%%       %
%%       &= (\fwd \prcov \fwd^* + \sigma^{-2} (\obs^*\obs + \eps I))^{-1}.
%%     \end{split}
%%   \end{align*}

%%   We conclude that $\forall \eps > 0$
%%   \begin{align*}
%%     \begin{split}
%%       \fwd( \prcov^{-1} + \sigma^{-2}  \fwd^* (\obs^* \obs +\eps I) \fwd )^{-1}\fwd^* 
%%      &= (\fwd \prcov \fwd^* + \sigma^{-2} (\obs^*\obs + \eps I))^{-1}.
%%     \end{split}
%%   \end{align*}
%%   Letting $\eps \to 0$ completes the proof.
%% \end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{lemma}[Increase due to an observation]\label{lemma:design increase}
%%   Let $\obs = (\meas_1,\dots,\meas_m)^t$ and $\obsm :=
%%   (\meas_1,\dots,\meas_{m-1})^t$. Then
%%   \begin{align*}
%%     \tar( \obs ) - \tar (\obsm ) &=
%%     \frac12 \log \left ( 1 + \frac{
%%       \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m,
%%       (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m \rangle
%%     }{
%%       \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
%%     }       
%%     \right ).
%%   \end{align*}
%% \end{lemma}
%% \begin{proof}
%%   We use the Schur complement:
%%   %% to write one inverse in terms of the other: and introduce
%%   %%  notations to make the derivation cleaner. Note that we think of
%%   %%  $\obsm$ and $\obsm^*$ as column and row vectors (respectively).
%%   \begin{align*}
%%     \Sigma( \obs ) &= \Sigma = 
%%     \begin{bmatrix}
%%       \Sigma (\obsm )           & \obsm \modcov \meas_m \\
%%       \meas_m \modcov \obsm^*   & \sigma^2 + \meas_m \modcov \meas_m
%%     \end{bmatrix}
%%     : =
%%     \begin{bmatrix}
%%       \Sigmam   & w \\
%%       w^t       & c
%%     \end{bmatrix}\\
%%     %
%%     %
%%     %
%%     \Sigma^{-1} &=
%%     \begin{bmatrix}
%%       \Sigmam^{-1} + \Sigmam^{-1} w ( c - w^t \Sigmam^{-1} w)^{-1} w^t \Sigmam^{-1} & - \Sigmam^{-1} w ( c - w^t \Sigmam^{-1} w)^{-1} \\
%%       -( c - w^t \Sigmam^{-1} w)^{-1} w^t \Sigmam^{-1}                            &  ( c - w^t \Sigmam^{-1} w)^{-1}
%%     \end{bmatrix} \\
%%     &=
%%     \begin{bmatrix}
%%       \Sigmam^{-1} & 0 \\
%%       0           & 0 
%%     \end{bmatrix}
%%     + (c -w^t \Sigmam^{-1} w )^{-1}
%%     \begin{bmatrix}
%%       \Sigmam^{-1} w \\
%%       -1
%%     \end{bmatrix}
%%     \begin{bmatrix}
%%       w^t \Sigmam^{-1} & -1 
%%     \end{bmatrix}
%%   \end{align*}
%%   %
%%   Further, define
%%   %
%%   \begin{align*}
%%     \M (\obs ):&= \prcov^{\frac12}\fwd^* \obs^* \Sigma^{-1} \obs \fwd
%%     \prcov^{\frac12}    
%%   \end{align*}
%%   %
%%   and note that:%% , using our understanding of what is a column vector and
%%   %% what is a row vector:
%%   %
%%   \begin{align*}
%%     \M(\obs) &= \prcov^{1/2} \fwd^* \obs^* \Sigma^{-1} \obs \fwd \prcov^{1/2} \\
%%     %
%%     %
%%     %
%%     &= \prcov^{1/2} \fwd^* \obs^* \left \{
%%     \begin{bmatrix}
%%       \Sigmam^{-1} & 0 \\
%%       0           & 0 
%%     \end{bmatrix}
%%     + (c -w^t \Sigmam^{-1} w )^{-1}
%%     \begin{bmatrix}
%%       \Sigmam^{-1} w \\
%%       -1
%%     \end{bmatrix}
%%     \begin{bmatrix}
%%       w^t \Sigmam^{-1} & -1 
%%     \end{bmatrix} 
%%     \right \} \obs \fwd \prcov^{1/2} \\
%%     %
%%     %
%%     %
%%     &= \M (\obsm) + (c -w^t \Sigmam^{-1} w )^{-1}
%%     \prcov^{1/2} \fwd^* \obs^*
%%     \begin{bmatrix}
%%       \Sigmam^{-1} w \\
%%       -1
%%     \end{bmatrix}
%%     \begin{bmatrix}
%%       w^t \Sigmam^{-1} & -1 
%%     \end{bmatrix} 
%%     \obs \fwd \prcov^{1/2}
%%   \end{align*}
%%   %
%%   Now, denote:
%%   %
%%   \begin{align}\label{eq:u}
%%     \begin{split}
%%       u :&= (c -w^t \Sigmam^{-1} w )^{-1/2}
%%       \prcov^{1/2} \fwd^* \obs^* 
%%       \begin{bmatrix}
%%         \Sigmam^{-1} w \\
%%         -1 
%%       \end{bmatrix} \\
%%       %
%%       %
%%       %
%%       & = (c -w^t \Sigmam^{-1} w )^{-1/2} ( \prcov^{1/2}\fwd^* \obsm^* \Sigmam^{-1} \obsm  \modcov \meas_m - \prcov^{1/2} \fwd^* \meas_m )\\
%%       %
%%       %
%%       %
%%       u^* :&=  (c -w^t \Sigmam^{-1} w )^{-1/2} (\meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \fwd \prcov^{1/2} - \meas_m \fwd \prcov^{1/2} ),
%%     \end{split}
%%   \end{align}
%%   %
%%   so that
%%   %
%%   \begin{equation}\label{eq:M plus I}
%%     I + \M( \obs ) = I + \M (\obsm ) + uu^*.
%%   \end{equation}
%%   %
%%   Note that
%%   \begin{equation}\label{eq:M postcov}
%%     \prcov^{1/2} \left (I + \M( \obsm ) \right )^{-1} \prcov^{1/2} = \postcovm.
%%   \end{equation}
%%   The increase in the design criterion gained by including $\meas_m$
%%   is then: 
%%   %
%%   \begin{align*}
%%     \tar( \obs ) - \tar( \obsm )
%%     %
%%     %
%%     %
%%     &= \frac12 \log \det \Big ( I + \M ( \obs ) \Big ) / \det \Big ( I + \M (\obsm) \Big ) \\
%%     %
%%     %
%%     %
%%     &= \frac12  \log \det \left ( I + \M(\obsm) + uu^* \right ) / \det \Big ( I + \M (\obsm) \Big ) \\
%%     %
%%     %
%%     %
%%     &= \frac12 \log \left ( 1 + \left \langle \left ( I+\M(\obsm) \right )^{-1} u, u  \right \rangle \right ) \text{ (Lemma \ref{lemma:MDL})}.
%%   \end{align*}
%%   Using \eqref{eq:u} and \eqref{eq:M postcov}:
%%   \begin{align*}
%%     &\left \langle \left (I+\M (\obsm)\right )^{-1}u, u \right \rangle\\
%%     &= \frac{
%%       \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m,
%%       (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m \rangle
%%     }{
%%       c- w^t \Sigmam^{-1} w
%%     }\\
%%     %
%%     %
%%     %
%%     &= 
%%     \frac{
%%       \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m,
%%       (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m \rangle
%%     }{
%%       \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
%%     }
%%   \end{align*}
%%   and the conclusion follows.
%% \end{proof}

%% Lemma \ref{lemma:design increase} implies the following corollary:
%% \begin{corollary}[Gain for No Model Error]\label{cor:zero mod err}
%%   If $\modcov = 0$, then
%%   \begin{equation*}
%%     \tar( \obs ) - \tar (\obsm )
%%     = -\frac12 \log (1 - \sigma^{-2} \langle \fwd \postcov \fwd^* \meas_m, \meas_m \rangle ).
%%   \end{equation*}
%% \end{corollary}
%% \begin{proof}
%%   Note that this is not immediate by substituting $\modcov = 0$ in the
%%   conclusion of Lemma \ref{lemma:design increase}, since we make a
%%   claim for $\postcov$, and not $\postcovm$. Let us first review
%%   \eqref{eq:u} and note that since $\modcov = 0$ the covariance
%%   $\Sigmam = \sigma^2I_{m-1}$ and $w = 0$, so $c - w^t\Sigmam^{-1}w =
%%   \sigma^2$:
%%   \begin{align*}
%%     u :&=-\sigma^{-1}\prcov^{1/2} \fwd^* \meas_m\\
%%     %
%%     %
%%     u^* :&= -\sigma^{-1} \meas_m \fwd \prcov^{1/2}.
%%   \end{align*}
%%   From \eqref{eq:M plus I}:
%%   \begin{equation*}
%%     I + \M(\obsm) = I +\M(\obs) - uu^*,
%%   \end{equation*}
%%   and thus:
%%   \begin{equation*}
%%     \left \langle \left ( I +\M(\obs) \right )^{-1} u, u \right \rangle
%%     = \sigma^{-2} \langle \fwd \postcov \fwd^* \meas_m, \meas_m \rangle.
%%   \end{equation*}
%%   Analogously to \eqref{eq:M postcov} we note that
%%   \begin{equation*}
%%     \prcov^{1/2} \left ( I + \M(\obs) \right )^{-1} \prcov^{1/2} = \postcov.
%%   \end{equation*}
%%   Using Lemma \ref{lemma:MDL} we conclude
%%   \begin{align*}
%%     \tar( \obs ) - \tar( \obs )
%%     &= \frac12 \log \det \left (I +\M(\obs) \right ) / \det \left (I + \M(\obsm) \right ) \\
%%     %
%%     %
%%     %
%%     &= \frac12 \log \det \left (I +\M(\obs) \right ) / \det \left (I + \M(\obs) - uu^* \right ) \\
%%     %
%%     %
%%     %
%%     &=-\frac12 \log (1 - \langle (I+\M(\obs))^{-1}u, u \rangle \\
%%     %
%%     %
%%     %
%%     &= -\frac12 \log (1 - \sigma^{-2} \langle \fwd \postcov \fwd^* \meas_m, \meas_m \rangle ).
%%   \end{align*}
%% \end{proof}

% \samemeas
%% \begin{proof} \label{cor:same meas proof}
%%   Denote $A:= \obs \modcov \obs^*$ and $v_j$ the $j$th column of $A$.
%%   Note that $v_j = \obsm \modcov \meas_m$, since $(\obsm \modcov
%%   \obsm^*)_{ij} = \meas_i(\modcov \meas_j)$, as explained in
%%   \eqref{eq:modcov explained}. One can now verify that
%%   \begin{equation}\label{eq:observation}
%%     \Sigmam^{-1} \obsm \modcov \meas_m = \Sigmam^{-1}v_j = (A +\sigma^2I_{m-1})^{-1} v_j =
%%     e_j -\sigma^2 \Sigmam^{-1}e_j.
%%   \end{equation}
%%   %
%%   Using \eqref{eq:observation}:
%%   \begin{align}\label{eq:denominator}
%%     \begin{split}
%%       \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m
%%       &= \meas_m \modcov \obsm^* ( e^j - \sigma^2 \Sigmam^{-1} e_j )\\
%%       %
%%       %
%%       %
%%       &= \meas_m \modcov \meas_j - \sigma^2 \meas_m \modcov \obsm^* \Sigmam^{-1}e_j \\
%%       %
%%       %
%%       %
%%       &= \meas_m \modcov \meas_j -\sigma^2 (e_j - \sigma^2 \Sigmam^{-1}e_j)^t e_j \\
%%       %
%%       %
%%       %
%%       &= \meas_m \modcov \meas_m -\sigma^2 + \sigma^4 e_j^t\Sigmam^{-1}e_j.
%%     \end{split}
%%   \end{align}
%%   We use \eqref{eq:observation} to simplify the terms in the enumerator of
%%   the conclusion of Lemma \ref{lemma:design increase}:
%%   \begin{align}\label{eq:enumerator}
%%     \begin{split}
%%       (\obsm^* \Sigmam^{-1} \obsm \modcov - I ) \meas_m
%%       &= \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m - \meas_m \\
%%       %
%%       %
%%       %
%%       &= \obsm^* (e_j - \sigma^2 \Sigmam^{-1} e_j) -\meas_j \\ 
%%       %
%%       %
%%       %
%%       &= -\sigma^2 \obsm^* \Sigma^{-1}e_j. 
%%     \end{split}
%%   \end{align}
%%   %
%%   Substitute \eqref{eq:enumerator} and \eqref{eq:denominator} to
%%   the enumerator and denominator (respectively) of the conclusion of
%%   Lemma \ref{lemma:design increase}:
%%   %
%%   \begin{align*}
%%     \tar( \obs ) - \tar (\obsm ) &=
%%     \log \left ( 1 + \frac{
%%       \langle \fwd \postcovm \fwd^* (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m,
%%       (\obsm^* \Sigmam^{-1} \modcov - I ) \meas_m \rangle
%%     }{
%%       \sigma^2 + \meas_m \modcov \meas_m - \meas_m \modcov \obsm^* \Sigmam^{-1} \obsm \modcov \meas_m 
%%     }       
%%     \right ) \\
%%     %
%%     %
%%     %
%%     &= \log \left ( 1 + \frac{\sigma^4
%%       \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1} e_j,
%%       \obsm^* \Sigmam^{-1}e_j \rangle
%%     }{
%%       2\sigma^2 - \sigma^4 e_j^t\Sigmam^{-1}e_j 
%%     }       
%%     \right ) \\
%%     %
%%     %
%%     %
%%     &= \log \left ( 1 + \frac{\sigma^2
%%       \langle \fwd \postcovm \fwd^* \obsm^* \Sigmam^{-1} e_j,
%%       \obsm^* \Sigmam^{-1}e_j \rangle
%%     }{
%%       2 - \sigma^2 e_j^t\Sigmam^{-1}e_j 
%%     }       
%%     \right ).
%%   \end{align*}
%% \end{proof}


