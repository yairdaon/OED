\documentclass[aspectratio=169]{beamer}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}

\usetheme{CambridgeUS}
\usecolortheme{beetle}

\title{Measurement Clusterization in Bayesian D-optimal Designs in Infinite Dimensions}
\author{Yair Daon}
\institute{Azrieli Faculty of Medicine, Bar-Ilan University}
\date{\today}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{Motivation}
\begin{itemize}
    \item Selecting optimal measurements is crucial for inferring parameters of physical processes
    \item Measurements should enhance accuracy, reduce costs, or both
    \item A-optimal and D-optimal design criteria are widely studied
    \item Optimal designs can yield tightly clustered measurement locations (clusterization)
\end{itemize}
\end{frame}

\begin{frame}{Example: 1D Heat Equation}
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig1.png}
    \caption{Measurement clusterization for optimal designs when inverting for the initial condition of the 1D heat equation.}
\end{figure}
\end{frame}

\begin{frame}{Existing Approaches to Mitigate Clusterization}
\begin{itemize}
    \item Merging close measurements
    \item Imposing distance constraints between measurements 
    \item Introducing correlated errors accounting for model misspecification
    \item Choosing measurements from a coarse grid
\end{itemize}
\textbf{Issue:} These approaches fundamentally alter the optimal design problem
\end{frame}

\begin{frame}{Key Questions}
\begin{enumerate}
    \item Why does imposing correlations between observations alleviate clusterization?
    \item Is measurement clusterization a generic phenomenon? 
    \item Why does measurement clusterization occur?
\end{enumerate}
\end{frame}

\section{Problem Formulation}

\begin{frame}{Bayesian Linear Inverse Problem}
Let $H_p$, $H_o$ be Hilbert spaces and $F: H_p \to H_o$ a linear compact forward operator.\\~\\
Measurement operator $O: H_o \to \mathbb{R}^m$ takes $m$ noisy measurements:
\begin{equation*}
    d = OFm + O\varepsilon' + \varepsilon
\end{equation*}
where $\varepsilon' \sim \mathcal{N}(0,\Gamma_\text{model})$ is model error and $\varepsilon \sim \mathcal{N}(0,\sigma^2I_m)$ is measurement noise.
\end{frame}

\begin{frame}{Prior and Posterior}
Prior on $m$ is Gaussian: $\mu_\text{pr} = \mathcal{N}(m_\text{pr},\Gamma_\text{pr})$\\~\\  
Posterior $\mu_\text{post}^{d,O}$ is also Gaussian with covariance:
\begin{equation*}
    \Gamma_\text{post} = \left( \Gamma_\text{pr}^{-1} + F^*O^*\Sigma^{-1}OF \right)^{-1}
\end{equation*}
where $\Sigma(O) = O\Gamma_\text{model}O^* + \sigma^2I_m$.
\end{frame}

\begin{frame}{Bayesian D-Optimal Design}
Maximizes expected KL divergence between posterior and prior:
\begin{equation*}
    \Psi(O) := \mathbb{E}_d\left[ D_\text{KL}(\mu_\text{post}^{d,O} \| \mu_\text{pr}) \right] = \frac{1}{2} \log\det\left(I + \Gamma_\text{pr}^{1/2}F^*O^*\Sigma^{-1}OF\Gamma_\text{pr}^{1/2}\right)
\end{equation*}
D-optimal design operator:
\begin{equation*}
    O = \arg\max_{\|o_j\|=1, j=1,\dots,m} \Psi(O)
\end{equation*}
\end{frame}

\section{Main Results}

\begin{frame}{Theorem 1}
Let $\{\lambda_i\}$ be the eigenvalues of $F\Gamma_\text{pr}F^*$ and $\{\eta_i\}$ the eigenvalues of $O^*O$. \\~\\
For a D-optimal design $O$: 
\begin{enumerate}
    \item $\text{tr}\{O^*O\} = m$
    \item $O^*O$ and $F\Gamma_\text{pr}F^*$ are simultaneously diagonalizable
    \item $\Psi(O) = \frac{1}{2} \sum_{i=1}^k \log(1 + \sigma^{-2}\lambda_i\eta_i)$ where $k=\text{rank}(O^*O) \leq m$
    \item $\eta_i = \begin{cases} 
          \frac{m}{k} - \sigma^2\lambda_i^{-1} + \sigma^2\frac{1}{k}\sum_{j=1}^k \lambda_j^{-1}, & 1 \leq i \leq k \\
          0, & i > k
       \end{cases}$
    \item Covariance of $F^*\mu_\text{post}^{d,O}$ is $\left((F\Gamma_\text{pr}F^*)^{-1} + \sigma^{-2}O^*O\right)^{-1}$ 
\end{enumerate}
\end{frame}

\begin{frame}{Model Error Mitigates Clusterization}
For $\Gamma_\text{model} \neq 0$, no increase in $\Psi$ is achieved by repeating a measurement as $\sigma^2 \to 0$. \\~\\
Designs exhibiting clusterization are not D-optimal.\\~\\
\textbf{Conclusion:} For small $\sigma^2$, clusterization is mitigated by non-zero model error $\Gamma_\text{model}$.
\end{frame}

\begin{frame}{D-Optimal Designs Without Model Error}
Necessary condition for D-optimality when $\Gamma_\text{model}=0$:
\begin{equation*}
    \sigma^{-2}F\Gamma_\text{post}F^*O^* = O^*\Xi
\end{equation*}
where $\Xi$ is diagonal. This is a nonlinear eigenvalue problem.\\~\\
Key results:
\begin{itemize}
    \item $O^*O$ and $F\Gamma_\text{pr}F^*$ are simultaneously diagonalizable
    \item D-optimal designs focus on a subset of prior covariance eigenvectors with largest eigenvalues
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{fig2.png}
    \caption{D-optimal measurement locations and weighted eigenvectors for 1D heat equation with $m=4$ measurements.}
\end{figure}
\end{frame}

\begin{frame}{Intuition for Theorem 1}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{fig3a.png}
    \caption{D-optimal design}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{fig3b.png}
    \caption{Sub-optimal design}
\end{figure}
\end{column}
\end{columns}
D-optimal designs increase precision where it is lowest in the prior.
\end{frame}

\section{Cause of Clusterization}

\begin{frame}{Why Clusterization Occurs}
\begin{itemize}
    \item D-optimal designs focus on eigenvectors with highest prior variance
    \item In spatial problems, designs favor locations where:
    \begin{enumerate}
        \item Select eigenvectors are large
        \item Other eigenvectors are small
    \end{enumerate}
    \item Number of such locations is limited
    \item Clusterization occurs due to pigeonhole principle - more measurements than optimal locations
\end{itemize}
\end{frame}

\section{Conclusions}

\begin{frame}{Summary}
\begin{itemize}
    \item Investigated measurement clusterization in Bayesian D-optimal designs
    \item Proposed analytically tractable model for D-optimality over Hilbert spaces
    \item Showed clusterization is generic for independent Gaussian errors
    \item Proven model error mitigates clusterization
    \item Characterized D-optimal designs as reducing uncertainty across subset of prior eigenvectors
    \item Argued clusterization occurs due to pigeonhole principle
\end{itemize}
\end{frame}

\begin{frame}{Limitations and Future Work}
\begin{itemize}
    \item Generic model does not correspond to a specific real problem
    \item Extend analysis to A-optimal designs
    \item Investigate clusterization in nonlinear inverse problems
    \item Develop computational methods to avoid clusterization without compromising optimality
\end{itemize}
\end{frame}

\begin{frame}{References}
\footnotesize
\bibliographystyle{plain}
\bibliography{references}
\end{frame}

\begin{frame}
\begin{center}
    {\huge Thank You!}\\~\\
    Questions?
\end{center}
\end{frame}

\end{document}
