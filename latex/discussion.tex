\section{Answers to questions posed in the introduction}
First of all, it is important to state that we do not view measurement
clusterization as undesirable, nor do we believe it should be avoided
at all. Clusterization is a peculiar phenomenon and it is perfectly
reasonable for someone to argue against the D-optimality criterion
based on the fact that it results in clustered designs. We have seen,
however that there is perfectly reasonable explanation for
clusterization, i.e.~that clusterization arises when we seek a
designWe have shown that clusterization is an innevitable conseqeunce
of having a problem with modes of higher uncertainty than
others. Clusterization manifests the true underlying objective



\subsubsection{Answer to Question \ref{q:generic}}
We have shown through analysis and numerical experiments that



\subsubsection{Answer to Question \ref{q:mitigate}}
We have show


\subsubsection{Answer to Question \ref{q:why}}
Theorem \ref{thm:char} helps us give a compelling explanation of the
measurement clusterization we obeserved for the inverse problem of the
heat equation (Fig.~\ref{fig:}). Below, we take the following steps:
We first utilize Theorem \ref{thm:char} to understand what a D-optimal
design will look like for our relaxed model; we then argue that
although not identical, a D-optimal design for the full inverse
problem of the heat equation shares similar charachteristics to the
relaxed design; finally, we show how ...

First, recall from Theorem \ref{thm:char} that in our relaxed model,
for a D-optimal design $\opt$:
\begin{enumerate}
  \item $\opt^*\opt$ is simultaneously diagonalizable with $\fwd
    \prcov \fwd^*$.
  \item $\opt^*\opt$ has non-zero eigenvalues only for the $k:=\rank
    \opt^*\opt$ largest eigenvalues $\lambda_j$ of $\fwd\prcov\fwd^*$,
    i.e.~$\eta_j=0$ for $j>k$.
  \item $k$ is monotonic in $m$, the number of measurements we take.
\end{enumerate}

Now, consider $\fwd$ and $\prcov$ from the \emph{the inverse problem
of the heat equation}. As before, we denote the eigenvalues of
$\fwd\prcov\fwd^*$ by $\lambda_j$. We input these eigenvalues into our
\emph{relaxed} model, and find a D-optimal design $\opt$ for our
relaxed model. In our relaxed model, the measurements we take are best
utilized in reducing uncertainty for the first $k$ eigenvectors. So, a
D-optimal design arising from our \emph{relaxed model} completely
avoids measuring eigenvectors $k+1$ and above.

Of course, in a real life problem --- such as the inverse problem of
the heat equation --- it is likely impossible to find measurement
locations for which all eigenvectors $k+1$ and above are
zero. However, we can still expect a D-optimal design to give as
little weight as possible to eigenvectors $k+1$ and above. Thus a
D-optimal design is a trade-off between measuring eigenvectors we want
(eigenvectors for which $\lambda_j$ is large) and treating the rest as
noise.


We explore the abovementione trade-off for the inverse problem of the
heat equation in Fig.~\ref{fig:eigenvectors}. We allow $m=4$
measurements in $\Omega = [0,1]$ and observe that D-optimal
measurement locations are clustered at $x_1 = 0.31$ and $x_2 =
0.69$. Upon close inspection of the scaled eigenvectors of $\fwd
\prcov \fwd^*$, we first observe that eigenvectors $3$ and above have
negligible prior amplitude. Since we only have $m=4$ measurements at
our disposal, we interpret these results, following Theorem
\ref{thm:char}, as implying we should only care about measuring the
first and second eigenvectors. Then, we note the D-optimal $x_1,x_2$
present a compromise between the amplitude of the first and second
eigenvectors. For example, a measurement at $x=0.5$ would have ignored
the second eigenvector altogether, since the second eigenvector is
zero at $x=0.5$.  Now we can understand measurement clusterization:
attempting to measure what we want and ignoring the rest, along with
the spatial limitations on measurements imposed by the structure of
eigenvectors of $\fwd \prcov \fwd^*$ is the cause for measurement
clusterization in D-optimal designs: we have $m=4$ measuremnts at our
disposal but only two spatial locations that are a good compromise
between the scaled eigenvectors. Thus, clusterization arises by the
pigeonhole principle.
