\documentclass{amsart}
\input{definitions}

\title{Corrections}

\begin{document}

\maketitle

Both referees made great comments that I feel improve the manuscript
substantially, and I would like to thank them for that. I followed up
on every point made. The referees' comments and my replies are listed
below. Again, thank you for taking the time and helping me improve
this manuscript.

\section{Referee 1}
Preliminary Assessment, Editorial Board Member The manuscript can be
sent to reviewers.


\section{Referee 2}
\subsection{Summary}
In this article, the author considers what causes sensor clustering in
optimal experimental design of Bayesian inverse problems. They show
how D-optimal designs may lead to sensor clustering in the case of iid
measurement noise. They also show that clustering does not occur for
spatially correlated measurement error. The paper is somewhat
difficult to follow due to writing and organizations corrections that
should be made, but does present original work. The article is
mathematically rigorous and significant in that the work is a
necessary step toward developing a deeper understanding of sensor
clustering. I suggest a major review and resubmission after reviewing
the comments below.


\subsection{General Comments}

\begin{enumerate}
\item It should be stated in the abstract whether the work in this
  paper pertains to only linear inverse problems, or if it is
  applicable in general to non-linear inverse problems as well.
  \ydnote{Title and abstract now reflect the fact that the paper only
    pertains to linear inverse problems.}
  
\item It is unclear from the contribution section of the article what
exactly the primary contribution is. This should be stated very
explicitly and emphasized clearly as the contribution of the article.
\ydnote{The contribution section was extended.}
  
\item In some equations (as seen below in the line comments) operators
are not clearly defined or described before they are given in a
proposition or theorem. Please clearly define all operators both with
mathematical definitions and descriptions (when applicable) before
using them.
\ydnote{Fixed.}

\item Overall, I find the article difficult to follow. I think this can
be improved by properly defining all operators and including section
overviews that give a description of the purpose of each section, and
a brief summary of how this is being accomplished.
\ydnote{Added section overviews and operator definitions.}


\item On page 10, figure 2 and section 4.2 deliver the main conclusion of
the article. These areas are deficient with only brief descriptions,
and should be much more thoroughly explained. In particular page 10,
lines 38-39 "If the precision in these is lower than the precision for
the next eigenvector, a repeated measurement is optimal" needs to be
elaborated on. How do you come to this conclusion? Additionally, the
wording of the statement can lead to confusion. From figure 2, it
looks like all measurements grant less precision than that of the
fourth eigenvector, so I first interpret this as all measurements
should be repeated. I believe these issues can be fixed by significant
elaboration in section 4.2.
\end{enumerate}


\subsection{Line Comments}

\begin{itemize}
\item Page 1, Line 39: "In a frequentist settings" should be changed
  to "in a frequentist setting" or "in frequentist
  settings".
  \ydnote{Changed.}

\item Page 2, Line 36: Add a comma: "In the process, we generalize..."
  \ydnote{Changed.}
  
\item Page 3, Line 37: "with homogeneous Dirichlet boundary condition"
  should be changed to "with a homogeneous Dirichlet boundary
  condiiton". \ydnote{Changed.}

\item Page 4, Line 14: In equation (2.1), $\Sigma$ has not been
  defined. Please formally define this before giving the equation.
  \ydnote{Moved the equation after $\Sigma$ is defined.}

\item Page 4, Line 21: The prior distribution $\mathcal{N}(0,
  \Gamma_{pr})$ assumes the prior mean is 0. This is worthy of some
  comment. Why must the prior mean be 0?  \ydnote{Assuming zero mean
    is hardly necessary, so I removed this statement.}

\item Page 5, Line 15: Add a comma: "Considering the sum of the error
  terms, it is easy..."
  \ydnote{Changed.}

  
\item Page 6, Line 7: Please describe what $\psi$ is in the
  theorem. It is defined mathematically but not described. Only much
  later is it stated that this is the objective
  function.
  \ydnote{Added comment before statment of theorem where $\tar$ first
    appears. Also, I added a formal definition of D-optimal design
    just after the theorem.}

\item Page 6, Line 19-20: "In section 3.2 we relax the constraints on
  $\mathcal{O}$ so that the optimization problem in (3.1) and find
  gradients for the new constraints." This sentence is unclear and
  should be reworded.
  \ydnote{Rephrased and added a short explanation.}

\item Page 7, Line 24, 27, 31: The proposition is labeled as
  Proposition 1, but it is refered to as Proposition 2, and relabeled
  Proposition 2 in the appendix.
  \ydnote{All such errors were fixed.}

\item Page 9, Line 6: Please be more exact when refering to "the big
  parentheses" to avoid any chance of confusion. Give the explicit
  formula here as you do for another term earlier in the sentence.
  \ydnote{Explicit expression given.}
  
\item Page 18, Line 6: This proposition was previously labeled
  Proposition 1. \ydnote{All such errors were fixed.}
\end{itemize}




\section{Referee 3}

\subsection{Overview}

This article discusses D-optimal experimental design for infinite
dimensional Bayesian linear inverse problems, and attempts to provide
a mathematical foundation to understanding the issue of clusterization
in such scenario, i.e., finding D-optimal designs.  As the author
mentioned in the Acknowledgments Section, the paper is paper is part
of the Ph.D. dissertation, and in general provides a good prospective
to sensor placement for Bayesian linear inverse problems formulated
using infinite dimensional approach.  I regret, however, to say that
the paper requires a major revision before being considered for
publication.  Below, I provide some comments, that hopefully will help
improve the manuscript.  Also, provided are some recent references,
the author may find useful.
%
\ydnote{The references are much appreciated. I extended the ``related
  work'' section accordingly.}


\subsection{General comments}

The author should clearly state why one should consider
infinite-dimensional formulation of the observation operator? One
could just merge the observation operator into the forward simulation
operator, and follow standard formulations adopted by the works in the
references below. Moreover, when it comes to sensor placement,
observations are almost surely pointwise in space/time. In this case,
I don't see the benefit of consider infinite-dimensional formulation.
%
\ydnote{I was not able to arrive at the same results when the forward
  and observation operators were merged. A short remark on this was
  added. It is placed after equation (2.5). Specifically, we need it
  in equation (3.1).}

The contributions detailed in Subsection 1.2 (page 2) are very much
appreciated, however they raise some concerns:

\begin{enumerate}
\item line 30: the author claims to present an analytically tractable
  model for understanding D-optimal designs. By reading the paper, I
  couldn't figure out how this reflects to any practical scenario. I'm
  not sure how this can be utilized for example in spatial sensor
  placement problems. An algorithmic description would be very useful
  here.
  \ydnote{This work does have some potential applications to (e.g.)
    spatial sensor placement problems. I believe they are out of scope
    for the current paper and I plan on studying those in the
    future. Here I only offer a toy model for understanding sensor
    clusterization.}

\item The statement made on page 2, lines 40-41 (... apply equally to
  any of the scenarios...) is rather strong and unjustified.
  \ydnote{Said statement was removed.}
\end{enumerate}

On page 8, Equation 4.2, which is crucial in this study, assumes that
the forward operator is invertible. This is clearly impossible in
practical Bayesian inverse problems, otherwise, it reduces to a
standard regression problem. This is understandable for clarification,
however, the following argument on Page 9 that extends this assumption
is not well-justified.
\ydnote{This is a great point! The flawed explanation was removed. I
  was able to remove that assumption, at the cost of changing several
  claims in the paper. Mainly, it forced me to work with Hilbert
  spaces everywhere. I believe this is for the best, because
  considering Banach spaces did not add much.}


In Equation 4.3, not sure what $\fwd^{-*}$ stands for. Moreover, an
inverse of the forward operator is not well-defined even if
$\mathcal{F}\Gamma_{prior}\mathcal{F}^*$ is invertible! This is very
limiting, and can't be used in practical inverse problems. This should
be clearly discussed for the reader to appreciate the valuable effort
made.
\ydnote{Again, great point. $\fwd^{-*} = (\fwd^*)^{-1}$ but now I do
  not assume $\fwd$ is invertible so this was removed.}


A numerical example with the setup studied in the paper, should help
clarify several confusions that a reader can fall for easily.
\ydnote{I added several plots that I believe make the arguments and
  conclusions clearer. I also added an illustration story with
  graduated lab cylinders. I also added a theorem summarizing the
  results for vanishing model error.}

The title should reflect the fact that the study is restricted to
linear inverse problems. Also, this should be clarified in the
abstract, in addition to the fact that observations errors are
uncorrelated with fixed variance $\sigma^2$.
\ydnote{The title and abstract were changed to reflect the fact that
  linear problems are concerned. The treatment of correlated errors
  was emphasized.}



\subsection{Other Comments}

In the abstract (line 20, page 1), and on page 5, lines 30-40, the
author mentions that the effect of observation-correlations on sensor
clusterization will be considered later, but I failed to find any
relevant discussion to that issue which I find interesting.
\ydnote{The discussion on observation correlation and how it mitigates
  clusterization can be found in the last section. It is now stated
  clearly in the contributions section.}

Even if the model error covariance $\Gamma_{model}$ is known, this
doesn't amount for observation correlations, .e.g,
single-instrument-observations, because the observation operator
$\mathcal{O}$ is non invertible. This issue should addressed and
clarified clearly.  \ydnote{I am not sure I understand this
  point. $\modcov$ can model anything you want, including correlated
  observation instruments. It is true that $\obs$ is not invertible
  but I fail to understand why this is problematic.}


I found it difficult to trace the referenced Lemmas, Corollaries,
etc., mainly because bad referencing in the appendix, and the links do
not works properly.  For example Corollary 2 should be Corollary B.x,
etc.  \ydnote{All such bad references and links were fixed.}

The trace operator should be followed with some sort of brackets to
make it easier to follow and validated the formulae.
\ydnote{Curly brackets added where needed.}

Sensor clusterization in OED is central to this study. Figure 2 is
used to illustrate the phenomena, however, it is underrepresented, and
is hard to decipher. Specifically, Section 4.2 (Page 10, lines 36-39),
and caption of Figure 2 should be rewritten with clear explanation of
the axes, and the plot-generation process.
%
\ydnote{I extended the
  discussion relating to clusterization, added another plot and
  explained how they arise.}

On page 10, lines 30-35, the argument doesn't seem to be quite
accurate. It's not always true that one associates a higher
weight/value to ``better'' observation. If one defines ``better'' as
lower uncertainty, then, variable observation variances must be
considered.
%
\ydnote{Unfortunately, I am again not sure I understand
  fully. Variable observation variances are fixed to $\sigma^2$
  througout this section. The eigenvalues are always weigted by the
  obeservation variance $sigma^2$. Before, $\sigma^{-2}$ was
  multiplying $\eta_i$. Now $\sigma^2$ multiplies $\lambda_i^{-1}$. In
  any case, obesevation error is taken into account.}

On the other hand, if being ``better'' corresponds to a more accurate
realization of the observation (true snapshot of reality), then the
whole statement would be wrong, because optimal design relies only on
the uncertainty levels in linear Bayesian settings, as manifested by
the posterior covariance matrix.
%
\ydnote{I think the referee implies that the D-optimal design is only
  as good as the assumptions it relies on. I could not agree
  more. However, as long as we do believe the assumptions made --- a
  D-optimal design is indeed optimal, in a precise and quantifiable
  way. I think the source of misunderstanding is that I used the term
  ``measurement'' where I should have used ``eigenvector''. I changed
  the paragraph accordingly. The transition from thinking of
  ``measurements'' to thinking of ``eigenvectors'' is confusing and I
  believe it is better explained in the updated version of the
  manusctipt.}


The last statement of the proof of Lemma A.1 (see page 12, line 24),
stating that ``there is nothing special about $t=0$, and the relation
holds for all t''. This is rather odd. In fact, It is assumed that
$X(t=0)$ explicitly (see line 32 on page 11). A clarification is
required here.
%
\ydnote{Clarification was added at the end of the
  proof. ``There is nothing special about $t=0$'' means we could have
  chosen any other value $t_0$ instead.}


\subsection{Minor Comments}

Page 1, line 22; ... ``calculating derivative of log det of
oeprators''. Please clarify for the reader, which operators you're
referring to.
%
\ydnote{Made explicit.}

Page 2, line 11; wrong reference to Figure 1.3?
%
\ydnote{Fixed.}

Page 6; in equation 2.7, don't you need an expectation over all priors
to achieve this result? Please clarify.
%
\ydnote{The expectation / integration over the prior space happens
  inside the KL divergence:
  $$
  \mathbb{E}_{\data}[KL(\post||\pr)] = \int KL(\post||\pr) \der
  \mu(\data) = \int \int \log \frac{\der \post}{\der \pr}(\param) \der
  \post(\param) \der \mu(\data)
  $$
  where $\mu(\data)$ is the distribution of data (usually called
  evidence). I improved the notation to reflect the dependence of the
  posterior on the data $\data$. I added above definition of KL
  divergence to the manuscript.}

Page 6, line 13; the optimization problem is solved for $\mathcal{O}$,
not $\Psi$ ?  \ydnote{This line was unclear. I extended the
  explanation. The mauscript now reflects the claim that the optimal
  $\obs^{\star}$ is the maximizer of $\tar(\obs)$.}
  
Page 6, lines 19; the statement made about relaxing the constraints on
the observation operator is unclear.
%
\ydnote{This sentence was poorly
  written. It is now rephrased.}

Page 6, line 24, I believe the reference to (3.2) should be replaced
with (3.1).
%
\ydnote{This is actually OK. The gradient was recorded in (3.2).}

Page 6, line 26, proof is provided in which Appendix? Please state
clearly.
%
\ydnote{Fixed.}

page 6, In the proof of Lemma 3.1, the author utilized the operator
$T$, which is only introduced in the appendix (Proof of Lemma
A.1). Maybe state that clearly here. Also, in the proof, from step 4
to 5, the first and third terms vanished; why?
%
\ydnote{This point is spot on. I added the definition of $T$ before
  the derivation. The proof referee 2 refers to is not the proof of
  Lemma 3.1. It is the first variation of the objective. I reworded a
  few sentences to make this clearer. Regarding the vanishing terms
  --- this is done using trace rules (notice the $\frac{1}{2}$
  prefactor disappears). I added a comment explaining this.}

Page 7; Proposition 1 should be replaced with something like
Proposition 3.2 or something similar to match previous referencing
(e.g., Lemma 3.1). \ydnote{Fixed.}

Page 7, line 27 (and also line 31); the reference to Proposition 2
seems to be incorrect. Also, state clearly which appendix the proof is
delegated to.  \ydnote{Fixed.}

Page 10, Corollary 1 should be renamed (see the comment on Proposition
1 above). Same applies to Corollary 3 in the appendix. Better also to
clarify which appendix. Finally, the author utilizes
$\widehat{\Gamma_{post}}$ int the corollary, which is defined only in
the appendix, under the proof. \ydnote{All comments on numbering and
  references are great and correct. All were fixed. I added an
  explanation of what $\widehat{\Gamma_{post}}$ means where it
  appears.}

Page 11, in the proof of Lemma A.1 (lines 39-43), the partial
derivative vanished from step 2 to step 3; please explain!
\ydnote{I added an extra equality line for explanation.}

References:

* - Koval, Karina, Alen Alexanderian, and Georg
Stadler. "Optimal experimental design under irreducible uncertainty
for linear inverse problems governed by PDEs." Inverse Problems
(2020).

* - Attia, Ahmed, and Emil Constantinescu. "Optimal Experimental
Design for Inverse Problems in the Presence of Observation
Correlations." arXiv preprint arXiv:2007.14476 (2020).

Letter reference: DSMa01


\end{document}
